{
    "Adversarial": {
    "http://arxiv.org/abs/2312.10302v3": {
        "Paper Title": "One Shot Learning as Instruction Data Prospector for Large Language\n  Models",
        "Sentences": [
            {
                "Sentence ID": 6,
                "Sentence": "demonstrated\nremarkably strong performance by strategically selecting a thousand high-quality data points for\nlearning. InstructMining ",
                "Citation Text": "Yihan Cao, Yanbin Kang, and Lichao Sun. Instruction mining: High-quality instruction data selection for\nlarge language models. arXiv preprint arXiv:2307.06290 , 2023.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2307.06290",
                    "Citation Paper Title": "Title:Instruction Mining: When Data Mining Meets Large Language Model Finetuning",
                    "Citation Paper Abstract": "Abstract:Large language models (LLMs) are initially pretrained for broad capabilities and then finetuned with instruction-following datasets to improve their performance in interacting with humans. Despite advances in finetuning, a standardized guideline for selecting high-quality datasets to optimize this process remains elusive. In this paper, we first propose InstructMining, an innovative method designed for automatically selecting premium instruction-following data for finetuning LLMs. Specifically, InstructMining utilizes natural language indicators as a measure of data quality, applying them to evaluate unseen datasets. During experimentation, we discover that double descent phenomenon exists in large language model finetuning. Based on this observation, we further leverage BlendSearch to help find the best subset among the entire dataset (i.e., 2,532 out of 100,000). Experiment results show that InstructMining-7B achieves state-of-the-art performance on two of the most popular benchmarks: LLM-as-a-judge and Huggingface OpenLLM leaderboard.",
                    "Citation Paper Authors": "Authors:Yihan Cao, Yanbin Kang, Chi Wang, Lichao Sun"
                }
            },
            {
                "Sentence ID": 61,
                "Sentence": "progressively modified the original instruction in a step-by-step manner, allowing\nfor precise control over the difficulty and complexity of the generated instructions. Tree-Instruct ",
                "Citation Text": "Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Fei Huang, Yongbin Li, and Nevin L Zhang.\nA preliminary study of the intrinsic relationship between complexity and alignment. arXiv preprint\narXiv:2308.05696 , 2023.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2308.05696",
                    "Citation Paper Title": "Title:A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment",
                    "Citation Paper Abstract": "Abstract:Training large language models (LLMs) with open-domain instruction data has yielded remarkable success in aligning to end tasks and user preferences. Extensive research has highlighted that enhancing the quality and diversity of instruction data consistently improves performance. However, the impact of data complexity, as a crucial metric, remains relatively unexplored in three aspects: (1) scaling law, where the sustainability of performance improvements with increasing complexity is uncertain, (2) additional tokens, whether the improvement brought by complexity comes from introducing more training tokens, and (3) curriculum tuning, where the potential advantages of incorporating instructions ranging from easy to difficult are not yet fully understood. In this paper, we propose \\textit{tree-instruct} to systematically enhance the complexity of instruction data in a controllable manner. This approach adds a specified number of nodes into the instruction semantic tree, yielding new instruction data based on the modified tree. By adjusting the number of added nodes, we can control the difficulty level in the modified instruction data. Our preliminary experiments reveal the following insights: (1) Increasing complexity consistently leads to sustained performance improvements. For instance, using 1,000 instruction data and 10 nodes resulted in a substantial 24\\% increase in win rate. (2) Under the same token budget, a few complex instructions outperform diverse yet simple instructions. (3) Curriculum instruction tuning might not yield the anticipated results; focusing on increasing complexity appears to be the key.",
                    "Citation Paper Authors": "Authors:Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Fei Huang, Yongbin Li, Nevin L. Zhang"
                }
            },
            {
                "Sentence ID": 53,
                "Sentence": "selected a handful of instances randomly from the initial task pool and used them as demonstrations to\ninstruct a language model in generating new instructions along with corresponding input-output pairs.\nEvol-Instruct ",
                "Citation Text": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint\narXiv:2304.12244 , 2023.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2304.12244",
                    "Citation Paper Title": "Title:WizardLM: Empowering Large Language Models to Follow Complex Instructions",
                    "Citation Paper Abstract": "Abstract:Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at this https URL",
                    "Citation Paper Authors": "Authors:Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang"
                }
            },
            {
                "Sentence ID": 42,
                "Sentence": "took the initiative to delve into open-domain\ninstruction tuning using the open-source LLM LLaMA ",
                "Citation Text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971 , 2023.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2302.13971",
                    "Citation Paper Title": "Title:LLaMA: Open and Efficient Foundation Language Models",
                    "Citation Paper Abstract": "Abstract:We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
                    "Citation Paper Authors": "Authors:Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample"
                }
            },
            {
                "Sentence ID": 33,
                "Sentence": "introduced the novel\nconcept of instruction tuning , aiming to improve zero shot task performance by transforming NLP\ntasks into natural language instructions during model training. Furthermore, InstructGPT ",
                "Citation Text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. In NeurIPS , pages 27730\u201327744, 2022.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2203.02155",
                    "Citation Paper Title": "Title:Training language models to follow instructions with human feedback",
                    "Citation Paper Abstract": "Abstract:Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
                    "Citation Paper Authors": "Authors:Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe"
                }
            },
            {
                "Sentence ID": 47,
                "Sentence": "pioneered the initial effort of training various natural\nlanguage processing (NLP) tasks in a unified text-to-text format. FLAN ",
                "Citation Text": "Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. In ICLR , 2021.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2109.01652",
                    "Citation Paper Title": "Title:Finetuned Language Models Are Zero-Shot Learners",
                    "Citation Paper Abstract": "Abstract:This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks.\nWe take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
                    "Citation Paper Authors": "Authors:Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le"
                }
            },
            {
                "Sentence ID": 35,
                "Sentence": "Instruction Tuning Recent works have introduced a series of techniques that aim at refining\nlarge language models (LLMs), showcasing their ability to generalize effectively to instructions not\nencountered before. For instance, T5 ",
                "Citation Text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research , 21(1):5485\u20135551, 2020.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1910.10683",
                    "Citation Paper Title": "Title:Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                    "Citation Paper Abstract": "Abstract:Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
                    "Citation Paper Authors": "Authors:Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2306.10759v4": {
        "Paper Title": "SGFormer: Simplifying and Empowering Transformers for Large-Graph\n  Representations",
        "Sentences": [
            {
                "Sentence ID": 54,
                "Sentence": "proposes\nkernelized Gumbel-Softmax message passing that utilizes random feature maps to approximate the\nall-pair attention with linear complexity. Another work ",
                "Citation Text": "Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, and Junchi Yan. DIFFormer:\nScalable (graph) transformers induced by energy constrained diffusion. In International Confer-\nence on Learning Representations , 2023.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2301.09474",
                    "Citation Paper Title": "Title:DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion",
                    "Citation Paper Abstract": "Abstract:Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances' information by their interactions. The diffusion process is constrained by descent criteria w.r.t.~a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFormer (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instance numbers, and an advanced version for learning complex structures. Experiments highlight the wide applicability of our model as a general-purpose encoder backbone with superior performance in various tasks, such as node classification on large graphs, semi-supervised image/text classification, and spatial-temporal dynamics prediction.",
                    "Citation Paper Authors": "Authors:Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, Junchi Yan"
                }
            },
            {
                "Sentence ID": 59,
                "Sentence": "as baselines. On top of this, we compare with\nadvanced GNN models, including JKNet ",
                "Citation Text": "Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and\nStefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In\nInternational Conference on Machine Learning , pages 5449\u20135458, 2018.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1806.03536",
                    "Citation Paper Title": "Title:Representation Learning on Graphs with Jumping Knowledge Networks",
                    "Citation Paper Abstract": "Abstract:Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of \"neighboring\" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.",
                    "Citation Paper Authors": "Authors:Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, Stefanie Jegelka"
                }
            },
            {
                "Sentence ID": 1,
                "Sentence": "that requires O(N2). While the Softmax attention possesses provable\nexpressivity ",
                "Citation Text": "Anson Bastos, Abhishek Nadgeri, Kuldeep Singh, Hiroki Kanezashi, Toyotaro Suzumura, and\nIsaiah Onando Mulang\u2019. How expressive are transformers in spectral domain for graphs? Trans.\nMach. Learn. Res. , 2022, 2022.\n10",
                "Citation": {
                    "Citation Paper ID": "arXiv:2201.09332",
                    "Citation Paper Title": "Title:How Expressive are Transformers in Spectral Domain for Graphs?",
                    "Citation Paper Abstract": "Abstract:The recent works proposing transformer-based models for graphs have proven the inadequacy of Vanilla Transformer for graph representation learning. To understand this inadequacy, there is a need to investigate if spectral analysis of the transformer will reveal insights into its expressive power. Similar studies already established that spectral analysis of Graph neural networks (GNNs) provides extra perspectives on their expressiveness. In this work, we systematically study and establish the link between the spatial and spectral domain in the realm of the transformer. We further provide a theoretical analysis and prove that the spatial attention mechanism in the transformer cannot effectively capture the desired frequency response, thus, inherently limiting its expressiveness in spectral space. Therefore, we propose FeTA, a framework that aims to perform attention over the entire graph spectrum (i.e., actual frequency components of the graphs) analogous to the attention in spatial space. Empirical results suggest that FeTA provides homogeneous performance gain against vanilla transformer across all tasks on standard benchmarks and can easily be extended to GNN-based models with low-pass characteristics (e.g., GAT).",
                    "Citation Paper Authors": "Authors:Anson Bastos, Abhishek Nadgeri, Kuldeep Singh, Hiroki Kanezashi, Toyotaro Suzumura, Isaiah Onando Mulang'"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2312.16713v2": {
        "Paper Title": "Knowledge Enhanced Conditional Imputation for Healthcare Time-series",
        "Sentences": [
            {
                "Sentence ID": 27,
                "Sentence": ". Addressing this issue typically involves integrating posi-\ntional encodings, yet there is ongoing debate about their effectiveness in capturing\nfine-grained temporal nuances, particularly in scenarios with complex missing patterns\nand inter-feature relationships.\nRDIS ",
                "Citation Text": "Choi, T.-M., Kang, J.-S., Kim, J.-H.: Rdis: Random drop imputation with self-\ntraining for incomplete time series data. IEEE Access (2023)",
                "Citation": {
                    "Citation Paper ID": "arXiv:2010.10075",
                    "Citation Paper Title": "Title:RDIS: Random Drop Imputation with Self-Training for Incomplete Time Series Data",
                    "Citation Paper Abstract": "Abstract:Time-series data with missing values are commonly encountered in many fields, such as healthcare, meteorology, and robotics. The imputation aims to fill the missing values with valid values. Most imputation methods trained the models implicitly because missing values have no ground truth. In this paper, we propose Random Drop Imputation with Self-training (RDIS), a novel training method for time-series data imputation models. In RDIS, we generate extra missing values by applying a random drop on the observed values in incomplete data. We can explicitly train the imputation models by filling in the randomly dropped values. In addition, we adopt self-training with pseudo values to exploit the original missing values. To improve the quality of pseudo values, we set the threshold and filter them by calculating the entropy. To verify the effectiveness of RDIS on the time series imputation, we test RDIS to various imputation models and achieve competitive results on two real-world datasets.",
                    "Citation Paper Authors": "Authors:Tae-Min Choi, Ji-Su Kang, Jong-Hwan Kim"
                }
            },
            {
                "Sentence ID": 22,
                "Sentence": ". How-\never, all existing approaches require additional uncertainty modules added to the\nimputing network, which translates to unstable training due to increased coupling\nand leads to less accurate imputation ",
                "Citation Text": "Mescheder, L., Geiger, A., Nowozin, S.: Which training methods for gans do\nactually converge? In: International Conference on Machine Learning, pp. 3481\u2013\n3490 (2018). PMLR",
                "Citation": {
                    "Citation Paper ID": "arXiv:1801.04406",
                    "Citation Paper Title": "Title:Which Training Methods for GANs do actually Converge?",
                    "Citation Paper Abstract": "Abstract:Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.",
                    "Citation Paper Authors": "Authors:Lars Mescheder, Andreas Geiger, Sebastian Nowozin"
                }
            },
            {
                "Sentence ID": 18,
                "Sentence": "incorporates temporal decay for missing data imputation, and its\nextensions, MRNN ",
                "Citation Text": "Yoon, J., Zame, W.R., Schaar, M.: Multi-directional recurrent neural networks:\nA novel method for estimating missing data. In: Time Series Workshop in\nInternational Conference on Machine Learning (2017)",
                "Citation": {
                    "Citation Paper ID": "arXiv:1711.08742",
                    "Citation Paper Title": "Title:Estimating Missing Data in Temporal Data Streams Using Multi-directional Recurrent Neural Networks",
                    "Citation Paper Abstract": "Abstract:Missing data is a ubiquitous problem. It is especially challenging in medical settings because many streams of measurements are collected at different - and often irregular - times. Accurate estimation of those missing measurements is critical for many reasons, including diagnosis, prognosis and treatment. Existing methods address this estimation problem by interpolating within data streams or imputing across data streams (both of which ignore important information) or ignoring the temporal aspect of the data and imposing strong assumptions about the nature of the data-generating process and/or the pattern of missing data (both of which are especially problematic for medical data). We propose a new approach, based on a novel deep learning architecture that we call a Multi-directional Recurrent Neural Network (M-RNN) that interpolates within data streams and imputes across data streams. We demonstrate the power of our approach by applying it to five real-world medical datasets. We show that it provides dramatically improved estimation of missing measurements in comparison to 11 state-of-the-art benchmarks (including Spline and Cubic Interpolations, MICE, MissForest, matrix completion and several RNN methods); typical improvements in Root Mean Square Error are between 35% - 50%. Additional experiments based on the same five datasets demonstrate that the improvements provided by our method are extremely robust.",
                    "Citation Paper Authors": "Authors:Jinsung Yoon, William R. Zame, Mihaela van der Schaar"
                }
            },
            {
                "Sentence ID": 17,
                "Sentence": "Efforts to impute multivariate time-series data have resulted in numerous strategies\nwhere we mainly focus on highly performing deep learning models. Among those, the\nGRUD model ",
                "Citation Text": "Che, Z., Purushotham, S., Cho, K., Sontag, D., Liu, Y.: Recurrent neural net-\nworks for multivariate time series with missing values. Scientific reports 8(1),\n1\u201312 (2018)",
                "Citation": {
                    "Citation Paper ID": "arXiv:1606.01865",
                    "Citation Paper Title": "Title:Recurrent Neural Networks for Multivariate Time Series with Missing Values",
                    "Citation Paper Abstract": "Abstract:Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.",
                    "Citation Paper Authors": "Authors:Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, Yan Liu"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2310.12081v2": {
        "Paper Title": "DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical\n  Optimal Transport Framework",
        "Sentences": [
            {
                "Sentence ID": 45,
                "Sentence": "includes an online graph with 16,328 interactions among 3,906\nusers and an offline graph with 3,022 interactions among 1,118 users. The user\u2019s location\nrepresents node attributes. The ground-truth matching is the 1,118 users appearing in both\ngraphs.\n\u2022Cora ",
                "Citation Text": "Z. Yang, W. W. Cohen, and R. Salakhutdinov. Revisiting semi-supervised learning with graph\nembeddings. In M. Balcan and K. Q. Weinberger, editors, Proceedings of the 33nd International\nConference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016 ,\nvolume 48 of JMLR Workshop and Conference Proceedings , pages 40\u201348. JMLR.org, 2016.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1603.08861",
                    "Citation Paper Title": "Title:Revisiting Semi-Supervised Learning with Graph Embeddings",
                    "Citation Paper Abstract": "Abstract:We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.",
                    "Citation Paper Authors": "Authors:Zhilin Yang, William W. Cohen, Ruslan Salakhutdinov"
                }
            },
            {
                "Sentence ID": 33,
                "Sentence": "extends the GW distance so that it can be applied to the task\nof matching attribute graphs. SLOTAlign ",
                "Citation Text": "J. Tang, W. Zhang, J. Li, K. Zhao, F. Tsung, and J. Li. Robust attributed graph alignment\nvia joint structure learning and optimal transport. ArXiv preprint , abs/2301.12721, 2023.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2301.12721",
                    "Citation Paper Title": "Title:Robust Attributed Graph Alignment via Joint Structure Learning and Optimal Transport",
                    "Citation Paper Abstract": "Abstract:Graph alignment, which aims at identifying corresponding entities across multiple networks, has been widely applied in various domains. As the graphs to be aligned are usually constructed from different sources, the inconsistency issues of structures and features between two graphs are ubiquitous in real-world applications. Most existing methods follow the ``embed-then-cross-compare'' paradigm, which computes node embeddings in each graph and then processes node correspondences based on cross-graph embedding comparison. However, we find these methods are unstable and sub-optimal when structure or feature inconsistency appears. To this end, we propose SLOTAlign, an unsupervised graph alignment framework that jointly performs Structure Learning and Optimal Transport Alignment. We convert graph alignment to an optimal transport problem between two intra-graph matrices without the requirement of cross-graph comparison. We further incorporate multi-view structure learning to enhance graph representation power and reduce the effect of structure and feature inconsistency inherited across graphs. Moreover, an alternating scheme based algorithm has been developed to address the joint optimization problem in SLOTAlign, and the provable convergence result is also established. Finally, we conduct extensive experiments on six unsupervised graph alignment datasets and the DBP15K knowledge graph (KG) alignment benchmark dataset. The proposed SLOTAlign shows superior performance and strongest robustness over seven unsupervised graph alignment methods and five specialized KG alignment methods.",
                    "Citation Paper Authors": "Authors:Jianheng Tang, Weiqi Zhang, Jiajin Li, Kangfei Zhao, Fugee Tsung, Jia Li"
                }
            },
            {
                "Sentence ID": 27,
                "Sentence": ". The algorithmic scheme is shown in Algorithm 1.\nFor a graph with Vnodes and Kmodalities, the computational complexity of this algorithm is\nO(K2V3) and can be reduced to O(K2V2d) when considering the low-rank structure of relational\nmatrices ",
                "Citation Text": "M. Scetbon, G. Peyr\u00b4 e, and M. Cuturi. Linear-time gromov wasserstein distances using low rank\ncouplings and costs. In International Conference on Machine Learning , pages 19347\u201319365.\nPMLR, 2022.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2106.01128",
                    "Citation Paper Title": "Title:Linear-Time Gromov Wasserstein Distances using Low Rank Couplings and Costs",
                    "Citation Paper Abstract": "Abstract:The ability to align points across two related yet incomparable point clouds (e.g. living in different spaces) plays an important role in machine learning. The Gromov-Wasserstein (GW) framework provides an increasingly popular answer to such problems, by seeking a low-distortion, geometry-preserving assignment between these points. As a non-convex, quadratic generalization of optimal transport (OT), GW is NP-hard. While practitioners often resort to solving GW approximately as a nested sequence of entropy-regularized OT problems, the cubic complexity (in the number $n$ of samples) of that approach is a roadblock. We show in this work how a recent variant of the OT problem that restricts the set of admissible couplings to those having a low-rank factorization is remarkably well suited to the resolution of GW: when applied to GW, we show that this approach is not only able to compute a stationary point of the GW problem in time $O(n^2)$, but also uniquely positioned to benefit from the knowledge that the initial cost matrices are low-rank, to yield a linear time $O(n)$ GW approximation. Our approach yields similar results, yet orders of magnitude faster computation than the SoTA entropic GW approaches, on both simulated and real data.",
                    "Citation Paper Authors": "Authors:Meyer Scetbon, Gabriel Peyr\u00e9, Marco Cuturi"
                }
            },
            {
                "Sentence ID": 24,
                "Sentence": ". By solving OT plans at different levels, HOT has\nachieved encouraging performance in multi-modal distribution [18, 46], multi-modal learning ",
                "Citation Text": "D. Luo, H. Xu, and L. Carin. Differentiable hierarchical optimal transport for robust multi-\nview learning. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2006.03160",
                    "Citation Paper Title": "Title:Hierarchical Optimal Transport for Robust Multi-View Learning",
                    "Citation Paper Abstract": "Abstract:Traditional multi-view learning methods often rely on two assumptions: ($i$) the samples in different views are well-aligned, and ($ii$) their representations in latent space obey the same distribution. Unfortunately, these two assumptions may be questionable in practice, which limits the application of multi-view learning. In this work, we propose a hierarchical optimal transport (HOT) method to mitigate the dependency on these two assumptions. Given unaligned multi-view data, the HOT method penalizes the sliced Wasserstein distance between the distributions of different views. These sliced Wasserstein distances are used as the ground distance to calculate the entropic optimal transport across different views, which explicitly indicates the clustering structure of the views. The HOT method is applicable to both unsupervised and semi-supervised learning, and experimental results show that it performs robustly on both synthetic and real-world tasks.",
                    "Citation Paper Authors": "Authors:Dixin Luo, Hongteng Xu, Lawrence Carin"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2312.12728v2": {
        "Paper Title": "Lookahead: An Inference Acceleration Framework for Large Language Model\n  with Lossless Generation Accuracy",
        "Sentences": [
            {
                "Sentence ID": 16,
                "Sentence": ", which utilizes an input-guided method that copiesLookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy\ncontent from the input sentence through prefix matching. Another\nstrategy LLMA, proposed by Yang et al. ",
                "Citation Text": "Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan\nMajumder, and Furu Wei. Inference with reference: Lossless acceleration of large\nlanguage models. ArXiv , abs/2304.04487, 2023.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2304.04487",
                    "Citation Paper Title": "Title:Inference with Reference: Lossless Acceleration of Large Language Models",
                    "Citation Paper Abstract": "Abstract:We propose LLMA, an LLM accelerator to losslessly speed up Large Language Model (LLM) inference with references. LLMA is motivated by the observation that there are abundant identical text spans between the decoding result by an LLM and the reference that is available in many real world scenarios (e.g., retrieved documents). LLMA first selects a text span from the reference and copies its tokens to the decoder and then efficiently checks the tokens' appropriateness as the decoding result in parallel within one decoding step. The improved computational parallelism allows LLMA to achieve over 2x speed-up for LLMs with identical generation results as greedy decoding in many practical generation scenarios where significant overlap between in-context reference and outputs exists (e.g., search engines and multi-turn conversations).",
                    "Citation Paper Authors": "Authors:Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, Furu Wei"
                }
            },
            {
                "Sentence ID": 12,
                "Sentence": "Recently, several strategies have been proposed and developed\nto enhance the inference speed of LLMs while maintaining the\noutput quality within an acceptable range. One such strategy is\nthe non-auto-regressive approach, specifically non-auto-regressive\ntranslation (NAT) ",
                "Citation Text": "Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li, and Richard Socher.\nNon-autoregressive neural machine translation. ArXiv , abs/1711.02281, 2017.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1711.02281",
                    "Citation Paper Title": "Title:Non-Autoregressive Neural Machine Translation",
                    "Citation Paper Abstract": "Abstract:Existing approaches to neural machine translation condition each output word on previously generated outputs. We introduce a model that avoids this autoregressive property and produces its outputs in parallel, allowing an order of magnitude lower latency during inference. Through knowledge distillation, the use of input token fertilities as a latent variable, and policy gradient fine-tuning, we achieve this at a cost of as little as 2.0 BLEU points relative to the autoregressive Transformer network used as a teacher. We demonstrate substantial cumulative improvements associated with each of the three aspects of our training strategy, and validate our approach on IWSLT 2016 English-German and two WMT language pairs. By sampling fertilities in parallel at inference time, our non-autoregressive model achieves near-state-of-the-art performance of 29.8 BLEU on WMT 2016 English-Romanian.",
                    "Citation Paper Authors": "Authors:Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, Richard Socher"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1812.09352v1": {
        "Paper Title": "Determinants of cyclization-decyclization kinetics of short DNA with\n  sticky ends",
        "Sentences": [
            {
                "Sentence ID": 52,
                "Sentence": ". As shown in this plot, the\nJ factors of DNA set 1 correspond to persistence lengths\nbetween 44 and 49 nm. This 5-nm variability is still\nwithin the accepted range of experimentally determined\nvalues ",
                "Citation Text": "A. Brunet, C. Tardin, L. Salome, P. Rousseau,\nN. Destainville, and M. Manghi, Macromolecules 48,\n3641 (2015).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1504.02666",
                    "Citation Paper Title": "Title:Dependence of DNA persistence length on ionic strength of solutions with monovalent and divalent salts: a joint theory-experiment study",
                    "Citation Paper Abstract": "Abstract:Using high-throughput Tethered Particle Motion single molecule experiments, the double-stranded DNA persistence length, $L_p$, is measured in solutions with Na$^+$ and Mg$^{2+}$ ions of various ionic strengths, $I$. Several theoretical equations for $L_p(I)$ are fitted to the experimental data, but no decisive theory is found which fits all the $L_p$ values for the two ion valencies. Properly extracted from the particle trajectory using simulations, $L_p$ varies from 30~nm to 55~nm, and is compared to previous experimental results. For the Na$^+$ only case, $L_p$ is an increasing concave function of $I^{-1}$, well fitted by Manning's electrostatic stretching approach, but not by classical Odjik-Skolnick-Fixman theories with or without counter-ion condensation. With added Mg$^{2+}$ ions, $L_p$ shows a marked decrease at low $I$, interpreted as an ion-ion correlation effect, with an almost linear law in $I^{-1}$, fitted by a proposed variational approach.",
                    "Citation Paper Authors": "Authors:Anna\u00ebl Brunet, Catherine Tardin, Laurence Salom\u00e9, Philippe Rousseau, Nicolas Destainville, Manoel Manghi"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1804.11081v1": {
        "Paper Title": "Visualizing mitochondrial FoF1-ATP synthase as the target of the\n  immunomodulatory drug Bz-423",
        "Sentences": [
            {
                "Sentence ID": 34,
                "Sentence": "fused to the C -terminus of the \uf067 subunit of F oF1-ATP \nsynthase ",
                "Citation Text": "F. Foertsch, M. Ilchenko, T. Heitkamp, S. Nossmann, B. Hoffmann, I. Starke, R. Mrowka, C. \nBiskup, M. Borsch, Imaging cytochrome C oxidase and F0F1 -ATP synthase in mitochondrial cristae of \nliving hu man cells by FLIM and superresolution microscopy, Proc. SPIE, 10071 (2017) 100710P.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1702.00512",
                    "Citation Paper Title": "Title:Imaging cytochrome C oxidase and FoF1-ATP synthase in mitochondrial cristae of living human cells by FLIM and superresolution microscopy",
                    "Citation Paper Abstract": "Abstract:Cytochrome C oxidase and FoF1-ATP synthase constitute complex IV and V, respectively, of the five membrane-bound enzymes in mitochondria comprising the respiratory chain. These enzymes are located in the inner mitochondrial membrane (IMM), which exhibits large invaginations called cristae. According to recent cryo-tomography, FoF1-ATP synthases are located predominantly at the rim of the cristae, while cytochrome C oxidases are likely distributed in planar membrane areas of the cristae. Previous FLIM measurements (K. Busch and coworkers) of complex II and III unravelled differences in the local environment of the membrane enzymes in the cristae. Here, we tagged complex IV and V with mNeonGreen and investigated their mitochondrial nano-environment by FLIM and superresolution microscopy in living human cells. Different lifetimes and anisotropy values were found and will be discussed.",
                    "Citation Paper Authors": "Authors:Franziska Foertsch, Mykhailo Ilchenko, Thomas Heitkamp, Silke Nossmann, Birgit Hoffmann, Ilka Starke, Ralf Mrowka, Christoph Biskup, Michael Borsch"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1804.06281v1": {
        "Paper Title": "Deep transfer learning in the assessment of the quality of protein\n  models",
        "Sentences": [
            {
                "Sentence ID": 6,
                "Sentence": "Y. LeCun, Y. Bengio, and G. Hinton, Nature 521,\n436?444 (2015). ",
                "Citation Text": "J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, in Pro-\nceedings of the 27th International Conference on Neu-\nral Information Processing Systems - Volume 2 , NIPS\u201914\n(MIT Press, Cambridge, MA, USA, 2014) pp. 3320\u20133328.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1411.1792",
                    "Citation Paper Title": "Title:How transferable are features in deep neural networks?",
                    "Citation Paper Abstract": "Abstract:Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",
                    "Citation Paper Authors": "Authors:Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2110.03339v2": {
        "Paper Title": "Morphology and high frequency bio-electric fields",
        "Sentences": [
            {
                "Sentence ID": 32,
                "Sentence": "the emergence of a collective oscillating\nmode of dipolar molecules driven by a random energy supply was shown, and in ",
                "Citation Text": "Jordane Preto, Marco Pettini, and Jack A. Tuszynski, Possible role of electrodynamic\ninteractions in long-distance biomolecular recognition , Phys. Rev. E 91, 05271 (2015).\nhttps://doi.org/10.1103/PhysRevE.91.052710",
                "Citation": {
                    "Citation Paper ID": "arXiv:1403.2477",
                    "Citation Paper Title": "Title:On the role of electrodynamic interactions in long-distance biomolecular recognition",
                    "Citation Paper Abstract": "Abstract:The issue of retarded long-range resonant interactions between two molecules with oscillating dipole moments is reinvestigated within the framework of classical electrodynamics. By taking advantage of a theorem in complex analysis, we present a simple method to calculate the frequencies of the normal modes, which are then used to estimate the interaction potential. The main results thus found are in perfect agreement with several results obtained from quantum computations. Moreover, when applied in a biophysical context, our findings shed new light on Fr\u007fohlich's theory of selective long-range interactions between biomolecules. In particular, at variance with a long-standing belief, we show that sizable resonant long-range interactions may exist only if the interacting system is out of thermal equilibrium.",
                    "Citation Paper Authors": "Authors:Jordane Preto, Marco Pettini, Jack A. Tuszynski"
                }
            },
            {
                "Sentence ID": 31,
                "Sentence": ". Fr\u007f ohlich's work still stimulates research on how living systems might make use\nof oscillatory resonances between molecules. E.g., in ",
                "Citation Text": "Simona Olmi, Matteo Gori, Irene Donato and Marco Pettini, Collective behavior of os-\ncillating electric dipoles , Scienti\fc Reports 8, 15748 (2018). https://doi.org/10.1038/\ns41598-018-33990-y",
                "Citation": {
                    "Citation Paper ID": "arXiv:1711.07547",
                    "Citation Paper Title": "Title:Collective behavior of oscillating electric dipoles",
                    "Citation Paper Abstract": "Abstract:The present work reports about the dynamics of a collection of randomly distributed, and randomly oriented, oscillators in 3D space, coupled by an interaction potential falling as $1/r^3$, where r stands for the inter-particle distance. This model schematically represents a collection of identical biomolecules, coherently vibrating at some common frequency, coupled with a $1/r^3$ potential stemming from the electrodynamic interaction between oscillating dipoles. The oscillating dipole moment of each molecule being a direct consequence of its coherent (collective) vibration. By changing the average distance among the molecules, neat and substantial changes in the power spectrum of the time variation of a collective observable are found. As the average intermolecular distance can be varied by changing the concentration of the solvated molecules, and as the collective variable investigated is proportional to the projection of the total dipole moment of the model biomolecules on a coordinate plane, we have found a prospective experimental strategy of spectroscopic kind to check whether the mentioned intermolecular electrodynamic interactions can be strong enough to be detectable, and thus to be of possible relevance to biology.",
                    "Citation Paper Authors": "Authors:Simona Olmi, Matteo Gori, Irene Donato, Marco Pettini"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2111.14053v2": {
        "Paper Title": "Towards Conditional Generation of Minimal Action Potential Pathways for\n  Molecular Dynamics",
        "Sentences": [
            {
                "Sentence ID": 19,
                "Sentence": ". Lastly, methods\nhave used HNNs with GNNs in order to train on data to enforce a potential energy bias in order to\npropel a string of beads in an MD simulation towards a desired shape (by demonstrating differentiable\ncontrol) ",
                "Citation Text": "Wujie Wang, Simon Axelrod, and Rafael G\u00f3mez-Bombarelli. Differentiable molecular simula-\ntions for control and learning, 2020.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2003.00868",
                    "Citation Paper Title": "Title:Differentiable Molecular Simulations for Control and Learning",
                    "Citation Paper Abstract": "Abstract:Molecular dynamics simulations use statistical mechanics at the atomistic scale to enable both the elucidation of fundamental mechanisms and the engineering of matter for desired tasks. The behavior of molecular systems at the microscale is typically simulated with differential equations parameterized by a Hamiltonian, or energy function. The Hamiltonian describes the state of the system and its interactions with the environment. In order to derive predictive microscopic models, one wishes to infer a molecular Hamiltonian that agrees with observed macroscopic quantities. From the perspective of engineering, one wishes to control the Hamiltonian to achieve desired simulation outcomes and structures, as in self-assembly and optical control, to then realize systems with the desired Hamiltonian in the lab. In both cases, the goal is to modify the Hamiltonian such that emergent properties of the simulated system match a given target. We demonstrate how this can be achieved using differentiable simulations where bulk target observables and simulation outcomes can be analytically differentiated with respect to Hamiltonians, opening up new routes for parameterizing Hamiltonians to infer macroscopic models and develop control protocols.",
                    "Citation Paper Authors": "Authors:Wujie Wang, Simon Axelrod, Rafael G\u00f3mez-Bombarelli"
                }
            },
            {
                "Sentence ID": 20,
                "Sentence": ". Symplectic-ODE Net is a different approach that uses multiple neural networks to approximate\na scalar value to which they take the symplectic gradient and use adjoint sensitivity to update the\nparameters ",
                "Citation Text": "Yaofeng Desmond Zhong, Biswadip Dey, and Amit Chakraborty. Symplectic ode-net: Learning\nhamiltonian dynamics with control. arXiv preprint arXiv:1909.12077 , 2019.\n6",
                "Citation": {
                    "Citation Paper ID": "arXiv:1909.12077",
                    "Citation Paper Title": "Title:Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control",
                    "Citation Paper Abstract": "Abstract:In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system, given by an ordinary differential equation (ODE), from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.",
                    "Citation Paper Authors": "Authors:Yaofeng Desmond Zhong, Biswadip Dey, Amit Chakraborty"
                }
            },
            {
                "Sentence ID": 4,
                "Sentence": ".\nSymplectic-RNN attempts to improve on the stability of HNNs by introducing recurrent training ",
                "Citation Text": "Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, and L\u00e9on Bottou. Symplectic recurrent neural\nnetworks. arXiv preprint arXiv:1909.13334 , 2019.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1909.13334",
                    "Citation Paper Title": "Title:Symplectic Recurrent Neural Networks",
                    "Citation Paper Abstract": "Abstract:We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.",
                    "Citation Paper Authors": "Authors:Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, L\u00e9on Bottou"
                }
            },
            {
                "Sentence ID": 7,
                "Sentence": ". Hamiltonian Neural Networks (HNNs) predict a scalar value and take the\nsymplectic gradient in order to approximate a differential equation that conserves total energy ",
                "Citation Text": "Sam Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. CoRR ,\nabs/1906.01563, 2019.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1906.01563",
                    "Citation Paper Title": "Title:Hamiltonian Neural Networks",
                    "Citation Paper Abstract": "Abstract:Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.",
                    "Citation Paper Authors": "Authors:Sam Greydanus, Misko Dzamba, Jason Yosinski"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2109.09173v4": {
        "Paper Title": "Unravelling looping efficiency of stochastic Cosserat polymers",
        "Sentences": [
            {
                "Sentence ID": 66,
                "Sentence": "A. J. McKane and M. B. Tarlie. Regularization of func-\ntional determinants using boundary perturbations. J.\nPhys. A: Mathematical and General , 28(23):6931\u20136942,\ndec 1995. ",
                "Citation Text": "G. M. Falco, A. A. Fedorenko, and I. A. Gruzberg. On\nfunctional determinants of matrix di\ufb00erential operators\nwith multiple zero modes. J. Phys. A Math. Theor. ,\n50(48):485201, nov 2017.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1703.07329",
                    "Citation Paper Title": "Title:On functional determinants of matrix differential operators with multiple zero modes",
                    "Citation Paper Abstract": "Abstract:We generalize the method of computing functional determinants with a single excluded zero eigenvalue developed by McKane and Tarlie to differential operators with multiple zero eigenvalues. We derive general formulas for such functional determinants of $r\\times r$ matrix second order differential operators $O$ with $0 < n \\leqslant 2r$ linearly independent zero modes. We separately discuss the cases of the homogeneous Dirichlet boundary conditions, when the number of zero modes cannot exceed $r$, and the case of twisted boundary conditions, including the periodic and anti-periodic ones, when the number of zero modes is bounded above by $2r$. In all cases the determinants with excluded zero eigenvalues can be expressed only in terms of the $n$ zero modes and other $r-n$ or $2r-n$ (depending on the boundary conditions) solutions of the homogeneous equation $O h=0$, in the spirit of Gel'fand-Yaglom approach. In instanton calculations, the contribution of the zero modes is taken into account by introducing the so-called collective coordinates. We show that there is a remarkable cancellation of a factor (involving scalar products of zero modes) between the Jacobian of the transformation to the collective coordinates and the functional fluctuation determinant with excluded zero eigenvalues. This cancellation drastically simplifies instanton calculations when one uses our formulas.",
                    "Citation Paper Authors": "Authors:G.M. Falco, Andrei A. Fedorenko, Ilya A. Gruzberg"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1802.04249v5": {
        "Paper Title": "CoCoS: Fast and Accurate Distributed Triangle Counting in Graph Streams",
        "Sentences": [
            {
                "Sentence ID": 1,
                "Sentence": ", (parallel) Neighborhood\nSampling ( NS) [28,29,38], and Graph Sample and Hold ( GSH \ud835\udc47) ",
                "Citation Text": "Nesreen K Ahmed, Nick Duffield, Jennifer Neville, and Ramana Kompella. 2014. Graph sample and hold: A framework\nfor big-graph analytics. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and\ndata mining . ACM, 1446\u20131455.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1403.3909",
                    "Citation Paper Title": "Title:Graph Sample and Hold: A Framework for Big-Graph Analytics",
                    "Citation Paper Abstract": "Abstract:Sampling is a standard approach in big-graph analytics; the goal is to efficiently estimate the graph properties by consulting a sample of the whole population. A perfect sample is assumed to mirror every property of the whole population. Unfortunately, such a perfect sample is hard to collect in complex populations such as graphs (e.g. web graphs, social networks etc), where an underlying network connects the units of the population. Therefore, a good sample will be representative in the sense that graph properties of interest can be estimated with a known degree of accuracy. While previous work focused particularly on sampling schemes used to estimate certain graph properties (e.g. triangle count), much less is known for the case when we need to estimate various graph properties with the same sampling scheme. In this paper, we propose a generic stream sampling framework for big-graph analytics, called Graph Sample and Hold (gSH). To begin, the proposed framework samples from massive graphs sequentially in a single pass, one edge at a time, while maintaining a small state. We then show how to produce unbiased estimators for various graph properties from the sample. Given that the graph analysis algorithms will run on a sample instead of the whole population, the runtime complexity of these algorithm is kept under control. Moreover, given that the estimators of graph properties are unbiased, the approximation error is kept under control. Finally, we show the performance of the proposed framework (gSH) on various types of graphs, such as social graphs, among others.",
                    "Citation Paper Authors": "Authors:Nesreen K. Ahmed, Nick Duffield, Jennifer Neville, Ramana Kompella"
                }
            },
            {
                "Sentence ID": 9,
                "Sentence": "proposed REPT , which is a parallel version of Mascot in multi-core settings.\nEach processor maintains a separate sample of edges, while all processors update their estimates\nwhenever an edge arrives.\nDe Stefani et al. ",
                "Citation Text": "Lorenzo De Stefani, Alessandro Epasto, Matteo Riondato, and Eli Upfal. 2017. Tri\u00e8st: counting local and global triangles\nin fully dynamic streams with fixed memory size. ACM Transactions on Knowledge Discovery from Data 11, 4 (2017), 43.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1602.07424",
                    "Citation Paper Title": "Title:TRI\u00c8ST: Counting Local and Global Triangles in Fully-dynamic Streams with Fixed Memory Size",
                    "Citation Paper Abstract": "Abstract:We present TRI\u00c8ST, a suite of one-pass streaming algorithms to compute unbiased, low-variance, high-quality approximations of the global and local (i.e., incident to each vertex) number of triangles in a fully-dynamic graph represented as an adversarial stream of edge insertions and deletions. Our algorithms use reservoir sampling and its variants to exploit the user-specified memory space at all times. This is in contrast with previous approaches which use hard-to-choose parameters (e.g., a fixed sampling probability) and offer no guarantees on the amount of memory they will use. We show a full analysis of the variance of the estimations and novel concentration bounds for these quantities. Our experimental results on very large graphs show that TRI\u00c8ST outperforms state-of-the-art approaches in accuracy and exhibits a small update time.",
                    "Citation Paper Authors": "Authors:Lorenzo De Stefani, Alessandro Epasto, Matteo Riondato, Eli Upfal"
                }
            },
            {
                "Sentence ID": 3,
                "Sentence": ", Park et al. [ 24\u201327], and Arifuzzaman et al. ",
                "Citation Text": "Shaikh Arifuzzaman, Maleq Khan, and Madhav Marathe. 2013. PATRIC: A parallel algorithm for counting triangles in\nmassive networks. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management .\nACM, 529\u2013538.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1406.5687",
                    "Citation Paper Title": "Title:Parallel Algorithms for Counting Triangles in Networks with Large Degrees",
                    "Citation Paper Abstract": "Abstract:Finding the number of triangles in a network is an important problem in the analysis of complex networks. The number of triangles also has important applications in data mining. Existing distributed memory parallel algorithms for counting triangles are either Map-Reduce based or message passing interface (MPI) based and work with overlapping partitions of the given network. These algorithms are designed for very sparse networks and do not work well when the degrees of the nodes are relatively larger. For networks with larger degrees, Map-Reduce based algorithm generates prohibitively large intermediate data, and in MPI based algorithms with overlapping partitions, each partition can grow as large as the original network, wiping out the benefit of partitioning the network.\nIn this paper, we present two efficient MPI-based parallel algorithms for counting triangles in massive networks with large degrees. The first algorithm is a space-efficient algorithm for networks that do not fit in the main memory of a single compute node. This algorithm divides the network into non-overlapping partitions. The second algorithm is for the case where the main memory of each node is large enough to contain the entire network. We observe that for such a case, computation load can be balanced dynamically and present a dynamic load balancing scheme which improves the performance significantly. Both of our algorithms scale well to large networks and to a large number of processors.",
                    "Citation Paper Authors": "Authors:Shaikh Arifuzzaman, Maleq Khan, Madhav Marathe"
                }
            },
            {
                "Sentence ID": 38,
                "Sentence": "proposed sampling\nwedges (i.e., paths of length two) in addition to edges; and Ahmed et al. [ 1,2] proposed sampling\nedges with different probabilities, depending on the counts of adjacent sampled edges and incident\ntriangles. Tangwongsan et al. ",
                "Citation Text": "Kanat Tangwongsan, Aduri Pavan, and Srikanta Tirthapura. 2013. Parallel triangle counting in massive streaming\ngraphs. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management . ACM,\n781\u2013786.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1308.2166",
                    "Citation Paper Title": "Title:Parallel Triangle Counting in Massive Streaming Graphs",
                    "Citation Paper Abstract": "Abstract:The number of triangles in a graph is a fundamental metric, used in social network analysis, link classification and recommendation, and more. Driven by these applications and the trend that modern graph datasets are both large and dynamic, we present the design and implementation of a fast and cache-efficient parallel algorithm for estimating the number of triangles in a massive undirected graph whose edges arrive as a stream. It brings together the benefits of streaming algorithms and parallel algorithms. By building on the streaming algorithms framework, the algorithm has a small memory footprint. By leveraging the paralell cache-oblivious framework, it makes efficient use of the memory hierarchy of modern multicore machines without needing to know its specific parameters. We prove theoretical bounds on accuracy, memory access cost, and parallel runtime complexity, as well as showing empirically that the algorithm yields accurate results and substantial speedups compared to an optimized sequential implementation.\n(This is an expanded version of a CIKM'13 paper of the same title.)",
                    "Citation Paper Authors": "Authors:Kanat Tangwongsan, A. Pavan, Srikanta Tirthapura"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1802.01065v3": {
        "Paper Title": "Detecting Group Anomalies in Tera-Scale Multi-Aspect Data via\n  Dense-Subtensor Mining",
        "Sentences": [
            {
                "Sentence ID": 3,
                "Sentence": ", have been\nused for anomaly and fraud detection in graphs. See ",
                "Citation Text": "Leman Akoglu, Hanghang Tong, and Danai Koutra. Graph based anomaly detection and description: a\nsurvey.Data Mining and Knowledge Discovery , 29(3):626\u2013688, 2015.\n22",
                "Citation": {
                    "Citation Paper ID": "arXiv:1404.4679",
                    "Citation Paper Title": "Title:Graph-based Anomaly Detection and Description: A Survey",
                    "Citation Paper Abstract": "Abstract:Detecting anomalies in data is a vital task, with numerous high-impact applications in areas such as security, finance, health care, and law enforcement. While numerous techniques have been developed in past years for spotting outliers and anomalies in unstructured collections of multi-dimensional points, with graph data becoming ubiquitous, techniques for structured {\\em graph} data have been of focus recently. As objects in graphs have long-range correlations, a suite of novel technology has been developed for anomaly detection in graph data.\nThis survey aims to provide a general, comprehensive, and structured overview of the state-of-the-art methods for anomaly detection in data represented as graphs. As a key contribution, we provide a comprehensive exploration of both data mining and machine learning algorithms for these {\\em detection} tasks. we give a general framework for the algorithms categorized under various settings: unsupervised vs. (semi-)supervised approaches, for static vs. dynamic graphs, for attributed vs. plain graphs. We highlight the effectiveness, scalability, generality, and robustness aspects of the methods. What is more, we stress the importance of anomaly {\\em attribution} and highlight the major techniques that facilitate digging out the root cause, or the `why', of the detected anomalies for further analysis and sense-making. Finally, we present several real-world applications of graph-based anomaly detection in diverse domains, including financial, auction, computer traffic, and social networks. We conclude our survey with a discussion on open theoretical and practical challenges in the field.",
                    "Citation Paper Authors": "Authors:Leman Akoglu, Hanghang Tong, Danai Koutra"
                }
            },
            {
                "Sentence ID": 38,
                "Sentence": "starts from the output of M-Zoom and repeats adding or removing an attribute greedily until a local\noptimum is reached. Given a dynamic tensor, DenseAlert andDenseStream incrementally compute\na single dense subtensor in it ",
                "Citation Text": "Kijung Shin, Bryan Hooi, Jisu Kim, and Christos Faloutsos. Densealert: Incremental dense-subtensor\ndetection in tensor streams. In Proceedings of the 23rd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining , pages 1057\u20131066. ACM, 2017.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1706.03374",
                    "Citation Paper Title": "Title:DenseAlert: Incremental Dense-Subtensor Detection in Tensor Streams",
                    "Citation Paper Abstract": "Abstract:Consider a stream of retweet events - how can we spot fraudulent lock-step behavior in such multi-aspect data (i.e., tensors) evolving over time? Can we detect it in real time, with an accuracy guarantee? Past studies have shown that dense subtensors tend to indicate anomalous or even fraudulent behavior in many tensor data, including social media, Wikipedia, and TCP dumps. Thus, several algorithms have been proposed for detecting dense subtensors rapidly and accurately. However, existing algorithms assume that tensors are static, while many real-world tensors, including those mentioned above, evolve over time.\nWe propose DenseStream, an incremental algorithm that maintains and updates a dense subtensor in a tensor stream (i.e., a sequence of changes in a tensor), and DenseAlert, an incremental algorithm spotting the sudden appearances of dense subtensors. Our algorithms are: (1) Fast and 'any time': updates by our algorithms are up to a million times faster than the fastest batch algorithms, (2) Provably accurate: our algorithms guarantee a lower bound on the density of the subtensor they maintain, and (3) Effective: our DenseAlert successfully spots anomalies in real-world tensors, especially those overlooked by existing algorithms.",
                    "Citation Paper Authors": "Authors:Kijung Shin, Bryan Hooi, Jisu Kim, Christos Faloutsos"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1812.09526v4": {
        "Paper Title": "Functional Aggregate Queries with Additive Inequalities",
        "Sentences": [
            {
                "Sentence ID": 12,
                "Sentence": ". There are more recent improv ements to Chazelle\u2019s result (see, e.g.,\nChan et al. ",
                "Citation Text": "Chan, T. M., Larsen, K. G., and P \u02d8atras \u00b8cu, M. Orthogonal range searching on the ram, revisited.\nInSoCG(2011), pp. 1\u201310.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1103.5510",
                    "Citation Paper Title": "Title:Orthogonal Range Searching on the RAM, Revisited",
                    "Citation Paper Abstract": "Abstract:We present several new results on one of the most extensively studied topics in computational geometry, orthogonal range searching. All our results are in the standard word RAM model for points in rank space:\n** We present two data structures for 2-d orthogonal range emptiness. The first achieves O(n lglg n) space and O(lglg n) query time. This improves the previous results by Alstrup, Brodal, and Rauhe(FOCS'00), with O(n lg^eps n) space and O(lglg n) query time, or with O(nlglg n) space and O(lg^2 lg n) query time. Our second data structure uses O(n) space and answers queries in O(lg^eps n) time. The best previous O(n)-space data structure, due to Nekrich (WADS'07), answers queries in O(lg n/lglg n) time.\n** For 3-d orthogonal range reporting, we obtain space O(n lg^{1+eps} n) and query time O(lglg n + k), for any constant eps>0. This improves previous results by Afshani (ESA'08), Karpinski and Nekrich (COCOON'09), and Chan (SODA'11), with O(n lg^3 n) space and O(lglg n + k) query time, or with O(n lg^{1+eps} n) space and O(lg^2 lg n + k) query time. This implies improved bounds for orthogonal range reporting in all constant dimensions above 3.\n** We give a randomized algorithm for 4-d offline dominance range reporting/emptiness with running time O(n lg n + k). This resolves two open problems from Preparata and Shamos' seminal book:\n**** given n axis-aligned rectangles in the plane, we can report all k enclosure pairs in O(n lg n + k) expected time. The best known result was an O([n lg n + k] lglg n) algorithm from SoCG'95 by Gupta, Janardan, Smid, and Dasgupta.\n**** given n points in 4-d, we can find all maximal points in O(n lg n) expected time. The best previous result was an O(n lg n lglg n) algorithm due to Gabow, Bentley, and Tarjan (STOC'84). This implies record time bounds for the maxima problem in all constant dimensions above 4.",
                    "Citation Paper Authors": "Authors:Timothy M. Chan, Kasper Green Larsen, Mihai Patrascu"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1810.02935v2": {
        "Paper Title": "Towards Self-Tuning Parameter Servers",
        "Sentences": [
            {
                "Sentence ID": 28,
                "Sentence": ", which\npicks the next setting that can maximize the expected im-\nprovement over the current best; (iii) Upper Con\ufb01dence Bound\n(UCB) ",
                "Citation Text": "E. Contal, D. Buffoni, A. Robicquet, and N. Vayatis, \u201cParallel gaussian\nprocess optimization with upper con\ufb01dence bound and pure exploration,\u201d\ninECML PKDD , 2013, pp. 225\u2013240.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1304.5350",
                    "Citation Paper Title": "Title:Parallel Gaussian Process Optimization with Upper Confidence Bound and Pure Exploration",
                    "Citation Paper Abstract": "Abstract:In this paper, we consider the challenge of maximizing an unknown function f for which evaluations are noisy and are acquired with high cost. An iterative procedure uses the previous measures to actively select the next estimation of f which is predicted to be the most useful. We focus on the case where the function can be evaluated in parallel with batches of fixed size and analyze the benefit compared to the purely sequential procedure in terms of cumulative regret. We introduce the Gaussian Process Upper Confidence Bound and Pure Exploration algorithm (GP-UCB-PE) which combines the UCB strategy and Pure Exploration in the same batch of evaluations along the parallel iterations. We prove theoretical upper bounds on the regret with batches of size K for this procedure which show the improvement of the order of sqrt{K} for fixed iteration cost over purely sequential versions. Moreover, the multiplicative constants involved have the property of being dimension-free. We also confirm empirically the efficiency of GP-UCB-PE on real and synthetic problems compared to state-of-the-art competitors.",
                    "Citation Paper Authors": "Authors:Emile Contal, David Buffoni, Alexandre Robicquet, Nicolas Vayatis"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1703.04780v5": {
        "Paper Title": "Learning Models over Relational Data using Sparse Tensors and Functional\n  Dependencies",
        "Sentences": [
            {
                "Sentence ID": 20,
                "Sentence": "that support generalized linear models, Na\u007f \u0010ve Bayes classi\fcation, and respectively linear regression models with\ncontinuous features. This class also contains the recent e\u000borts on in-database linear algebra ",
                "Citation Text": "L. Chen, A. Kumar, J. F. Naughton, and J. M. Patel. Towards linear algebra over normalized data. PVLDB ,\n10(11):1214{1225, 2017.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1612.07448",
                    "Citation Paper Title": "Title:Towards Linear Algebra over Normalized Data",
                    "Citation Paper Abstract": "Abstract:Providing machine learning (ML) over relational data is a mainstream requirement for data analytics systems. While almost all the ML tools require the input data to be presented as a single table, many datasets are multi-table, which forces data scientists to join those tables first, leading to data redundancy and runtime waste. Recent works on \"factorized\" ML mitigate this issue for a few specific ML algorithms by pushing ML through joins. But their approaches require a manual rewrite of ML implementations. Such piecemeal methods create a massive development overhead when extending such ideas to other ML algorithms. In this paper, we show that it is possible to mitigate this overhead by leveraging a popular formal algebra to represent the computations of many ML algorithms: linear algebra. We introduce a new logical data type to represent normalized data and devise a framework of algebraic rewrite rules to convert a large set of linear algebra operations over denormalized data into operations over normalized data. We show how this enables us to automatically \"factorize\" several popular ML algorithms, thus unifying and generalizing several prior works. We prototype our framework in the popular ML environment R and an industrial R-over-RDBMS tool. Experiments with both synthetic and real normalized data show that our framework also yields significant speed-ups, up to 36x on real data.",
                    "Citation Paper Authors": "Authors:Lingjiao Chen, Arun Kumar, Jeffrey Naughton, Jignesh M. Patel"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1812.04379v2": {
        "Paper Title": "On the expressive power of linear algebra on graphs",
        "Sentences": [
            {
                "Sentence ID": 13,
                "Sentence": "for a similar approach). More\ngeneral semirings could open the way for modelling and querying labeled graphs using matrix\nquery languages (see also ",
                "Citation Text": "Robert Brijder, Marc Gyssens, and Jan Van den Bussche. On matrices and k-relations.\nCoRR , 2019. URL: http://arxiv.org/abs/1904.03934 .",
                "Citation": {
                    "Citation Paper ID": "arXiv:1904.03934",
                    "Citation Paper Title": "Title:On matrices and $K$-relations",
                    "Citation Paper Abstract": "Abstract:We show that the matrix query language $\\mathsf{MATLANG}$ corresponds to a natural fragment of the positive relational algebra on $K$-relations. The fragment is defined by introducing a composition operator and restricting $K$-relation arities to two. We then proceed to show that $\\mathsf{MATLANG}$ can express all matrix queries expressible in the positive relational algebra on $K$-relations, when intermediate arities are restricted to three. Thus we offer an analogue, in a model with numerical data, to the situation in classical logic, where the algebra of binary relations is equivalent to first-order logic with three variables.",
                    "Citation Paper Authors": "Authors:Robert Brijder, Marc Gyssens, Jan Van den Bussche"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1806.07722v3": {
        "Paper Title": "Stylized innovation: generating timelines by interrogating incrementally\n  available randomised dictionaries",
        "Sentences": [
            {
                "Sentence ID": 4,
                "Sentence": "Y . Liu, C. Mathis, M. D. Bajczyk, S. M. Marshall, L. Wilbraham,\nand L. Cronin,\nExploring and mapping chemical space with molecular assembly\ntrees,\nScience Advances 7, eabj2465 (2021). ",
                "Citation Text": "V . Sood, M. Mathieu, A. Shreim, P. Grassberger, and\nM. Paczuski,\nInteracting branching process as a simple model of innovation,\nPhys. Rev. Lett. 105, 178701 (2010).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1003.5797",
                    "Citation Paper Title": "Title:The Interacting Branching Process as a Simple Model of Innovation",
                    "Citation Paper Abstract": "Abstract:We describe innovation in terms of a generalized branching process. Each new invention pairs with any existing one to produce a number of offspring, which is Poisson distributed with mean p. Existing inventions die with probability p/\\tau at each generation. In contrast to mean field results, no phase transition occurs; the chance for survival is finite for all p > 0. For \\tau = \\infty, surviving processes exhibit a bottleneck before exploding super-exponentially - a growth consistent with a law of accelerating returns. This behavior persists for finite \\tau. We analyze, in detail, the asymptotic behavior as p \\to 0.",
                    "Citation Paper Authors": "Authors:Vishal Sood, Myl\u00e9ne Mathieu, Amer Shreim, Peter Grassberger, Maya Paczuski"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1706.03762v7": {
        "Paper Title": "Attention Is All You Need",
        "Sentences": [
            {
                "Sentence ID": 23,
                "Sentence": "semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) ",
                "Citation Text": "Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1511.06114",
                    "Citation Paper Title": "Title:Multi-task Sequence to Sequence Learning",
                    "Citation Paper Abstract": "Abstract:Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.",
                    "Citation Paper Authors": "Authors:Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser"
                }
            },
            {
                "Sentence ID": 1,
                "Sentence": "around each of\nthe two sub-layers, followed by layer normalization ",
                "Citation Text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1607.06450",
                    "Citation Paper Title": "Title:Layer Normalization",
                    "Citation Paper Abstract": "Abstract:Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
                    "Citation Paper Authors": "Authors:Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1708.09667v3": {
        "Paper Title": "Video Captioning with Guidance of Multimodal Latent Topics",
        "Sentences": [
            {
                "Sentence ID": 25,
                "Sentence": "propose the tempo-\nral aention mechanism, and Pan et al. ",
                "Citation Text": "Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, and Yueting Zhuang. 2016. Hier-\narchical recurrent neural encoder for video representation with application to\ncaptioning. In CVPR . 1029\u20131038.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1511.03476",
                    "Citation Paper Title": "Title:Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning",
                    "Citation Paper Abstract": "Abstract:Recently, deep learning approach, especially deep Convolutional Neural Networks (ConvNets), have achieved overwhelming accuracy with fast processing speed for image classification. Incorporating temporal structure with deep ConvNets for video representation becomes a fundamental problem for video content analysis. In this paper, we propose a new approach, namely Hierarchical Recurrent Neural Encoder (HRNE), to exploit temporal information of videos. Compared to recent video representation inference approaches, this paper makes the following three contributions. First, our HRNE is able to efficiently exploit video temporal structure in a longer range by reducing the length of input information flow, and compositing multiple consecutive inputs at a higher level. Second, computation operations are significantly lessened while attaining more non-linearity. Third, HRNE is able to uncover temporal transitions between frame chunks with different granularities, i.e., it can model the temporal transitions between frames as well as the transitions between segments. We apply the new method to video captioning where temporal information plays a crucial role. Experiments demonstrate that our method outperforms the state-of-the-art on video captioning benchmarks. Notably, even using a single network with only RGB stream as input, HRNE beats all the recent systems which combine multiple inputs, such as RGB ConvNet plus 3D ConvNet.",
                    "Citation Paper Authors": "Authors:Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, Yueting Zhuang"
                }
            },
            {
                "Sentence ID": 40,
                "Sentence": "41.40 28.30 48.90\nYoutube\n2TextM&M TGM 48.76 34.36 80.45\nLSTM-YT ",
                "Citation Text": "Subhashini Venugopalan, Huijuan Xu, Je Donahue, Marcus Rohrbach, Raymond\nMooney, and Kate Saenko. 2014. Translating Videos to Natural Language Using\nDeep Recurrent Neural Networks. Computer Science (2014).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1412.4729",
                    "Citation Paper Title": "Title:Translating Videos to Natural Language Using Deep Recurrent Neural Networks",
                    "Citation Paper Abstract": "Abstract:Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.",
                    "Citation Paper Authors": "Authors:Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko"
                }
            },
            {
                "Sentence ID": 33,
                "Sentence": "utilize the textual mined topics tolearn interpretable features. Shen et al. ",
                "Citation Text": "Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, and\nXiangyang Xue. 2017. Weakly Supervised Dense Video Captioning. In CVPR .",
                "Citation": {
                    "Citation Paper ID": "arXiv:1704.01502",
                    "Citation Paper Title": "Title:Weakly Supervised Dense Video Captioning",
                    "Citation Paper Abstract": "Abstract:This paper focuses on a novel and challenging vision task, dense video captioning, which aims to automatically describe a video clip with multiple informative and diverse caption sentences. The proposed method is trained without explicit annotation of fine-grained sentence to video region-sequence correspondence, but is only based on weak video-level sentence annotations. It differs from existing video captioning systems in three technical aspects. First, we propose lexical fully convolutional neural networks (Lexical-FCN) with weakly supervised multi-instance multi-label learning to weakly link video regions with lexical labels. Second, we introduce a novel submodular maximization scheme to generate multiple informative and diverse region-sequences based on the Lexical-FCN outputs. A winner-takes-all scheme is adopted to weakly associate sentences to region-sequences in the training phase. Third, a sequence-to-sequence learning based language model is trained with the weakly supervised information obtained through the association process. We show that the proposed method can not only produce informative and diverse dense captions, but also outperform state-of-the-art single video captioning methods by a large margin.",
                    "Citation Paper Authors": "Authors:Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, Xiangyang Xue"
                }
            },
            {
                "Sentence ID": 46,
                "Sentence": "propose the spa-\ntial aention mechanism based on the basic encoder-decoder. You\net al. ",
                "Citation Text": "anzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. 2016.\nImage captioning with semantic aention. arXiv:1603.03925 (2016).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1603.03925",
                    "Citation Paper Title": "Title:Image Captioning with Semantic Attention",
                    "Citation Paper Abstract": "Abstract:Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.",
                    "Citation Paper Authors": "Authors:Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo"
                }
            },
            {
                "Sentence ID": 41,
                "Sentence": "(LSTM), is utilized to generate sequential words\nconditioned on image features ",
                "Citation Text": "Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show\nand tell: A neural image caption generator. In CVPR . 3156\u20133164.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1411.4555",
                    "Citation Paper Title": "Title:Show and Tell: A Neural Image Caption Generator",
                    "Citation Paper Abstract": "Abstract:Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
                    "Citation Paper Authors": "Authors:Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1812.02303v4": {
        "Paper Title": "Neural Abstractive Text Summarization with Sequence-to-Sequence Models",
        "Sentences": [
            {
                "Sentence ID": 45,
                "Sentence": "(https://github.com/JafferWilson/Process-\nData-of-CNN-DailyMail).\n5.2.2 Newsroom Dataset. The Cornell Newsroom dataset (https://summari.es/) ",
                "Citation Text": "Max Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A Dataset of 1.3 Million Summaries with Diverse\nExtractive Strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , Vol. 1. 708\u2013719.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1804.11283",
                    "Citation Paper Title": "Title:Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies",
                    "Citation Paper Abstract": "Abstract:We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges.",
                    "Citation Paper Authors": "Authors:Max Grusky, Mor Naaman, Yoav Artzi"
                }
            },
            {
                "Sentence ID": 110,
                "Sentence": ". It has also been demonstrated to be effective in\nimproving the performance of seq2seq models for the task of abstractive text summarization ",
                "Citation Text": "Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summarization.\narXiv preprint arXiv:1705.04304 (2017).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1705.04304",
                    "Citation Paper Title": "Title:A Deep Reinforced Model for Abstractive Summarization",
                    "Citation Paper Abstract": "Abstract:Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit \"exposure bias\" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.",
                    "Citation Paper Authors": "Authors:Romain Paulus, Caiming Xiong, Richard Socher"
                }
            },
            {
                "Sentence ID": 114,
                "Sentence": ". In practice, the gradient\nwith the baseline is approximated with\n\u2207\u03b8L\u03b8\u2248\u2212(R(y\u03c0\n<T+1)\u2212b)T\u00d5\nt=1\u2207\u03b8logP\u03b8(y\u03c0\nt|y\u03c0\n<t,x) (39)\nBetter ways of sampling a sequence and different approaches to calculate the baseline can be found\nin [63, 114, 148, 152].\n3.2.2 MIXER ",
                "Citation Text": "Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level training with\nrecurrent neural networks. arXiv preprint arXiv:1511.06732 (2015).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1511.06732",
                    "Citation Paper Title": "Title:Sequence Level Training with Recurrent Neural Networks",
                    "Citation Paper Abstract": "Abstract:Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.",
                    "Citation Paper Authors": "Authors:Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, Wojciech Zaremba"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1809.06297v2": {
        "Paper Title": "Adversarial Text Generation via Feature-Mover's Distance",
        "Sentences": [
            {
                "Sentence ID": 62,
                "Sentence": "uses GANs to tackle the task of unsupervised cipher cracking, utilizing the frame-\nwork of CycleGAN ",
                "Citation Text": "J. Zhu, T. Park, P. Isola, and A. Efros. Unpaired image-to-image translation using cycle-\nconsistent adversarial networks. In ICCV , 2017.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1703.10593",
                    "Citation Paper Title": "Title:Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
                    "Citation Paper Abstract": "Abstract:Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$ such that the distribution of images from $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$ and introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",
                    "Citation Paper Authors": "Authors:Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros"
                }
            },
            {
                "Sentence ID": 26,
                "Sentence": ". Moreover, our model even\noutperforms the controllable text generation method ",
                "Citation Text": "Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing. Toward controlled generation of\ntext. In ICML , 2017.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1703.00955",
                    "Citation Paper Title": "Title:Toward Controlled Generation of Text",
                    "Citation Paper Abstract": "Abstract:Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.",
                    "Citation Paper Authors": "Authors:Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P. Xing"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1801.06353v4": {
        "Paper Title": "Transfer Learning for Improving Speech Emotion Classification Accuracy",
        "Sentences": [
            {
                "Sentence ID": 21,
                "Sentence": ".\nApart from DNNs, researchers have also used interesting\ndeep architectures for transfer learning. In ",
                "Citation Text": "J. Gideon, S. Khorram, Z. Aldeneh, D. Dimitriadis, and E . M.\nProvost, \u201cProgressive neural networks for transfer learni ng in\nemotion recognition,\u201d arXiv preprint arXiv:1706.03256 , 2017.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1706.03256",
                    "Citation Paper Title": "Title:Progressive Neural Networks for Transfer Learning in Emotion Recognition",
                    "Citation Paper Abstract": "Abstract:Many paralinguistic tasks are closely related and thus representations learned in one domain can be leveraged for another. In this paper, we investigate how knowledge can be transferred between three paralinguistic tasks: speaker, emotion, and gender recognition. Further, we extend this problem to cross-dataset tasks, asking how knowledge captured in one emotion dataset can be transferred to another. We focus on progressive neural networks and compare these networks to the conventional deep learning method of pre-training and fine-tuning. Progressive neural networks provide a way to transfer knowledge and avoid the forgetting effect present when pre-training neural networks on different tasks. Our experiments demonstrate that: (1) emotion recognition can benefit from using representations originally learned for different paralinguistic tasks and (2) transfer learning can effectively leverage additional datasets to improve the performance of emotion recognition systems.",
                    "Citation Paper Authors": "Authors:John Gideon, Soheil Khorram, Zakaria Aldeneh, Dimitrios Dimitriadis, Emily Mower Provost"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1809.10044v2": {
        "Paper Title": "No One is Perfect: Analysing the Performance of Question Answering\n  Components over the DBpedia Knowledge Graph",
        "Sentences": [
            {
                "Sentence ID": 9,
                "Sentence": "is another recently released datasetthat contains 5,000 questions for DBpedia and has been used for\nevaluating question answering systems ",
                "Citation Text": "Dennis Diefenbach, Andreas Both, Kamal Singh, and Pierre Maret. 2018. To-\nwards a Question Answering System over the Semantic Web. arXiv preprint\narXiv:1803.00832 (2018).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1803.00832",
                    "Citation Paper Title": "Title:Towards a Question Answering System over the Semantic Web",
                    "Citation Paper Abstract": "Abstract:Thanks to the development of the Semantic Web, a lot of new structured data has become available on the Web in the form of knowledge bases (KBs). Making this valuable data accessible and usable for end-users is one of the main goals of Question Answering (QA) over KBs. Most current QA systems query one KB, in one language (namely English). The existing approaches are not designed to be easily adaptable to new KBs and languages. We first introduce a new approach for translating natural language questions to SPARQL queries. It is able to query several KBs simultaneously, in different languages, and can easily be ported to other KBs and languages. In our evaluation, the impact of our approach is proven using 5 different well-known and large KBs: Wikidata, DBpedia, MusicBrainz, DBLP and Freebase as well as 5 different languages namely English, German, French, Italian and Spanish. Second, we show how we integrated our approach, to make it easily accessible by the research community and by end-users. To summarize, we provided a conceptional solution for multilingual, KB-agnostic Question Answering over the Semantic Web. The provided first approximation validates this concept.",
                    "Citation Paper Authors": "Authors:Dennis Diefenbach, Andreas Both, Kamal Singh, Pierre Maret"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1801.07311v3": {
        "Paper Title": "Early Detection of Social Media Hoaxes at Scale",
        "Sentences": [
            {
                "Sentence ID": 32,
                "Sentence": ", among others. As a state-of-the-art approach for semantic word\nrepresentation, here we make use of word embeddings ",
                "Citation Text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their\ncompositionality. In Advances in neural information processing systems . 3111\u20133119.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1310.4546",
                    "Citation Paper Title": "Title:Distributed Representations of Words and Phrases and their Compositionality",
                    "Citation Paper Abstract": "Abstract:The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                    "Citation Paper Authors": "Authors:Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1805.07745v6": {
        "Paper Title": "Abstractive Text Classification Using Sequence-to-convolution Neural\n  Networks",
        "Sentences": [
            {
                "Sentence ID": 38,
                "Sentence": "and processed the summary with crop&pad . Same method was used to generate\nsummary example used in training Sequential Block.\n4.1 Datasets\nWe evaluated our model on three different datasets: AG\u2019s News, DBPedia, and Yahoo Answers ",
                "Citation Text": "Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for\ntext classi\ufb01cation. CoRR , abs/1509.01626, 2015.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1509.01626",
                    "Citation Paper Title": "Title:Character-level Convolutional Networks for Text Classification",
                    "Citation Paper Abstract": "Abstract:This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.",
                    "Citation Paper Authors": "Authors:Xiang Zhang, Junbo Zhao, Yann LeCun"
                }
            },
            {
                "Sentence ID": 39,
                "Sentence": ". Convolution Block can be replaced\nwith other text classi\ufb01cation models such as C-LSTM ",
                "Citation Text": "Chunting Zhou, Chonglin Sun, Zhiyuan Liu, and Francis C. M. Lau. A C-LSTM neural network\nfor text classi\ufb01cation. CoRR , abs/1511.08630, 2015.\n10",
                "Citation": {
                    "Citation Paper ID": "arXiv:1511.08630",
                    "Citation Paper Title": "Title:A C-LSTM Neural Network for Text Classification",
                    "Citation Paper Abstract": "Abstract:Neural network models have been demonstrated to be capable of achieving remarkable performance in sentence and document modeling. Convolutional neural network (CNN) and recurrent neural network (RNN) are two mainstream architectures for such modeling tasks, which adopt totally different ways of understanding natural languages. In this work, we combine the strengths of both architectures and propose a novel and unified model called C-LSTM for sentence representation and text classification. C-LSTM utilizes CNN to extract a sequence of higher-level phrase representations, and are fed into a long short-term memory recurrent neural network (LSTM) to obtain the sentence representation. C-LSTM is able to capture both local features of phrases as well as global and temporal sentence semantics. We evaluate the proposed architecture on sentiment classification and question classification tasks. The experimental results show that the C-LSTM outperforms both CNN and LSTM and can achieve excellent performance on these tasks.",
                    "Citation Paper Authors": "Authors:Chunting Zhou, Chonglin Sun, Zhiyuan Liu, Francis C.M. Lau"
                }
            },
            {
                "Sentence ID": 1,
                "Sentence": "who were the \ufb01rst to use Seq2seq RNNs and Attention model for\nabstractive text summarization. Our Tensor\ufb02ow ",
                "Citation Text": "Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,\nGreg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\nAndrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,\nManjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek\nMurray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal\nTalwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete\nWarden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-\nscale machine learning on heterogeneous systems, 2015. Software available from tensor\ufb02ow.org.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1603.04467",
                    "Citation Paper Title": "Title:TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
                    "Citation Paper Abstract": "Abstract:TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at this http URL.",
                    "Citation Paper Authors": "Authors:Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, Xiaoqiang Zheng"
                }
            },
            {
                "Sentence ID": 12,
                "Sentence": "with gradient norm limited to 5. We\nset loss balancing weight \u0015to 1.0 when training AG\u2019s News and 2.0 for other datasets.\nWe initialized convolution layers using He Normal ",
                "Citation Text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into recti\ufb01ers:\nSurpassing human-level performance on imagenet classi\ufb01cation. CoRR , abs/1502.01852, 2015.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1502.01852",
                    "Citation Paper Title": "Title:Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
                    "Citation Paper Abstract": "Abstract:Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
                    "Citation Paper Authors": "Authors:Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun"
                }
            },
            {
                "Sentence ID": 24,
                "Sentence": "to improve the performance of our Sequential Block.\nOur approach to use Attentional Encoder-Decoder RNNs for abstractive summarization is closely\nrelated to Nallapati et al. ",
                "Citation Text": "Ramesh Nallapati, Bing Xiang, and Bowen Zhou. Sequence-to-sequence rnns for text summa-\nrization. CoRR , abs/1602.06023, 2016.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1602.06023",
                    "Citation Paper Title": "Title:Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond",
                    "Citation Paper Abstract": "Abstract:In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.",
                    "Citation Paper Authors": "Authors:Ramesh Nallapati, Bowen Zhou, Cicero Nogueira dos santos, Caglar Gulcehre, Bing Xiang"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1911.02621v3": {
        "Paper Title": "The Threat of Adversarial Attacks on Machine Learning in Network\n  Security -- A Survey",
        "Sentences": [
            {
                "Sentence ID": 44,
                "Sentence": ". However, the \ufb01rst notable discovery in\nadversarial attacks for computer vision was by Szegedy et\nal. ",
                "Citation Text": "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfel-\nlow, and R. Fergus, \u201cIntriguing properties of neural networks,\u201d arXiv\npreprint arXiv:1312.6199 , 2013.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1312.6199",
                    "Citation Paper Title": "Title:Intriguing properties of neural networks",
                    "Citation Paper Abstract": "Abstract:Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.\nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.\nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
                    "Citation Paper Authors": "Authors:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus"
                }
            },
            {
                "Sentence ID": 36,
                "Sentence": ".\n\u000fURLNet -Learning a URL Representation with Deep\nLearning for Malicious URL Detection Le et al. ",
                "Citation Text": "H. Le, Q. Pham, D. Sahoo, and S. C. Hoi, \u201cUrlnet: Learning a url\nrepresentation with deep learning for malicious url detection,\u201d arXiv\npreprint arXiv:1802.03162 , 2018.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1802.03162",
                    "Citation Paper Title": "Title:URLNet: Learning a URL Representation with Deep Learning for Malicious URL Detection",
                    "Citation Paper Abstract": "Abstract:Malicious URLs host unsolicited content and are used to perpetrate cybercrimes. It is imperative to detect them in a timely manner. Traditionally, this is done through the usage of blacklists, which cannot be exhaustive, and cannot detect newly generated malicious URLs. To address this, recent years have witnessed several efforts to perform Malicious URL Detection using Machine Learning. The most popular and scalable approaches use lexical properties of the URL string by extracting Bag-of-words like features, followed by applying machine learning models such as SVMs. There are also other features designed by experts to improve the prediction performance of the model. These approaches suffer from several limitations: (i) Inability to effectively capture semantic meaning and sequential patterns in URL strings; (ii) Requiring substantial manual feature engineering; and (iii) Inability to handle unseen features and generalize to test data. To address these challenges, we propose URLNet, an end-to-end deep learning framework to learn a nonlinear URL embedding for Malicious URL Detection directly from the URL. Specifically, we apply Convolutional Neural Networks to both characters and words of the URL String to learn the URL embedding in a jointly optimized framework. This approach allows the model to capture several types of semantic information, which was not possible by the existing models. We also propose advanced word-embeddings to solve the problem of too many rare words observed in this task. We conduct extensive experiments on a large-scale dataset and show a significant performance gain over existing methods. We also conduct ablation studies to evaluate the performance of various components of URLNet.",
                    "Citation Paper Authors": "Authors:Hung Le, Quang Pham, Doyen Sahoo, Steven C.H. Hoi"
                }
            },
            {
                "Sentence ID": 27,
                "Sentence": ". Signature based IDS detects\nattacks based on the repository of attacks signatures with no\nfalse alarm ",
                "Citation Text": "M. Almseidin, M. Alzubi, S. Kovacs, and M. Alkasassbeh, \u201cEvalua-\ntion of machine learning algorithms for intrusion detection system,\u201d\ninIntelligent Systems and Informatics (SISY), 2017 IEEE 15th\nInternational Symposium on , pp. 000277\u2013000282, IEEE, 2017.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1801.02330",
                    "Citation Paper Title": "Title:Evaluation of Machine Learning Algorithms for Intrusion Detection System",
                    "Citation Paper Abstract": "Abstract:Intrusion detection system (IDS) is one of the implemented solutions against harmful attacks. Furthermore, attackers always keep changing their tools and techniques. However, implementing an accepted IDS system is also a challenging task. In this paper, several experiments have been performed and evaluated to assess various machine learning classifiers based on KDD intrusion dataset. It succeeded to compute several performance metrics in order to evaluate the selected classifiers. The focus was on false negative and false positive performance metrics in order to enhance the detection rate of the intrusion detection system. The implemented experiments demonstrated that the decision table classifier achieved the lowest value of false negative while the random forest classifier has achieved the highest average accuracy rate.",
                    "Citation Paper Authors": "Authors:Mohammad Almseidin, Maen Alzubi, Szilveszter Kovacs, Mouhammd Alkasassbeh"
                }
            },
            {
                "Sentence ID": 23,
                "Sentence": "in which\nvarious adversarial attacks and defenses in images, graphs\nand texts were reviewed. In the \ufb01eld of natural language\nprocessing, zhang et.al ",
                "Citation Text": "W. E. Zhang, Q. Z. Sheng, A. Alhazmi, and C. Li, \u201cAdversarial\nattacks on deep-learning models in natural language processing: A\nsurvey,\u201d ACM Transactions on Intelligent Systems and Technology\n(TIST) , vol. 11, no. 3, pp. 1\u201341, 2020.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1901.06796",
                    "Citation Paper Title": "Title:Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey",
                    "Citation Paper Abstract": "Abstract:With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs were vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples for image DNNs, research efforts on attacking DNNs for textual applications emerges in recent years. However, existing perturbation methods for images cannotbe directly applied to texts as text data is discrete. In this article, we review research works that address this difference and generatetextual adversarial examples on DNNs. We collect, select, summarize, discuss and analyze these works in a comprehensive way andcover all the related information to make the article self-contained. Finally, drawing on the reviewed literature, we provide further discussions and suggestions on this topic.",
                    "Citation Paper Authors": "Authors:Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, Chenliang Li"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1909.11261v3": {
        "Paper Title": "Practical Low Latency Proof of Work Consensus",
        "Sentences": [
            {
                "Sentence ID": 6,
                "Sentence": ". However, how to scale the latency has\nbeen a major challenge in blockchain research for years. Prism++\nborrows an idea from Prism ",
                "Citation Text": "Vivek Bagaria, Sreeram Kannan, David Tse, Giulia Fanti, and Pramod Viswanath.\nPrism: Deconstructing the blockchain to approach physical limits. In Proceedings\nof the 2019 ACM SIGSAC Conference on Computer and Communications Security ,\nCCS \u201919, page 585\u2013602, New York, NY, USA, 2019. Association for Computing\nMachinery.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1810.08092",
                    "Citation Paper Title": "Title:Deconstructing the Blockchain to Approach Physical Limits",
                    "Citation Paper Abstract": "Abstract:Transaction throughput, confirmation latency and confirmation reliability are fundamental performance measures of any blockchain system in addition to its security. In a decentralized setting, these measures are limited by two underlying physical network attributes: communication capacity and speed-of-light propagation delay. Existing systems operate far away from these physical limits. In this work we introduce Prism, a new proof-of-work blockchain protocol, which can achieve 1) security against up to 50% adversarial hashing power; 2) optimal throughput up to the capacity C of the network; 3) confirmation latency for honest transactions proportional to the propagation delay D, with confirmation error probability exponentially small in CD ; 4) eventual total ordering of all transactions. Our approach to the design of this protocol is based on deconstructing the blockchain into its basic functionalities and systematically scaling up these functionalities to approach their physical limits.",
                    "Citation Paper Authors": "Authors:Vivek Bagaria, Sreeram Kannan, David Tse, Giulia Fanti, Pramod Viswanath"
                }
            },
            {
                "Sentence ID": 14,
                "Sentence": "Therearebroadlythreedifferentapproachestoscaletheperformance\nof blockchains. First, on-chain scaling aims to design consensus pro-\ntocols with inherently high throughput and low latency. Protocols\nsuch as Bitcoin-NG ",
                "Citation Text": "Ittay Eyal, Adem Efe Gencer, Emin Gun Sirer, and Robbert Van Renesse. Bitcoin-ng:\nA scalable blockchain protocol. In 13th USENIX Symposium on Networked Systems\nDesign and Implementation (NSDI 16) , pages 45\u201359, Santa Clara, CA, March 2016.\nUSENIX Association.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1510.02037",
                    "Citation Paper Title": "Title:Bitcoin-NG: A Scalable Blockchain Protocol",
                    "Citation Paper Abstract": "Abstract:Cryptocurrencies, based on and led by Bitcoin, have shown promise as infrastructure for pseudonymous online payments, cheap remittance, trustless digital asset exchange, and smart contracts. However, Bitcoin-derived blockchain protocols have inherent scalability limits that trade-off between throughput and latency and withhold the realization of this potential.\nThis paper presents Bitcoin-NG, a new blockchain protocol designed to scale. Based on Bitcoin's blockchain protocol, Bitcoin-NG is Byzantine fault tolerant, is robust to extreme churn, and shares the same trust model obviating qualitative changes to the ecosystem.\nIn addition to Bitcoin-NG, we introduce several novel metrics of interest in quantifying the security and efficiency of Bitcoin-like blockchain protocols. We implement Bitcoin-NG and perform large-scale experiments at 15% the size of the operational Bitcoin system, using unchanged clients of both protocols. These experiments demonstrate that Bitcoin-NG scales optimally, with bandwidth limited only by the capacity of the individual nodes and latency limited only by the propagation time of the network.",
                    "Citation Paper Authors": "Authors:Ittay Eyal, Adem Efe Gencer, Emin Gun Sirer, Robbert van Renesse"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1910.08634v3": {
        "Paper Title": "Universal Composability is Robust Compilation",
        "Sentences": [
            {
                "Sentence ID": 15,
                "Sentence": ". We believe\nourconnectiondoesnotneedtoaccountforsourceandtargettraced i\ufb00erencesince UCdealswith\nthe samelanguageforprotocolsandfunctionalities.\nAnovelcriterionforsecurecompilation, RChasbeencomparedtoexistingsecurecompilation\nnotionssuchasfully-abstractcompilation ",
                "Citation Text": "Carmine Abate, Matteo Busi, and Stelios Tsampas. 2021. Fully Abstract and Robust Compilation: And How to Rec-\noncile the Two, Abstractly. In Programming Languages and Systems - 19th Asian Symposium, A PLAS 2021, Chicago,\nIL, USA, October 17-18, 2021, Proceedings (Lecture Notes in Computer Science, Vol. 13008) , Hakjoo Oh (Ed.). Springer,\n83\u2013101. https://doi.org/10.1007/978-3-030-89051-3_6",
                "Citation": {
                    "Citation Paper ID": "arXiv:2006.14969",
                    "Citation Paper Title": "Title:Fully Abstract and Robust Compilation and How to Reconcile the Two, Abstractly",
                    "Citation Paper Abstract": "Abstract:The most prominent formal criterion for secure compilation is full abstraction, the preservation and reflection of contextual equivalence. Recent work introduced robust compilation, defined as the preservation of robust satisfaction of hyperproperties, i.e., their satisfaction against arbitrary attackers. In this paper, we initially set out to compare these two approaches to secure compilation. To that end, we provide an exact description of the hyperproperties that are robustly satisfied by programs compiled with a fully abstract compiler, and show that they can be meaningless or trivial. We then propose a novel criterion for secure compilation formulated in the framework of Mathematical Operational Semantics (MOS), guaranteeing both full abstraction and the preservation of robust satisfaction of hyperproperties in a more sensible manner.",
                    "Citation Paper Authors": "Authors:Carmine Abate, Matteo Busi, Stelios Tsampas"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1909.10480v4": {
        "Paper Title": "FENCE: Feasible Evasion Attacks on Neural Networks in Constrained\n  Environments",
        "Sentences": [
            {
                "Sentence ID": 46,
                "Sentence": ".\nDefenses against evasion attacks. Standard methods to defend against adversarial evasion\nattacks in continuous domains include: adversarial training ",
                "Citation Text": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017. Towards deep\nlearning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 (2017).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1706.06083",
                    "Citation Paper Title": "Title:Towards Deep Learning Models Resistant to Adversarial Attacks",
                    "Citation Paper Abstract": "Abstract:Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
                    "Citation Paper Authors": "Authors:Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu"
                }
            },
            {
                "Sentence ID": 43,
                "Sentence": "generate adversarial examples for\ndenial of service attacks. Lin et al. ",
                "Citation Text": "Zilong Lin, Yong Shi, and Zhi Xue. 2018. Idsgan: Generative adversarial networks for attack generation against\nintrusion detection. arXiv preprint arXiv:1809.02077 (2018).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1809.02077",
                    "Citation Paper Title": "Title:IDSGAN: Generative Adversarial Networks for Attack Generation against Intrusion Detection",
                    "Citation Paper Abstract": "Abstract:As an essential tool in security, the intrusion detection system bears the responsibility of the defense to network attacks performed by malicious traffic. Nowadays, with the help of machine learning algorithms, intrusion detection systems develop rapidly. However, the robustness of this system is questionable when it faces adversarial attacks. For the robustness of detection systems, more potential attack approaches are under research. In this paper, a framework of the generative adversarial networks, called IDSGAN, is proposed to generate the adversarial malicious traffic records aiming to attack intrusion detection systems by deceiving and evading the detection. Given that the internal structure and parameters of the detection system are unknown to attackers, the adversarial attack examples perform the black-box attacks against the detection system. IDSGAN leverages a generator to transform original malicious traffic records into adversarial malicious ones. A discriminator classifies traffic examples and dynamically learns the real-time black-box detection system. More significantly, the restricted modification mechanism is designed for the adversarial generation to preserve original attack functionalities of adversarial traffic records. The effectiveness of the model is indicated by attacking multiple algorithm-based detection models with different attack categories. The robustness is verified by changing the number of the modified features. A comparative experiment with adversarial attack baselines demonstrates the superiority of our model.",
                    "Citation Paper Authors": "Authors:Zilong Lin, Yong Shi, Zhi Xue"
                }
            },
            {
                "Sentence ID": 16,
                "Sentence": "analyzing the ro-\nbustness of random forest for botnet classification; Clements et al. ",
                "Citation Text": "Joseph Clements, Yuzhe Yang, Ankur Sharma, Hongxin Hu, and Yingjie Lao. 2019. Rallying Adversarial Techniques\nagainst Deep Learning for Network Security. arXiv preprint arXiv:1903.11688 (2019).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1903.11688",
                    "Citation Paper Title": "Title:Rallying Adversarial Techniques against Deep Learning for Network Security",
                    "Citation Paper Abstract": "Abstract:Recent advances in artificial intelligence and the increasing need for powerful defensive measures in the domain of network security, have led to the adoption of deep learning approaches for use in network intrusion detection systems. These methods have achieved superior performance against conventional network attacks, which enable the deployment of practical security systems to unique and dynamic sectors. Adversarial machine learning, unfortunately, has recently shown that deep learning models are inherently vulnerable to adversarial modifications on their input data. Because of this susceptibility, the deep learning models deployed to power a network defense could in fact be the weakest entry point for compromising a network system. In this paper, we show that by modifying on average as little as 1.38 of the input features, an adversary can generate malicious inputs which effectively fool a deep learning based NIDS. Therefore, when designing such systems, it is crucial to consider the performance from not only the conventional network security perspective but also the adversarial machine learning domain.",
                    "Citation Paper Authors": "Authors:Joseph Clements, Yuzhe Yang, Ankur Sharma, Hongxin Hu, Yingjie Lao"
                }
            },
            {
                "Sentence ID": 55,
                "Sentence": "propose a graphical\nframework for discrete domains with guarantees of minimal adversarial cost. Recently, Pierazzi et\nal. ",
                "Citation Text": "Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, and Lorenzo Cavallaro. 2019. Intriguing Properties of Adversarial\nML Attacks in the Problem Space. arXiv preprint arXiv:1911.02142 (2019).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1911.02142",
                    "Citation Paper Title": "Title:Intriguing Properties of Adversarial ML Attacks in the Problem Space",
                    "Citation Paper Abstract": "Abstract:Recent research efforts on adversarial ML have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored. This paper makes two major contributions. First, we propose a novel formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, robustness to preprocessing, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the byproduct of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. We further demonstrate the expressive power of our formalization by using it to describe several attacks from related literature across different domains. Second, building on our formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations. Experiments on a dataset with 170K Android apps from 2017 and 2018 show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Our results demonstrate that \"adversarial-malware as a service\" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial app. Our formalization of problem-space attacks paves the way to more principled research in this domain.",
                    "Citation Paper Authors": "Authors:Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, Lorenzo Cavallaro"
                }
            },
            {
                "Sentence ID": 38,
                "Sentence": "construct a general black-box framework based on reinforcement learning for\nattacking static portable executable anti-malware engines. Kulynych et al. ",
                "Citation Text": "Bogdan Kulynych, Jamie Hayes, Nikita Samarin, and Carmela Troncoso. 2018. Evading classifiers in discrete domains\nwith provable optimality guarantees. arXiv preprint arXiv:1810.10939 (2018).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1810.10939",
                    "Citation Paper Title": "Title:Evading classifiers in discrete domains with provable optimality guarantees",
                    "Citation Paper Abstract": "Abstract:Machine-learning models for security-critical applications such as bot, malware, or spam detection, operate in constrained discrete domains. These applications would benefit from having provable guarantees against adversarial examples. The existing literature on provable adversarial robustness of models, however, exclusively focuses on robustness to gradient-based attacks in domains such as images. These attacks model the adversarial cost, e.g., amount of distortion applied to an image, as a $p$-norm. We argue that this approach is not well-suited to model adversarial costs in constrained domains where not all examples are feasible.\nWe introduce a graphical framework that (1) generalizes existing attacks in discrete domains, (2) can accommodate complex cost functions beyond $p$-norms, including financial cost incurred when attacking a classifier, and (3) efficiently produces valid adversarial examples with guarantees of minimal adversarial cost. These guarantees directly translate into a notion of adversarial robustness that takes into account domain constraints and the adversary's capabilities. We show how our framework can be used to evaluate security by crafting adversarial examples that evade a Twitter-bot detection classifier with provably minimal number of changes; and to build privacy defenses by crafting adversarial examples that evade a privacy-invasive website-fingerprinting classifier.",
                    "Citation Paper Authors": "Authors:Bogdan Kulynych, Jamie Hayes, Nikita Samarin, Carmela Troncoso"
                }
            },
            {
                "Sentence ID": 4,
                "Sentence": "propose a black-box\nattack against PDF malware classifiers that uses hill-climbing over a set of feasible transformations.\nAnderson et al. ",
                "Citation Text": "Hyrum S Anderson, Anant Kharkar, Bobby Filar, David Evans, and Phil Roth. 2018. Learning to evade static PE machine\nlearning malware models via reinforcement learning. arXiv preprint arXiv:1801.08917 (2018).\nACM Trans. Priv. Sec., Vol. 1, No. 1, Article . Publication date: June 2022.32 Alesia Chernikova and Alina Oprea",
                "Citation": {
                    "Citation Paper ID": "arXiv:1801.08917",
                    "Citation Paper Title": "Title:Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning",
                    "Citation Paper Abstract": "Abstract:Machine learning is a popular approach to signatureless malware detection because it can generalize to never-before-seen malware families and polymorphic strains. This has resulted in its practical use for either primary detection engines or for supplementary heuristic detection by anti-malware vendors. Recent work in adversarial machine learning has shown that deep learning models are susceptible to gradient-based attacks, whereas non-differentiable models that report a score can be attacked by genetic algorithms that aim to systematically reduce the score. We propose a more general framework based on reinforcement learning (RL) for attacking static portable executable (PE) anti-malware engines. The general framework does not require a differentiable model nor does it require the engine to produce a score. Instead, an RL agent is equipped with a set of functionality-preserving operations that it may perform on the PE file. Through a series of games played against the anti-malware engine, it learns which sequences of operations are likely to result in evading the detector for any given malware sample. This enables completely black-box attacks against static PE anti-malware, and produces functional evasive malware samples as a direct result. We show in experiments that our method can attack a gradient-boosted machine learning model with evasion rates that are substantial and appear to be strongly dependent on the dataset. We demonstrate that attacks against this model appear to also evade components of publicly hosted antivirus engines. Adversarial training results are also presented: by retraining the model on evasive ransomware samples, a subsequent attack is 33% less effective. However, there are overfitting dangers when adversarial training, which we note. We release code to allow researchers to reproduce and improve this approach.",
                    "Citation Paper Authors": "Authors:Hyrum S. Anderson, Anant Kharkar, Bobby Filar, David Evans, Phil Roth"
                }
            },
            {
                "Sentence ID": 18,
                "Sentence": "propose a black-box attack based on genetic algorithms for\nmanipulating PDF files while maintaining the required format. Dang et al. ",
                "Citation Text": "Hung Dang, Yue Huang, and Ee-Chien Chang. 2017. Evading classifiers by morphing in the dark. In Proceedings of the\n2017 ACM SIGSAC Conference on Computer and Communications Security . ACM, 119\u2013133.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1705.07535",
                    "Citation Paper Title": "Title:Evading Classifiers by Morphing in the Dark",
                    "Citation Paper Abstract": "Abstract:Learning-based systems have been shown to be vulnerable to evasion through adversarial data manipulation. These attacks have been studied under assumptions that the adversary has certain knowledge of either the target model internals, its training dataset or at least classification scores it assigns to input samples. In this paper, we investigate a much more constrained and realistic attack scenario wherein the target classifier is minimally exposed to the adversary, revealing on its final classification decision (e.g., reject or accept an input sample). Moreover, the adversary can only manipulate malicious samples using a blackbox morpher. That is, the adversary has to evade the target classifier by morphing malicious samples \"in the dark\". We present a scoring mechanism that can assign a real-value score which reflects evasion progress to each sample based on the limited information available. Leveraging on such scoring mechanism, we propose an evasion method -- EvadeHC -- and evaluate it against two PDF malware detectors, namely PDFRate and Hidost. The experimental evaluation demonstrates that the proposed evasion attacks are effective, attaining $100\\%$ evasion rate on the evaluation dataset. Interestingly, EvadeHC outperforms the known classifier evasion technique that operates based on classification scores output by the classifiers. Although our evaluations are conducted on PDF malware classifier, the proposed approaches are domain-agnostic and is of wider application to other learning-based systems.",
                    "Citation Paper Authors": "Authors:Hung Dang, Yue Huang, Ee-Chien Chang"
                }
            },
            {
                "Sentence ID": 36,
                "Sentence": "discover regions in executables that would not affect the intended malware behavior.\nKolosnjaji et al. ",
                "Citation Text": "Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio Giacinto, Claudia Eckert, and Fabio Roli.\n2018. Adversarial malware binaries: Evading deep learning for malware detection in executables. In 2018 26th European\nSignal Processing Conference (EUSIPCO) . IEEE, 533\u2013537.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1803.04173",
                    "Citation Paper Title": "Title:Adversarial Malware Binaries: Evading Deep Learning for Malware Detection in Executables",
                    "Citation Paper Abstract": "Abstract:Machine-learning methods have already been exploited as useful tools for detecting malicious executable files. They leverage data retrieved from malware samples, such as header fields, instruction sequences, or even raw bytes, to learn models that discriminate between benign and malicious software. However, it has also been shown that machine learning and deep neural networks can be fooled by evasion attacks (also referred to as adversarial examples), i.e., small changes to the input data that cause misclassification at test time. In this work, we investigate the vulnerability of malware detection methods that use deep networks to learn from raw bytes. We propose a gradient-based attack that is capable of evading a recently-proposed deep network suited to this purpose by only changing few specific bytes at the end of each malware sample, while preserving its intrusive functionality. Promising results show that our adversarial malware binaries evade the targeted network with high probability, even though less than 1% of their bytes are modified.",
                    "Citation Paper Authors": "Authors:Bojan Kolosnjaji, Ambra Demontis, Battista Biggio, Davide Maiorca, Giorgio Giacinto, Claudia Eckert, Fabio Roli"
                }
            },
            {
                "Sentence ID": 37,
                "Sentence": "add bytes to malicious binaries either at the end or in slack regions to create adversarial examples.\nKreuk ",
                "Citation Text": "Felix Kreuk, Assi Barak, Shir Aviv-Reuven, Moran Baruch, Benny Pinkas, and Joseph Keshet. 2018. Deceiving end-to-end\ndeep learning malware detectors using adversarial examples. arXiv preprint arXiv:1802.04528 (2018).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1802.04528",
                    "Citation Paper Title": "Title:Deceiving End-to-End Deep Learning Malware Detectors using Adversarial Examples",
                    "Citation Paper Abstract": "Abstract:In recent years, deep learning has shown performance breakthroughs in many applications, such as image detection, image segmentation, pose estimation, and speech recognition. However, this comes with a major concern: deep networks have been found to be vulnerable to adversarial examples. Adversarial examples are slightly modified inputs that are intentionally designed to cause a misclassification by the model. In the domains of images and speech, the modifications are so small that they are not seen or heard by humans, but nevertheless greatly affect the classification of the model.\nDeep learning models have been successfully applied to malware detection. In this domain, generating adversarial examples is not straightforward, as small modifications to the bytes of the file could lead to significant changes in its functionality and validity. We introduce a novel loss function for generating adversarial examples specifically tailored for discrete input sets, such as executable bytes. We modify malicious binaries so that they would be detected as benign, while preserving their original functionality, by injecting a small sequence of bytes (payload) in the binary file. We applied this approach to an end-to-end convolutional deep learning malware detection model and show a high rate of detection evasion. Moreover, we show that our generated payload is robust enough to be transferable within different locations of the same file and across different files, and that its entropy is low and similar to that of benign data sections.",
                    "Citation Paper Authors": "Authors:Felix Kreuk, Assi Barak, Shir Aviv-Reuven, Moran Baruch, Benny Pinkas, Joseph Keshet"
                }
            },
            {
                "Sentence ID": 66,
                "Sentence": "for a\nmalware classification application in which features can be added or removed. Suciu et al. ",
                "Citation Text": "Octavian Suciu, Scott E Coull, and Jeffrey Johns. 2018. Exploring adversarial examples in malware detection. arXiv\npreprint arXiv:1810.08280 (2018).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1810.08280",
                    "Citation Paper Title": "Title:Exploring Adversarial Examples in Malware Detection",
                    "Citation Paper Abstract": "Abstract:The convolutional neural network (CNN) architecture is increasingly being applied to new domains, such as malware detection, where it is able to learn malicious behavior from raw bytes extracted from executables. These architectures reach impressive performance with no feature engineering effort involved, but their robustness against active attackers is yet to be understood. Such malware detectors could face a new attack vector in the form of adversarial interference with the classification model. Existing evasion attacks intended to cause misclassification on test-time instances, which have been extensively studied for image classifiers, are not applicable because of the input semantics that prevents arbitrary changes to the binaries. This paper explores the area of adversarial examples for malware detection. By training an existing model on a production-scale dataset, we show that some previous attacks are less effective than initially reported, while simultaneously highlighting architectural weaknesses that facilitate new attack strategies for malware classification. Finally, we explore how generalizable different attack strategies are, the trade-offs when aiming to increase their effectiveness, and the transferability of single-step attacks.",
                    "Citation Paper Authors": "Authors:Octavian Suciu, Scott E. Coull, Jeffrey Johns"
                }
            },
            {
                "Sentence ID": 8,
                "Sentence": ". Research on the\nrobustness of DNNs at testing time started with the work of Biggio et al. ",
                "Citation Text": "Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, and Fabio\nRoli. 2013. Evasion Attacks against Machine Learning at Test Time. In Proc. Joint European Conference on Machine\nLearning and Knowledge Discovery in Databases (ECML PKDD) .",
                "Citation": {
                    "Citation Paper ID": "arXiv:1708.06131",
                    "Citation Paper Title": "Title:Evasion Attacks against Machine Learning at Test Time",
                    "Citation Paper Abstract": "Abstract:In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.",
                    "Citation Paper Authors": "Authors:Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, Fabio Roli"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2012.05435v2": {
        "Paper Title": "Optimization-Inspired Learning with Architecture Augmentations and\n  Control Mechanisms for Low-Level Vision",
        "Sentences": [
            {
                "Sentence ID": 32,
                "Sentence": ", we\nadopt the \u21131-norm. For GM, we establish a residual network\nwith seven convolution layers and six ReLU blocks, that are\nplugged behind each convolution layer. The DM is constructed\nas a standard CNN-based classifier ",
                "Citation Text": "C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,\nA. P. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, \u201cPhoto-realistic\nsingle image super-resolution using a generative adversarial network,\u201d\ninCVPR , 2017, pp. 105\u2013114.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1609.04802",
                    "Citation Paper Title": "Title:Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
                    "Citation Paper Abstract": "Abstract:  Despite the breakthroughs in accuracy and speed of single image\nsuper-resolution using faster and deeper convolutional neural networks, one\ncentral problem remains largely unsolved: how do we recover the finer texture\ndetails when we super-resolve at large upscaling factors? The behavior of\noptimization-based super-resolution methods is principally driven by the choice\nof the objective function. Recent work has largely focused on minimizing the\nmean squared reconstruction error. The resulting estimates have high peak\nsignal-to-noise ratios, but they are often lacking high-frequency details and\nare perceptually unsatisfying in the sense that they fail to match the fidelity\nexpected at the higher resolution. In this paper, we present SRGAN, a\ngenerative adversarial network (GAN) for image super-resolution (SR). To our\nknowledge, it is the first framework capable of inferring photo-realistic\nnatural images for 4x upscaling factors. To achieve this, we propose a\nperceptual loss function which consists of an adversarial loss and a content\nloss. The adversarial loss pushes our solution to the natural image manifold\nusing a discriminator network that is trained to differentiate between the\nsuper-resolved images and original photo-realistic images. In addition, we use\na content loss motivated by perceptual similarity instead of similarity in\npixel space. Our deep residual network is able to recover photo-realistic\ntextures from heavily downsampled images on public benchmarks. An extensive\nmean-opinion-score (MOS) test shows hugely significant gains in perceptual\nquality using SRGAN. The MOS scores obtained with SRGAN are closer to those of\nthe original high-resolution images than to those obtained with any\nstate-of-the-art method.",
                    "Citation Paper Authors": "Authors:Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2012.13633v3": {
        "Paper Title": "Detecting Road Obstacles by Erasing Them",
        "Sentences": [
            {
                "Sentence ID": 50,
                "Sentence": "likewise performs image reconstruction but\nthe bottleneck is the semantic segmenter\u2019s last layer acti-\nvation. The reconstructed image is then compared to the\noriginal one using an SSIM measure.\nSynBoost ",
                "Citation Text": "G. Di Biase, H. Blum, R. Siegwart, and C. Cadena, \u201cPixel-Wise\nAnomaly Detection in Complex Driving Scenes,\u201d in Conference\nonComputer Vision and Pattern Recognition, June 2021. 3, 4, 5, 7,\n8",
                "Citation": {
                    "Citation Paper ID": "arXiv:2103.05445",
                    "Citation Paper Title": "Title:Pixel-wise Anomaly Detection in Complex Driving Scenes",
                    "Citation Paper Abstract": "Abstract:  The inability of state-of-the-art semantic segmentation methods to detect\nanomaly instances hinders them from being deployed in safety-critical and\ncomplex applications, such as autonomous driving. Recent approaches have\nfocused on either leveraging segmentation uncertainty to identify anomalous\nareas or re-synthesizing the image from the semantic label map to find\ndissimilarities with the input image. In this work, we demonstrate that these\ntwo methodologies contain complementary information and can be combined to\nproduce robust predictions for anomaly segmentation. We present a pixel-wise\nanomaly detection framework that uses uncertainty maps to improve over existing\nre-synthesis methods in finding dissimilarities between the input and generated\nimages. Our approach works as a general framework around already trained\nsegmentation networks, which ensures anomaly detection without compromising\nsegmentation accuracy, while significantly outperforming all similar methods.\nTop-2 performance across a range of different anomaly datasets shows the\nrobustness of our approach to handling different anomaly instances.",
                    "Citation Paper Authors": "Authors:Giancarlo Di Biase, Hermann Blum, Roland Siegwart, Cesar Cadena"
                }
            },
            {
                "Sentence ID": 60,
                "Sentence": ".\nPerspective effects. Our approach does not explicitly\naccount for obstacles that are a further to be smaller, due\nto perspective effects. Correcting for this, as we did in ",
                "Citation Text": "K. Lis, S. Honari, P . Fua, and M. Salzmann, \u201cPerspective Aware\nRoad Obstacle Detection,\u201d IEEE Robotics and Automation Letters,\n2023, in Press. 7",
                "Citation": {
                    "Citation Paper ID": "arXiv:2210.01779v2",
                    "Citation Paper Title": "Title:Perspective Aware Road Obstacle Detection",
                    "Citation Paper Abstract": "Abstract:  While road obstacle detection techniques have become increasingly effective,\nthey typically ignore the fact that, in practice, the apparent size of the\nobstacles decreases as their distance to the vehicle increases. In this paper,\nwe account for this by computing a scale map encoding the apparent size of a\nhypothetical object at every image location. We then leverage this perspective\nmap to (i) generate training data by injecting onto the road synthetic objects\nwhose size corresponds to the perspective foreshortening; and (ii) incorporate\nperspective information in the decoding part of the detection network to guide\nthe obstacle detector. Our results on standard benchmarks show that, together,\nthese two strategies significantly boost the obstacle detection performance,\nallowing our approach to consistently outperform state-of-the-art methods in\nterms of instance-level obstacle detection.",
                    "Citation Paper Authors": "Authors:Krzysztof Lis, Sina Honari, Pascal Fua, Mathieu Salzmann"
                }
            },
            {
                "Sentence ID": 34,
                "Sentence": "performs semantic segmentation of\nthe image and then synthesizes an image solely from the\nresulting semantic map using a conditional GAN ",
                "Citation Text": "T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro,\n\u201cHigh-Resolution Image Synthesis and Semantic Manipulation\nwith Conditional GANs,\u201d in Conference onComputer Vision and\nPattern Recognition, 2018. 2, 5, 7",
                "Citation": {
                    "Citation Paper ID": "arXiv:1711.11585",
                    "Citation Paper Title": "Title:High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs",
                    "Citation Paper Abstract": "Abstract:  We present a new method for synthesizing high-resolution photo-realistic\nimages from semantic label maps using conditional generative adversarial\nnetworks (conditional GANs). Conditional GANs have enabled a variety of\napplications, but the results are often limited to low-resolution and still far\nfrom realistic. In this work, we generate 2048x1024 visually appealing results\nwith a novel adversarial loss, as well as new multi-scale generator and\ndiscriminator architectures. Furthermore, we extend our framework to\ninteractive visual manipulation with two additional features. First, we\nincorporate object instance segmentation information, which enables object\nmanipulations such as removing/adding objects and changing the object category.\nSecond, we propose a method to generate diverse results given the same input,\nallowing users to edit the object appearance interactively. Human opinion\nstudies demonstrate that our method significantly outperforms existing methods,\nadvancing both the quality and the resolution of deep image synthesis and\nediting.",
                    "Citation Paper Authors": "Authors:Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro"
                }
            },
            {
                "Sentence ID": 57,
                "Sentence": "learns the inlier\ndistribution of features extracted from a DeepLab ",
                "Citation Text": "L. Chen, G. Papandreou, I.Kokkinos, K. Murphy, and A. L. Yuille,\n\u201cDeeplab: Semantic Image Segmentation with Deep Convolu-\ntional Nets, Atrous Convolution, and Fully Connected CRFs,\u201d\nIEEE Transactions onPattern Analysis and Machine Intelligence,\n2018. 5",
                "Citation": {
                    "Citation Paper ID": "arXiv:1606.00915",
                    "Citation Paper Title": "Title:DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",
                    "Citation Paper Abstract": "Abstract:  In this work we address the task of semantic image segmentation with Deep\nLearning and make three main contributions that are experimentally shown to\nhave substantial practical merit. First, we highlight convolution with\nupsampled filters, or 'atrous convolution', as a powerful tool in dense\nprediction tasks. Atrous convolution allows us to explicitly control the\nresolution at which feature responses are computed within Deep Convolutional\nNeural Networks. It also allows us to effectively enlarge the field of view of\nfilters to incorporate larger context without increasing the number of\nparameters or the amount of computation. Second, we propose atrous spatial\npyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP\nprobes an incoming convolutional feature layer with filters at multiple\nsampling rates and effective fields-of-views, thus capturing objects as well as\nimage context at multiple scales. Third, we improve the localization of object\nboundaries by combining methods from DCNNs and probabilistic graphical models.\nThe commonly deployed combination of max-pooling and downsampling in DCNNs\nachieves invariance but has a toll on localization accuracy. We overcome this\nby combining the responses at the final DCNN layer with a fully connected\nConditional Random Field (CRF), which is shown both qualitatively and\nquantitatively to improve localization performance. Our proposed \"DeepLab\"\nsystem sets the new state-of-art at the PASCAL VOC-2012 semantic image\nsegmentation task, reaching 79.7% mIOU in the test set, and advances the\nresults on three other datasets: PASCAL-Context, PASCAL-Person-Part, and\nCityscapes. All of our code is made publicly available online.",
                    "Citation Paper Authors": "Authors:Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1805.10766v3": {
        "Paper Title": "Improving the Resolution of CNN Feature Maps Efficiently with\n  Multisampling",
        "Sentences": [
            {
                "Sentence ID": 27,
                "Sentence": "uses fractional max-pooling to randomly specify non-integer ratios between the spatial\ndimension sizes of the input and the output to pooling layers. Zhai et al. ",
                "Citation Text": "Shuangfei Zhai, Hui Wu, Abhishek Kumar, Yu Cheng, Yongxi Lu, Zhongfei Zhang,\nand Rog\u00e9rio Schmidt Feris. S3pool: Pooling with stochastic spatial sampling. CoRR ,\nabs/1611.05138, 2016.\n21",
                "Citation": {
                    "Citation Paper ID": "arXiv:1611.05138",
                    "Citation Paper Title": "Title:S3Pool: Pooling with Stochastic Spatial Sampling",
                    "Citation Paper Abstract": "Abstract:  Feature pooling layers (e.g., max pooling) in convolutional neural networks\n(CNNs) serve the dual purpose of providing increasingly abstract\nrepresentations as well as yielding computational savings in subsequent\nconvolutional layers. We view the pooling operation in CNNs as a two-step\nprocedure: first, a pooling window (e.g., $2\\times 2$) slides over the feature\nmap with stride one which leaves the spatial resolution intact, and second,\ndownsampling is performed by selecting one pixel from each non-overlapping\npooling window in an often uniform and deterministic (e.g., top-left) manner.\nOur starting point in this work is the observation that this regularly spaced\ndownsampling arising from non-overlapping windows, although intuitive from a\nsignal processing perspective (which has the goal of signal reconstruction), is\nnot necessarily optimal for \\emph{learning} (where the goal is to generalize).\nWe study this aspect and propose a novel pooling strategy with stochastic\nspatial sampling (S3Pool), where the regular downsampling is replaced by a more\ngeneral stochastic version. We observe that this general stochasticity acts as\na strong regularizer, and can also be seen as doing implicit data augmentation\nby introducing distortions in the feature maps. We further introduce a\nmechanism to control the amount of distortion to suit different datasets and\narchitectures. To demonstrate the effectiveness of the proposed approach, we\nperform extensive experiments on several popular image classification\nbenchmarks, observing excellent improvements over baseline models. Experimental\ncode is available at this https URL.",
                    "Citation Paper Authors": "Authors:Shuangfei Zhai, Hui Wu, Abhishek Kumar, Yu Cheng, Yongxi Lu, Zhongfei Zhang, Rogerio Feris"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1910.09455v3": {
        "Paper Title": "Depth-wise Decomposition for Accelerating Separable Convolutions in\n  Efficient Convolutional Neural Networks",
        "Sentences": [
            {
                "Sentence ID": 73,
                "Sentence": "factorized a layer into 3\u00d71and\n1\u00d73combination, driven by spatial feature map redun-\ndancy. Shown in Figure 1 (e), channel decomposition ",
                "Citation Text": "Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun.\nAccelerating very deep convolutional networks for classifi-\ncation and detection. IEEE transactions on pattern analysis\nand machine intelligence , 38(10):1943\u20131955, 2016. 1, 2, 3,\n4, 5, 6, 7",
                "Citation": {
                    "Citation Paper ID": "arXiv:1505.06798",
                    "Citation Paper Title": "Title:Accelerating Very Deep Convolutional Networks for Classification and Detection",
                    "Citation Paper Abstract": "Abstract:  This paper aims to accelerate the test-time computation of convolutional\nneural networks (CNNs), especially very deep CNNs that have substantially\nimpacted the computer vision community. Unlike previous methods that are\ndesigned for approximating linear filters or linear responses, our method takes\nthe nonlinear units into account. We develop an effective solution to the\nresulting nonlinear optimization problem without the need of stochastic\ngradient descent (SGD). More importantly, while previous methods mainly focus\non optimizing one or two layers, our nonlinear method enables an asymmetric\nreconstruction that reduces the rapidly accumulated error when multiple (e.g.,\n>=10) layers are approximated. For the widely used very deep VGG-16 model, our\nmethod achieves a whole-model speedup of 4x with merely a 0.3% increase of\ntop-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also\nshows a graceful accuracy degradation for object detection when plugged into\nthe Fast R-CNN detector.",
                    "Citation Paper Authors": "Authors:Xiangyu Zhang, Jianhua Zou, Kaiming He, Jian Sun"
                }
            },
            {
                "Sentence ID": 64,
                "Sentence": ". BinaryNet [9, 52]\nproposed to binarize both connections and weights. Re-\ncently, HAQ ",
                "Citation Text": "Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song\nHan. Haq: Hardware-aware automated quantization. arXiv\npreprint arXiv:1811.08886 , 2018. 3",
                "Citation": {
                    "Citation Paper ID": "arXiv:1811.08886",
                    "Citation Paper Title": "Title:HAQ: Hardware-Aware Automated Quantization with Mixed Precision",
                    "Citation Paper Abstract": "Abstract:  Model quantization is a widely used technique to compress and accelerate deep\nneural network (DNN) inference. Emergent DNN hardware accelerators begin to\nsupport mixed precision (1-8 bits) to further improve the computation\nefficiency, which raises a great challenge to find the optimal bitwidth for\neach layer: it requires domain experts to explore the vast design space trading\noff among accuracy, latency, energy, and model size, which is both\ntime-consuming and sub-optimal. Conventional quantization algorithm ignores the\ndifferent hardware architectures and quantizes all the layers in a uniform way.\nIn this paper, we introduce the Hardware-Aware Automated Quantization (HAQ)\nframework which leverages the reinforcement learning to automatically determine\nthe quantization policy, and we take the hardware accelerator's feedback in the\ndesign loop. Rather than relying on proxy signals such as FLOPs and model size,\nwe employ a hardware simulator to generate direct feedback signals (latency and\nenergy) to the RL agent. Compared with conventional methods, our framework is\nfully automated and can specialize the quantization policy for different neural\nnetwork architectures and hardware architectures. Our framework effectively\nreduced the latency by 1.4-1.95x and the energy consumption by 1.9x with\nnegligible loss of accuracy compared with the fixed bitwidth (8 bits)\nquantization. Our framework reveals that the optimal policies on different\nhardware architectures (i.e., edge and cloud architectures) under different\nresource constraints (i.e., latency, energy and model size) are drastically\ndifferent. We interpreted the implication of different quantization policies,\nwhich offer insights for both neural network architecture design and hardware\narchitecture design.",
                    "Citation Paper Authors": "Authors:Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, Song Han"
                }
            },
            {
                "Sentence ID": 29,
                "Sentence": ".\n2.1. Designing Efficient Architecture\nDepth-wise separable convolution is widely used in effi-\ncient networks like MobileNets ",
                "Citation Text": "Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861 , 2017. 1, 3",
                "Citation": {
                    "Citation Paper ID": "arXiv:1704.04861",
                    "Citation Paper Title": "Title:MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
                    "Citation Paper Abstract": "Abstract:  We present a class of efficient models called MobileNets for mobile and\nembedded vision applications. MobileNets are based on a streamlined\narchitecture that uses depth-wise separable convolutions to build light weight\ndeep neural networks. We introduce two simple global hyper-parameters that\nefficiently trade off between latency and accuracy. These hyper-parameters\nallow the model builder to choose the right sized model for their application\nbased on the constraints of the problem. We present extensive experiments on\nresource and accuracy tradeoffs and show strong performance compared to other\npopular models on ImageNet classification. We then demonstrate the\neffectiveness of MobileNets across a wide range of applications and use cases\nincluding object detection, finegrain classification, face attributes and large\nscale geo-localization.",
                    "Citation Paper Authors": "Authors:Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam"
                }
            },
            {
                "Sentence ID": 5,
                "Sentence": ". Many of them fall into several categories:\n2designing efficient architectures [29, 47], optimized imple-\nmentation ",
                "Citation Text": "Hessam Bagherinezhad, Mohammad Rastegari, and Ali\nFarhadi. Lcnn: Lookup-based convolutional neural network.\narXiv preprint arXiv:1611.06473 , 2016. 3",
                "Citation": {
                    "Citation Paper ID": "arXiv:1611.06473",
                    "Citation Paper Title": "Title:LCNN: Lookup-based Convolutional Neural Network",
                    "Citation Paper Abstract": "Abstract:  Porting state of the art deep learning algorithms to resource constrained\ncompute platforms (e.g. VR, AR, wearables) is extremely challenging. We propose\na fast, compact, and accurate model for convolutional neural networks that\nenables efficient learning and inference. We introduce LCNN, a lookup-based\nconvolutional neural network that encodes convolutions by few lookups to a\ndictionary that is trained to cover the space of weights in CNNs. Training LCNN\ninvolves jointly learning a dictionary and a small set of linear combinations.\nThe size of the dictionary naturally traces a spectrum of trade-offs between\nefficiency and accuracy. Our experimental results on ImageNet challenge show\nthat LCNN can offer 3.2x speedup while achieving 55.1% top-1 accuracy using\nAlexNet architecture. Our fastest LCNN offers 37.6x speed up over AlexNet while\nmaintaining 44.3% top-1 accuracy. LCNN not only offers dramatic speed ups at\ninference, but it also enables efficient training. In this paper, we show the\nbenefits of LCNN in few-shot learning and few-iteration learning, two crucial\naspects of on-device training of deep learning models.",
                    "Citation Paper Authors": "Authors:Hessam Bagherinezhad, Mohammad Rastegari, Ali Farhadi"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2007.15820v2": {
        "Paper Title": "Photorealism in Driving Simulations: Blending Generative Adversarial\n  Image Synthesis with Rendering",
        "Sentences": [
            {
                "Sentence ID": 10,
                "Sentence": "enabled the realization ofarXiv:2007.15820v2  [cs.CV]  21 Jul 20222\nphoto-realistic image synthesis ",
                "Citation Text": "A. Brock, J. Donahue, and K. Simonyan, \u201cLarge scale gan training for\nhigh \ufb01delity natural image synthesis,\u201d in International Conference on\nLearning Representations , 2018.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1809.11096",
                    "Citation Paper Title": "Title:Large Scale GAN Training for High Fidelity Natural Image Synthesis",
                    "Citation Paper Abstract": "Abstract:Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple \"truncation trick,\" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",
                    "Citation Paper Authors": "Authors:Andrew Brock, Jeff Donahue, Karen Simonyan"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2011.04728v3": {
        "Paper Title": "Similarity-Based Clustering for Enhancing Image Classification\n  Architectures",
        "Sentences": [
            {
                "Sentence ID": 20,
                "Sentence": ". We also saw the work in\nDeep sparse recti\fer networks ",
                "Citation Text": "X. Glorot, A. Bordes, Y. Bengio, Deep sparse recti\fer neural networks, in:\nProceedings of the fourteenth international conference on arti\fcial intelli-\ngence and statistics, JMLR Workshop and Conference Proceedings, 2011,\npp. 315{323.\n20",
                "Citation": {
                    "Citation Paper ID": "arXiv:1301.3485",
                    "Citation Paper Title": "Title:A Semantic Matching Energy Function for Learning with Multi-relational Data",
                    "Citation Paper Abstract": "Abstract:Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.",
                    "Citation Paper Authors": "Authors:Xavier Glorot, Antoine Bordes, Jason Weston, Yoshua Bengio"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2012.03918v4": {
        "Paper Title": "NeRD: Neural Reflectance Decomposition from Image Collections",
        "Sentences": [
            {
                "Sentence ID": 10,
                "Sentence": ".\nOverall, our image formation is defined as: Lo(x,\u03c9o)\u2248P24\nm=1\u03c1d(\u03c9o,\u0393m,n,b)+\u03c1s(\u03c9o,\u0393m,n,b). Our differen-\ntiable rendering implementation follows Boss et al. ",
                "Citation Text": "Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P.A.\nLensch, and Jan Kautz. Two-shot spatially-varying brdf and\nshape estimation. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2020. 1, 2, 4",
                "Citation": {
                    "Citation Paper ID": "arXiv:2004.00403",
                    "Citation Paper Title": "Title:Two-shot Spatially-varying BRDF and Shape Estimation",
                    "Citation Paper Abstract": "Abstract:Capturing the shape and spatially-varying appearance (SVBRDF) of an object from images is a challenging task that has applications in both computer vision and graphics. Traditional optimization-based approaches often need a large number of images taken from multiple views in a controlled environment. Newer deep learning-based approaches require only a few input images, but the reconstruction quality is not on par with optimization techniques. We propose a novel deep learning architecture with a stage-wise estimation of shape and SVBRDF. The previous predictions guide each estimation, and a joint refinement network later refines both SVBRDF and shape. We follow a practical mobile image capture setting and use unaligned two-shot flash and no-flash images as input. Both our two-shot image capture and network inference can run on mobile hardware. We also create a large-scale synthetic training dataset with domain-randomized geometry and realistic materials. Extensive experiments on both synthetic and real-world datasets show that our network trained on a synthetic dataset can generalize well to real-world images. Comparisons with recent approaches demonstrate the superior performance of the proposed approach.",
                    "Citation Paper Authors": "Authors:Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P.A. Lensch, Jan Kautz"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2011.14398v2": {
        "Paper Title": "RGBD-Net: Predicting color and depth images for novel views synthesis",
        "Sentences": [
            {
                "Sentence ID": 18,
                "Sentence": "for novel view synthesis. Each input image is projected onto\nsuccessive virtual planes of the target camera to form a PSV .\nKalantari et al. ",
                "Citation Text": "Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ra-\nmamoorthi. Learning-based view synthesis for light \ufb01eld\ncameras. ACM Trans. Graph. , 35(6), Nov. 2016. 2",
                "Citation": {
                    "Citation Paper ID": "arXiv:1609.02974",
                    "Citation Paper Title": "Title:Learning-Based View Synthesis for Light Field Cameras",
                    "Citation Paper Abstract": "Abstract:With the introduction of consumer light field cameras, light field imaging has recently become widespread. However, there is an inherent trade-off between the angular and spatial resolution, and thus, these cameras often sparsely sample in either spatial or angular domain. In this paper, we use machine learning to mitigate this trade-off. Specifically, we propose a novel learning-based approach to synthesize new views from a sparse set of input views. We build upon existing view synthesis techniques and break down the process into disparity and color estimation components. We use two sequential convolutional neural networks to model these two components and train both networks simultaneously by minimizing the error between the synthesized and ground truth images. We show the performance of our approach using only four corner sub-aperture views from the light fields captured by the Lytro Illum camera. Experimental results show that our approach synthesizes high-quality images that are superior to the state-of-the-art techniques on a variety of challenging real-world scenes. We believe our method could potentially decrease the required angular resolution of consumer light field cameras, which allows their spatial resolution to increase.",
                    "Citation Paper Authors": "Authors:Nima Khademi Kalantari, Ting-Chun Wang, Ravi Ramamoorthi"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1811.11606v4": {
        "Paper Title": "Escaping Plato's Cave: 3D Shape From Adversarial Rendering",
        "Sentences": [
            {
                "Sentence ID": 11,
                "Sentence": "produce points from 2D images, but\nsimilarly with 3D data as training input. Gadelhan et al. ",
                "Citation Text": "Matheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape\ninduction from 2d views of multiple objects. In 3DV, 2016.\n2, 5, 6",
                "Citation": {
                    "Citation Paper ID": "arXiv:1612.05872",
                    "Citation Paper Title": "Title:3D Shape Induction from 2D Views of Multiple Objects",
                    "Citation Paper Abstract": "Abstract:In this paper we investigate the problem of inducing a distribution over three-dimensional structures given two-dimensional views of multiple objects taken from unknown viewpoints. Our approach called \"projective generative adversarial networks\" (PrGANs) trains a deep generative model of 3D shapes whose projections match the distributions of the input 2D views. The addition of a projection module allows us to infer the underlying 3D shape distribution without using any 3D, viewpoint information, or annotation during the learning phase. We show that our approach produces 3D shapes of comparable quality to GANs trained on 3D data for a number of shape categories including chairs, airplanes, and cars. Experiments also show that the disentangled representation of 2D shapes into geometry and viewpoint leads to a good generative model of 2D shapes. The key advantage is that our model allows us to predict 3D, viewpoint, and generate novel views from an input image in a completely unsupervised manner.",
                    "Citation Paper Authors": "Authors:Matheus Gadelha, Subhransu Maji, Rui Wang"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2012.06434v2": {
        "Paper Title": "Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid\n  Representations",
        "Sentences": [
            {
                "Sentence ID": 15,
                "Sentence": "use two different encoder-\ndecoder networks to simultaneously predict both an explicit\natlas ",
                "Citation Text": "Thibault Groueix, Matthew Fisher, Vladimir G Kim,\nBryan C Russell, and Mathieu Aubry. A papier-m \u02c6ach\u00b4e ap-\nproach to learning 3D surface generation. In Proc. IEEE\nConf. on Computer Vision & Pattern Recognition , pages\n216\u2013224, 2018. 2",
                "Citation": {
                    "Citation Paper ID": "arXiv:1802.05384",
                    "Citation Paper Title": "Title:AtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation",
                    "Citation Paper Abstract": "Abstract:We introduce a method for learning to generate the surface of 3D shapes. Our approach represents a 3D shape as a collection of parametric surface elements and, in contrast to methods generating voxel grids or point clouds, naturally infers a surface representation of the shape. Beyond its novelty, our new shape generation framework, AtlasNet, comes with significant advantages, such as improved precision and generalization capabilities, and the possibility to generate a shape of arbitrary resolution without memory issues. We demonstrate these benefits and compare to strong baselines on the ShapeNet benchmark for two applications: (i) auto-encoding shapes, and (ii) single-view reconstruction from a still image. We also provide results showing its potential for other applications, such as morphing, parametrization, super-resolution, matching, and co-segmentation.",
                    "Citation Paper Authors": "Authors:Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, Mathieu Aubry"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2011.12893v1": {
        "Paper Title": "StyleUV: Diverse and High-fidelity UV Map Generative Model",
        "Sentences": [
            {
                "Sentence ID": 13,
                "Sentence": "regressed the shape and texture parame-\nters directly from an input image by leveraging the power of\nthe convolutional neural network (CNN). Genova et al. ",
                "Citation Text": "Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron\nSarna, Daniel Vlasic, and William T Freeman. Unsuper-vised training for 3d morphable model regression. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 8377\u20138386, 2018. 1, 2",
                "Citation": {
                    "Citation Paper ID": "arXiv:1806.06098",
                    "Citation Paper Title": "Title:Unsupervised Training for 3D Morphable Model Regression",
                    "Citation Paper Abstract": "Abstract:We present a method for training a regression network from image pixels to 3D morphable model coordinates using only unlabeled photographs. The training loss is based on features from a facial recognition network, computed on-the-fly by rendering the predicted faces with a differentiable renderer. To make training from features feasible and avoid network fooling effects, we introduce three objectives: a batch distribution loss that encourages the output distribution to match the distribution of the morphable model, a loopback loss that ensures the network can correctly reinterpret its own output, and a multi-view identity loss that compares the features of the predicted 3D face and the input photograph from multiple viewing angles. We train a regression network using these objectives, a set of unlabeled photographs, and the morphable model itself, and demonstrate state-of-the-art results.",
                    "Citation Paper Authors": "Authors:Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, Daniel Vlasic, William T. Freeman"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2011.12490v1": {
        "Paper Title": "DeRF: Decomposed Radiance Fields",
        "Sentences": [
            {
                "Sentence ID": 18,
                "Sentence": "A large literature exists on neural rendering. We refer\nthe reader to a recent survey ",
                "Citation Text": "Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitz-\nmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-\nBrualla, Tomas Simon, Jason Saragih, Matthias Niessner, et al.\nState of the Art on Neural Rendering. Eurographics 2020\nState of The Art Report , 2020. 2",
                "Citation": {
                    "Citation Paper ID": "arXiv:2004.03805",
                    "Citation Paper Title": "Title:State of the Art on Neural Rendering",
                    "Citation Paper Abstract": "Abstract:Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photo-realistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. This state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.",
                    "Citation Paper Authors": "Authors:Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason Saragih, Matthias Nie\u00dfner, Rohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman, Dan B Goldman, Michael Zollh\u00f6fer"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2009.10711v2": {
        "Paper Title": "MonoClothCap: Towards Temporally Coherent Clothing Capture from\n  Monocular RGB Video",
        "Sentences": [
            {
                "Sentence ID": 8,
                "Sentence": ". We refer\nreaders to the original papers [52, 29] for details. We also\napply regularization on our estimation, denoted by Eb\nreg,\nwhich consists of a Mixture of Gaussian prior for body pose\nf\u0012igF\ni=1 ",
                "Citation Text": "F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero,\nand M. J. Black. Keep it smpl: Automatic estimation of 3d\nhuman pose and shape from a single image. In European\nConference on Computer Vision , pages 561\u2013578. Springer,\n2016. 2, 5",
                "Citation": {
                    "Citation Paper ID": "arXiv:1607.08128",
                    "Citation Paper Title": "Title:Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image",
                    "Citation Paper Abstract": "Abstract:We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art.",
                    "Citation Paper Authors": "Authors:Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, Michael J. Black"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2011.02368v2": {
        "Paper Title": "An Empirical-cum-Statistical Approach to Power-Performance\n  Characterization of Concurrent GPU Kernels",
        "Sentences": [
            {
                "Sentence ID": 52,
                "Sentence": ", Hughes et al. have characterized the transaction memory workloads. In ",
                "Citation Text": "Hadi Esmaeilzadeh, Ting Cao, Yang Xi, Stephen M. Blackburn, and Kathryn S. McKinley, \"Looking back on the language and hardware revolu-tions: measured power, performance, and scaling,\" presented at the Proceed-ings of the sixteenth International conference on Architectural support for program-ming languages and operating systems, Newport Becah, CA, USA, March 2011, 319-332",
                "Citation": {
                    "Citation Paper ID": "arXiv:2105.07879",
                    "Citation Paper Title": "Title:Conscious AI",
                    "Citation Paper Abstract": "Abstract:Recent advances in artificial intelligence (AI) have achieved human-scale speed and accuracy for classification tasks. In turn, these capabilities have made AI a viable replacement for many human activities that at their core involve classification, such as basic mechanical and analytical tasks in low-level service jobs. Current systems do not need to be conscious to recognize patterns and classify them. However, for AI to progress to more complicated tasks requiring intuition and empathy, it must develop capabilities such as metathinking, creativity, and empathy akin to human self-awareness or consciousness. We contend that such a paradigm shift is possible only through a fundamental shift in the state of artificial intelligence toward consciousness, a shift similar to what took place for humans through the process of natural selection and evolution. As such, this paper aims to theoretically explore the requirements for the emergence of consciousness in AI. It also provides a principled understanding of how conscious AI can be detected and how it might be manifested in contrast to the dominant paradigm that seeks to ultimately create machines that are linguistically indistinguishable from humans.",
                    "Citation Paper Authors": "Authors:Hadi Esmaeilzadeh, Reza Vaezi"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1810.00786v2": {
        "Paper Title": "Caulking the Leakage Effect in MEEG Source Connectivity Analysis",
        "Sentences": [
            {
                "Sentence ID": 92,
                "Sentence": "Zhang, T. and Zou, H., 2014. Sparse precision matrix estimation via lasso penalized D -trace loss. \nBiometrika, 101(1), pp.103 -120.  \nhttps://doi.org/10.1093/biomet/ast059 ",
                "Citation Text": "Zhang, Z. and Rao, B.D., 2011. Sparse signal recovery with temporally correlated source vectors using \nsparse Bayesian learning. IEEE Journal of Selected Topics in Signal Processing, 5(5), pp.912 -926.  \nhttps://doi.org/ 10.1109/JSTSP.2011.2159773",
                "Citation": {
                    "Citation Paper ID": "arXiv:1102.3949",
                    "Citation Paper Title": "Title:Sparse Signal Recovery with Temporally Correlated Source Vectors Using Sparse Bayesian Learning",
                    "Citation Paper Abstract": "Abstract:We address the sparse signal recovery problem in the context of multiple measurement vectors (MMV) when elements in each nonzero row of the solution matrix are temporally correlated. Existing algorithms do not consider such temporal correlations and thus their performance degrades significantly with the correlations. In this work, we propose a block sparse Bayesian learning framework which models the temporal correlations. In this framework we derive two sparse Bayesian learning (SBL) algorithms, which have superior recovery performance compared to existing algorithms, especially in the presence of high temporal correlations. Furthermore, our algorithms are better at handling highly underdetermined problems and require less row-sparsity on the solution matrix. We also provide analysis of the global and local minima of their cost function, and show that the SBL cost function has the very desirable property that the global minimum is at the sparsest solution to the MMV problem. Extensive experiments also provide some interesting results that motivate future theoretical research on the MMV model.",
                    "Citation Paper Authors": "Authors:Zhilin Zhang, Bhaskar D. Rao"
                }
            },
            {
                "Sentence ID": 90,
                "Sentence": "Wu, W., Nagarajan, S. and Chen, Z., 2016. Bayesian Machine Learning: EEG \\/MEG signal processing \nmeasurements. IEEE Signal Processing Magazine, 33(1), pp.14 -36. \nhttps://doi.org/ 10.1109/MSP.2015.2481559 ",
                "Citation Text": "Yuan, G., Tan, H. and Zheng, W.S., 2017. A Coordinate -wise Optimization Algorithm for Sparse Inverse \nCovariance Selection. arXiv preprint arXiv:1711.07038.  \nhttps://arxiv.org/abs/1711.07038",
                "Citation": {
                    "Citation Paper ID": "arXiv:1711.07038",
                    "Citation Paper Title": "Title:A Coordinate-wise Optimization Algorithm for Sparse Inverse Covariance Selection",
                    "Citation Paper Abstract": "Abstract:Sparse inverse covariance selection is a fundamental problem for analyzing dependencies in high dimensional data. However, such a problem is difficult to solve since it is NP-hard. Existing solutions are primarily based on convex approximation and iterative hard thresholding, which only lead to sub-optimal solutions. In this work, we propose a coordinate-wise optimization algorithm to solve this problem which is guaranteed to converge to a coordinate-wise minimum point. The algorithm iteratively and greedily selects one variable or swaps two variables to identify the support set, and then solves a reduced convex optimization problem over the support set to achieve the greatest descent. As a side contribution of this paper, we propose a Newton-like algorithm to solve the reduced convex sub-problem, which is proven to always converge to the optimal solution with global linear convergence rate and local quadratic convergence rate. Finally, we demonstrate the efficacy of our method on synthetic data and real-world data sets. As a result, the proposed method consistently outperforms existing solutions in terms of accuracy.",
                    "Citation Paper Authors": "Authors:Ganzhao Yuan, Haoxian Tan, Wei-Shi Zheng"
                }
            },
            {
                "Sentence ID": 53,
                "Sentence": "McLachlan, G. and Krishnan, T., 2007. The EM algorithm and extensi ons (Vol. 382). John Wiley & Sons. ",
                "Citation Text": "McCoy, M.B. and Tropp, J.A., 2013. The achievable performance of convex demixing. arXiv preprint \narXiv:1309.7478.  \nhttps://arxiv.org/abs/1309.7478v1",
                "Citation": {
                    "Citation Paper ID": "arXiv:1309.7478",
                    "Citation Paper Title": "Title:The achievable performance of convex demixing",
                    "Citation Paper Abstract": "Abstract:Demixing is the problem of identifying multiple structured signals from a superimposed, undersampled, and noisy observation. This work analyzes a general framework, based on convex optimization, for solving demixing problems. When the constituent signals follow a generic incoherence model, this analysis leads to precise recovery guarantees. These results admit an attractive interpretation: each signal possesses an intrinsic degrees-of-freedom parameter, and demixing can succeed if and only if the dimension of the observation exceeds the total degrees of freedom present in the observation.",
                    "Citation Paper Authors": "Authors:Michael B. McCoy, Joel A. Tropp"
                }
            },
            {
                "Sentence ID": 38,
                "Sentence": "Huizenga, H.M., De Munck, J.C., Waldorp, L.J. and Grasman, R.P., 2002. Spatiotemporal EEG/MEG \nsource analysis based on a parametric  noise covariance model. IEEE Transactions on Biomedical \nEngineering, 49(6), pp.533 -539.  \nhttps://doi.org/ 10.1109/TBME.2002.1001967 ",
                "Citation Text": "Jankova, J. and Van De Geer, S., 2015. Confidence inter vals for high -dimensional inverse covariance \nestimation. Electronic Journal of Statistics, 9(1), pp.1205 -1229.  \nhttps://doi.org/ 10.1214/15 -EJS1031",
                "Citation": {
                    "Citation Paper ID": "arXiv:1403.6752",
                    "Citation Paper Title": "Title:Confidence intervals for high-dimensional inverse covariance estimation",
                    "Citation Paper Abstract": "Abstract:We propose methodology for statistical inference for low-dimensional parameters of sparse precision matrices in a high-dimensional setting. Our method leads to a non-sparse estimator of the precision matrix whose entries have a Gaussian limiting distribution. Asymptotic properties of the novel estimator are analyzed for the case of sub-Gaussian observations under a sparsity assumption on the entries of the true precision matrix and regularity conditions. Thresholding the de-sparsified estimator gives guarantees for edge selection in the associated graphical model. Performance of the proposed method is illustrated in a simulation study.",
                    "Citation Paper Authors": "Authors:Jana Jankova, Sara van de Geer"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1807.05214v2": {
        "Paper Title": "Invertible generalized synchronization: A putative mechanism for\n  implicit learning in biological and artificial neural systems",
        "Sentences": [
            {
                "Sentence ID": 71,
                "Sentence": "M. E. J. Newman, Modularity and community struc-\nture in networks, Proc. Natl. Acad. Sci. USA 103, 8577\n(2006). ",
                "Citation Text": "V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and\nE. Lefebvre, Fast unfolding of communities in large net-\nworks, J. Stat. Mech. , P10008 (2008).",
                "Citation": {
                    "Citation Paper ID": "arXiv:0803.0476",
                    "Citation Paper Title": "Title:Fast unfolding of communities in large networks",
                    "Citation Paper Abstract": "Abstract:  We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad-hoc modular networks. .",
                    "Citation Paper Authors": "Authors:Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne Lefebvre"
                }
            },
            {
                "Sentence ID": 69,
                "Sentence": "Y. Bengio, D.-H. Lee, J. Bornschein, T. Mesnard, and\nZ. Lin, Towards biologically plausible deep learning,\narXiv preprint arXiv:1502.04156 (2015). ",
                "Citation Text": "M. A. Porter, J.-P. Onnela, and P. J. Mucha, Communi-\nties in networks, Notices of the American Mathematical\nSociety 56, 1082 (2009).",
                "Citation": {
                    "Citation Paper ID": "arXiv:0902.3788",
                    "Citation Paper Title": "Title:Communities in Networks",
                    "Citation Paper Abstract": "Abstract:  We survey some of the concepts, methods, and applications of community detection, which has become an increasingly important area of network science. To help ease newcomers into the field, we provide a guide to available methodology and open problems, and discuss why scientists from diverse backgrounds are interested in these problems. As a running theme, we emphasize the connections of community detection to problems in statistical physics and computational optimization.",
                    "Citation Paper Authors": "Authors:Mason A. Porter, Jukka-Pekka Onnela, Peter J. Mucha"
                }
            },
            {
                "Sentence ID": 68,
                "Sentence": "G. Cybenko, Approximations by superpositions of a sig-\nmoidal function, Mathematics of Control, Signals and\nSystems 2, 183 (1989). ",
                "Citation Text": "Y. Bengio, D.-H. Lee, J. Bornschein, T. Mesnard, and\nZ. Lin, Towards biologically plausible deep learning,\narXiv preprint arXiv:1502.04156 (2015).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1502.04156",
                    "Citation Paper Title": "Title:Towards Biologically Plausible Deep Learning",
                    "Citation Paper Abstract": "Abstract:Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.",
                    "Citation Paper Authors": "Authors:Yoshua Bengio, Dong-Hyun Lee, Jorg Bornschein, Thomas Mesnard, Zhouhan Lin"
                }
            },
            {
                "Sentence ID": 35,
                "Sentence": "N. Mesgarani and E. F. Chang, Selective cortical repre-\nsentation of attended speaker in multi-talker speech per-\nception, Nature 485, 233 (2012). ",
                "Citation Text": "Z. Lu, B. R. Hunt, and E. Ott, Attractor recon-\nstruction by machine learning, Chaos: An Interdisci-\nplinary Journal of Nonlinear Science 28, 061104 (2018),\nhttps://doi.org/10.1063/1.5039508.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1805.03362",
                    "Citation Paper Title": "Title:Attractor Reconstruction by Machine Learning",
                    "Citation Paper Abstract": "Abstract:A machine-learning approach called \"reservoir computing\" has been used successfully for short-term prediction and attractor reconstruction of chaotic dynamical systems from time series data. We present a theoretical framework that describes conditions under which reservoir computing can create an empirical model capable of skillful short-term forecasts and accurate long-term ergodic behavior. We illustrate this theory through numerical experiments. We also argue that the theory applies to certain other machine learning methods for time series prediction.",
                    "Citation Paper Authors": "Authors:Zhixin Lu, Brian R. Hunt, Edward Ott"
                }
            },
            {
                "Sentence ID": 24,
                "Sentence": "D. Sussillo and L. F. Abbott, Generating coherent pat-\nterns of activity from chaotic neural networks, Neuron\n63, 544 (2009). ",
                "Citation Text": "J. Pathak, Z. Lu, B. R. Hunt, M. Girvan, and E. Ott, Us-\ning machine learning to replicate chaotic attractors and\ncalculate lyapunov exponents from data, Chaos: An In-\nterdisciplinary Journal of Nonlinear Science 27, 121102\n(2017).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1710.07313",
                    "Citation Paper Title": "Title:Using Machine Learning to Replicate Chaotic Attractors and Calculate Lyapunov Exponents from Data",
                    "Citation Paper Abstract": "Abstract:We use recent advances in the machine learning area known as 'reservoir computing' to formulate a method for model-free estimation from data of the Lyapunov exponents of a chaotic process. The technique uses a limited time series of measurements as input to a high-dimensional dynamical system called a 'reservoir'. After the reservoir's response to the data is recorded, linear regression is used to learn a large set of parameters, called the 'output weights'. The learned output weights are then used to form a modified autonomous reservoir designed to be capable of producing arbitrarily long time series whose ergodic properties approximate those of the input signal. When successful, we say that the autonomous reservoir reproduces the attractor's 'climate'. Since the reservoir equations and output weights are known, we can compute derivatives needed to determine the Lyapunov exponents of the autonomous reservoir, which we then use as estimates of the Lyapunov exponents for the original input generating system. We illustrate the effectiveness of our technique with two examples, the Lorenz system, and the Kuramoto-Sivashinsky (KS) equation. In particular, we use the Lorenz system to show that achieving climate reproduction may require tuning of the reservoir parameters. For the case of the KS equation, we note that as the system's spatial size is increased, the number of Lyapunov exponents increases, thus yielding a challenging test of our method, which we find the method successfully passes.",
                    "Citation Paper Authors": "Authors:Jaideep Pathak, Zhixin Lu, Brian R. Hunt, Michelle Girvan, Edward Ott"
                }
            },
            {
                "Sentence ID": 17,
                "Sentence": "K. Rajan, C. D. Harvey, and D. W. Tank, Recurrent net-\nwork models of sequence generation and memory, Neuron\n90, 128 (2016). ",
                "Citation Text": "A. Alemi, C. Machens, S. Den\u0012 eve, and J.-J. Slotine,\nLearning arbitrary dynamics in e\u000ecient, balanced spik-\ning networks using local plasticity rules, arXiv preprint\narXiv:1705.08026 (2017).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1705.08026",
                    "Citation Paper Title": "Title:Learning arbitrary dynamics in efficient, balanced spiking networks using local plasticity rules",
                    "Citation Paper Abstract": "Abstract:Understanding how recurrent neural circuits can learn to implement dynamical systems is a fundamental challenge in neuroscience. The credit assignment problem, i.e. determining the local contribution of each synapse to the network's global output error, is a major obstacle in deriving biologically plausible local learning rules. Moreover, spiking recurrent networks implementing such tasks should not be hugely costly in terms of number of neurons and spikes, as they often are when adapted from rate models. Finally, these networks should be robust to noise and neural deaths in order to sustain these representations in the face of such naturally occurring perturbation. We approach this problem by fusing the theory of efficient, balanced spiking networks (EBN) with nonlinear adaptive control theory. Local learning rules are ensured by feeding back into the network its own error, resulting in a synaptic plasticity rule depending solely on presynaptic inputs and post-synaptic feedback. The spiking efficiency and robustness of the network are guaranteed by maintaining a tight excitatory/inhibitory balance, ensuring that each spike represents a local projection of the global output error and minimizes a loss function. The resulting networks can learn to implement complex dynamics with very small numbers of neurons and spikes, exhibit the same spike train variability as observed experimentally, and are extremely robust to noise and neuronal loss.",
                    "Citation Paper Authors": "Authors:Alireza Alemi, Christian Machens, Sophie Den\u00e8ve, Jean-Jacques Slotine"
                }
            },
            {
                "Sentence ID": 12,
                "Sentence": "A. Hefny, C. Downey, and G. J. Gordon, Supervised\nlearning for dynamical system learning, in Advances in\nneural information processing systems (2015) pp. 1963{\n1971. ",
                "Citation Text": "M. Raissi, P. Perdikaris, and G. E. Karniadakis, Multi-\nstep neural networks for data-driven discovery of nonlin-\near dynamical systems, arXiv preprint arXiv:1801.01236\n(2018).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1801.01236",
                    "Citation Paper Title": "Title:Multistep Neural Networks for Data-driven Discovery of Nonlinear Dynamical Systems",
                    "Citation Paper Abstract": "Abstract:The process of transforming observed data into predictive mathematical models of the physical world has always been paramount in science and engineering. Although data is currently being collected at an ever-increasing pace, devising meaningful models out of such observations in an automated fashion still remains an open problem. In this work, we put forth a machine learning approach for identifying nonlinear dynamical systems from data. Specifically, we blend classical tools from numerical analysis, namely the multi-step time-stepping schemes, with powerful nonlinear function approximators, namely deep neural networks, to distill the mechanisms that govern the evolution of a given data-set. We test the effectiveness of our approach for several benchmark problems involving the identification of complex, nonlinear and chaotic dynamics, and we demonstrate how this allows us to accurately learn the dynamics, forecast future states, and identify basins of attraction. In particular, we study the Lorenz system, the fluid flow behind a cylinder, the Hopf bifurcation, and the Glycoltic oscillator model as an example of complicated nonlinear dynamics typical of biological systems.",
                    "Citation Paper Authors": "Authors:Maziar Raissi, Paris Perdikaris, George Em Karniadakis"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1912.04853v3": {
        "Paper Title": "Embedding Comparator: Visualizing Differences in Global Structure and\n  Local Neighborhoods via Small Multiples",
        "Sentences": [
            {
                "Sentence ID": 20,
                "Sentence": ". Our system\nexposes the objects that are most and least similar between two\nvector spaces via a reciprocal local neighborhood similarity metric,\nand local neighborhood based metrics have been shown to usefully\ncapture differences in embedding spaces ",
                "Citation Text": "William L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Cultural Shift or Lin-\nguistic Drift? Comparing Two Computational Measures of Semantic Change. In\nProceedings of the Conference on Empirical Methods in Natural Language Processing\n(EMNLP) . ACL, Austin, USA, 2116\u20132121.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1606.02821",
                    "Citation Paper Title": "Title:Cultural Shift or Linguistic Drift? Comparing Two Computational Measures of Semantic Change",
                    "Citation Paper Abstract": "Abstract:Words shift in meaning for many reasons, including cultural factors like new technologies and regular linguistic processes like subjectification. Understanding the evolution of language and culture requires disentangling these underlying causes. Here we show how two different distributional measures can be used to detect two different types of semantic change. The first measure, which has been used in many previous works, analyzes global shifts in a word's distributional semantics, it is sensitive to changes due to regular processes of linguistic drift, such as the semantic generalization of promise (\"I promise.\" -> \"It promised to be exciting.\"). The second measure, which we develop here, focuses on local changes to a word's nearest semantic neighbors; it is more sensitive to cultural shifts, such as the change in the meaning of cell (\"prison cell\" -> \"cell phone\"). Comparing measurements made by these two methods allows researchers to determine whether changes are more cultural or linguistic in nature, a distinction that is essential for work in the digital humanities and historical linguistics.",
                    "Citation Paper Authors": "Authors:William L. Hamilton, Jure Leskovec, Dan Jurafsky"
                }
            },
            {
                "Sentence ID": 77,
                "Sentence": "\u2014 a process that can require non-trivial ML\nexpertise. Prior work has demonstrated that the choice of dimen-\nsionality reduction technique can impact downstream data analy-\nsis ",
                "Citation Text": "Jiazhi Xia, Yuchen Zhang, Jie Song, Yang Chen, Yunhai Wang, and Shixia Liu. 2022.\nRevisiting Dimensionality Reduction Techniques for Visual Cluster Analysis: An\nEmpirical Study. IEEE Transactions on Visualization and Computer Graphics 28, 1\n(2022), 529\u2013539.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2110.02894",
                    "Citation Paper Title": "Title:Revisiting Dimensionality Reduction Techniques for Visual Cluster Analysis: An Empirical Study",
                    "Citation Paper Abstract": "Abstract:Dimensionality Reduction (DR) techniques can generate 2D projections and enable visual exploration of cluster structures of high-dimensional datasets. However, different DR techniques would yield various patterns, which significantly affect the performance of visual cluster analysis tasks. We present the results of a user study that investigates the influence of different DR techniques on visual cluster analysis. Our study focuses on the most concerned property types, namely the linearity and locality, and evaluates twelve representative DR techniques that cover the concerned properties. Four controlled experiments were conducted to evaluate how the DR techniques facilitate the tasks of 1) cluster identification, 2) membership identification, 3) distance comparison, and 4) density comparison, respectively. We also evaluated users' subjective preference of the DR techniques regarding the quality of projected clusters. The results show that: 1) Non-linear and Local techniques are preferred in cluster identification and membership identification; 2) Linear techniques perform better than non-linear techniques in density comparison; 3) UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-Distributed Stochastic Neighbor Embedding) perform the best in cluster identification and membership identification; 4) NMF (Nonnegative Matrix Factorization) has competitive performance in distance comparison; 5) t-SNLE (t-Distributed Stochastic Neighbor Linear Embedding) has competitive performance in density comparison.",
                    "Citation Paper Authors": "Authors:Jiazhi Xia, Yuchen Zhang, Jie Song, Yang Chen, Yunhai Wang, Shixia Liu"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1912.07773v4": {
        "Paper Title": "MEDIRL: Predicting the Visual Attention of Drivers via Maximum Entropy\n  Deep Inverse Reinforcement Learning",
        "Sentences": [
            {
                "Sentence ID": 18,
                "Sentence": "are the most\nwell-known large-scale annotated datasets in naturalistic\nand in-lab driving settings, respectively. Importantly, the\nrecently-released annotated driving attention dataset with\nin-lab settings, DADA-2000 ",
                "Citation Text": "Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, He\nWang, and Sen Li. Dada-2000: Can driving accident be pre-\ndicted by driver attention analyzed by a benchmark. In 2019\nIEEE Intelligent Transportation Systems Conference (ITSC) ,\npages 4303\u20134309. IEEE, 2019.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1904.12634",
                    "Citation Paper Title": "Title:DADA-2000: Can Driving Accident be Predicted by Driver Attention? Analyzed by A Benchmark",
                    "Citation Paper Abstract": "Abstract:Driver attention prediction is currently becoming the focus in safe driving research community, such as the DR(eye)VE project and newly emerged Berkeley DeepDrive Attention (BDD-A) database in critical situations. In safe driving, an essential task is to predict the incoming accidents as early as possible. BDD-A was aware of this problem and collected the driver attention in laboratory because of the rarity of such scenes. Nevertheless, BDD-A focuses the critical situations which do not encounter actual accidents, and just faces the driver attention prediction task, without a close step for accident prediction. In contrast to this, we explore the view of drivers' eyes for capturing multiple kinds of accidents, and construct a more diverse and larger video benchmark than ever before with the driver attention and the driving accident annotation simultaneously (named as DADA-2000), which has 2000 video clips owning about 658,476 frames on 54 kinds of accidents. These clips are crowd-sourced and captured in various occasions (highway, urban, rural, and tunnel), weather (sunny, rainy and snowy) and light conditions (daytime and nighttime). For the driver attention representation, we collect the maps of fixations, saccade scan path and focusing time. The accidents are annotated by their categories, the accident window in clips and spatial locations of the crash-objects. Based on the analysis, we obtain a quantitative and positive answer for the question in this paper.",
                    "Citation Paper Authors": "Authors:Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, He Wang, Sen Li"
                }
            },
            {
                "Sentence ID": 62,
                "Sentence": "which can handle raw image inputs\nand enables the model to handle the often sub-optimal and\nseemingly stochastic behaviors of drivers ",
                "Citation Text": "Markus Wulfmeier, Peter Ondruska, and Ingmar Posner.\nMaximum entropy deep inverse reinforcement learning.\narXiv preprint arXiv:1507.04888 , 2015.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1507.04888",
                    "Citation Paper Title": "Title:Maximum Entropy Deep Inverse Reinforcement Learning",
                    "Citation Paper Abstract": "Abstract:This paper presents a general framework for exploiting the representational capacity of neural networks to approximate complex, nonlinear reward functions in the context of solving the inverse reinforcement learning (IRL) problem. We show in this context that the Maximum Entropy paradigm for IRL lends itself naturally to the efficient training of deep architectures. At test time, the approach leads to a computational complexity independent of the number of demonstrations, which makes it especially well-suited for applications in life-long learning scenarios. Our approach achieves performance commensurate to the state-of-the-art on existing benchmarks while exceeding on an alternative benchmark based on highly varying reward structures. Finally, we extend the basic architecture - which is equivalent to a simplified subclass of Fully Convolutional Neural Networks (FCNNs) with width one - to include larger convolutions in order to eliminate dependency on precomputed spatial features and work on raw input representations.",
                    "Citation Paper Authors": "Authors:Markus Wulfmeier, Peter Ondruska, Ingmar Posner"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1901.03450v2": {
        "Paper Title": "Ubiquitous Acoustic Sensing on Commodity IoT Devices: A Survey",
        "Sentences": [
            {
                "Sentence ID": 145,
                "Sentence": ". It allows synthesizing audio sig-\nnals inaudible to humans to manipulate such devices ",
                "Citation Text": "G. Zhang, C. Yan, X. Ji, T. Zhang, T. Zhang, and W. Xu. Dolphi-\nnAttack: Inaudible V oice Commands. In Proc. of the 24th ACM CCS ,\n2017.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1708.09537",
                    "Citation Paper Title": "Title:DolphinAtack: Inaudible Voice Commands",
                    "Citation Paper Abstract": "Abstract:Speech recognition (SR) systems such as Siri or Google Now have become an increasingly popular human-computer interaction method, and have turned various systems into voice controllable systems(VCS). Prior work on attacking VCS shows that the hidden voice commands that are incomprehensible to people can control the systems. Hidden voice commands, though hidden, are nonetheless audible. In this work, we design a completely inaudible attack, DolphinAttack, that modulates voice commands on ultrasonic carriers (e.g., f > 20 kHz) to achieve inaudibility. By leveraging the nonlinearity of the microphone circuits, the modulated low frequency audio commands can be successfully demodulated, recovered, and more importantly interpreted by the speech recognition systems. We validate DolphinAttack on popular speech recognition systems, including Siri, Google Now, Samsung S Voice, Huawei HiVoice, Cortana and Alexa. By injecting a sequence of inaudible voice commands, we show a few proof-of-concept attacks, which include activating Siri to initiate a FaceTime call on iPhone, activating Google Now to switch the phone to the airplane mode, and even manipulating the navigation system in an Audi automobile. We propose hardware and software defense solutions. We validate that it is feasible to detect DolphinAttack by classifying the audios using supported vector machine (SVM), and suggest to re-design voice controllable systems to be resilient to inaudible voice command attacks.",
                    "Citation Paper Authors": "Authors:Guoming Zhang, Chen Yan, Xiaoyu Ji, Taimin Zhang, Tianchen Zhang, Wenyuan Xu"
                }
            },
            {
                "Sentence ID": 32,
                "Sentence": "only require limited\nlabels to quickly adapt on target environments. Some early\nattempts in this direction has been made. For example, the\nauthors in ",
                "Citation Text": "S. Ding, Z. Chen, T. Zheng, and J. Luo. RF-Net: A Uni\ufb01ed\nMeta-Learning Framework for RF-Enabled One-Shot Human Activity\nRecognition. In Proc. of the 18th ACM SenSys , pages 517\u2013530, 2020.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2111.04566",
                    "Citation Paper Title": "Title:RF-Net: a Unified Meta-learning Framework for RF-enabled One-shot Human Activity Recognition",
                    "Citation Paper Abstract": "Abstract:Radio-Frequency (RF) based device-free Human Activity Recognition (HAR) rises as a promising solution for many applications. However, device-free (or contactless) sensing is often more sensitive to environment changes than device-based (or wearable) sensing. Also, RF datasets strictly require on-line labeling during collection, starkly different from image and text data collections where human interpretations can be leveraged to perform off-line labeling. Therefore, existing solutions to RF-HAR entail a laborious data collection process for adapting to new environments. To this end, we propose RF-Net as a meta-learning based approach to one-shot RF-HAR; it reduces the labeling efforts for environment adaptation to the minimum level. In particular, we first examine three representative RF sensing techniques and two major meta-learning approaches. The results motivate us to innovate in two designs: i) a dual-path base HAR network, where both time and frequency domains are dedicated to learning powerful RF features including spatial and attention-based temporal ones, and ii) a metric-based meta-learning framework to enhance the fast adaption capability of the base network, including an RF-specific metric module along with a residual classification module. We conduct extensive experiments based on all three RF sensing techniques in multiple real-world indoor environments; all results strongly demonstrate the efficacy of RF-Net compared with state-of-the-art baselines.",
                    "Citation Paper Authors": "Authors:Shuya Ding, Zhe Chen, Tianyue Zheng, Jun Luo"
                }
            },
            {
                "Sentence ID": 126,
                "Sentence": "allow a model to generalize well on even unseen data, while\nfew-shot learning techniques ",
                "Citation Text": "Y . Wang, Q. Yao, J. T. Kwok, and L. M. Ni. Generalizing from a\nFew Examples: A Survey on Few-Shot Learning. ACM Comput. Surv. ,\n53(3), June 2020.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1904.05046",
                    "Citation Paper Title": "Title:Generalizing from a Few Examples: A Survey on Few-Shot Learning",
                    "Citation Paper Abstract": "Abstract:Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.",
                    "Citation Paper Authors": "Authors:Yaqing Wang, Quanming Yao, James Kwok, Lionel M. Ni"
                }
            }
        ]
    },
    "http://arxiv.org/abs/1909.03409v2": {
        "Paper Title": "Conditional Text Generation for Harmonious Human-Machine Interaction",
        "Sentences": [
            {
                "Sentence ID": 28,
                "Sentence": "Conditional GAN (CGAN)CNN captures information in an image,\nand LSTM generates the relevant\ndescriptions; the discriminator evaluates\nthe quality of generated descriptions\nDaset al. ",
                "Citation Text": "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 MF Moura, Devi Parikh, and Dhruv Batra.\n2017. Visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 326\u2013335.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1611.08669",
                    "Citation Paper Title": "Title:Visual Dialog",
                    "Citation Paper Abstract": "Abstract:We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog question-answer pairs.\nWe introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network -- and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first 'visual chatbot'! Our dataset, code, trained models and visual chatbot are available on this https URL",
                    "Citation Paper Authors": "Authors:Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 M. F. Moura, Devi Parikh, Dhruv Batra"
                }
            },
            {
                "Sentence ID": 32,
                "Sentence": "is a document grounded conversation dataset\nwhere each conversation is followed by specified documents about popular movies extracted from\nWikipedia articles. Wizard of Wikipedia24 ",
                "Citation Text": "Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of Wikipedia:\nKnowledge-Powered Conversational Agents. In International Conference on Learning Representations .",
                "Citation": {
                    "Citation Paper ID": "arXiv:1811.01241",
                    "Citation Paper Title": "Title:Wizard of Wikipedia: Knowledge-Powered Conversational agents",
                    "Citation Paper Abstract": "Abstract:In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically \"generate and hope\" generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction.",
                    "Citation Paper Authors": "Authors:Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston"
                }
            },
            {
                "Sentence ID": 164,
                "Sentence": "publish another high-quality emotional dialogue dataset\ncollecting from telescripts and dialogues in Facebook, named EmotionLines22. All utterances in it\nare labeled with specific emotion according to textual contents to guide the emotional dialogue\nresponse generation.\nKnowledge-based datasets. CMU DoG23 ",
                "Citation Text": "Kangyan Zhou, Shrimai Prabhumoye, and Alan W Black. 2018. A Dataset for Document Grounded Conversations. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . 708\u2013713.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1809.07358",
                    "Citation Paper Title": "Title:A Dataset for Document Grounded Conversations",
                    "Citation Paper Abstract": "Abstract:This paper introduces a document grounded dataset for text conversations. We define \"Document Grounded Conversations\" as conversations that are about the contents of a specified document. In this dataset the specified documents were Wikipedia articles about popular movies. The dataset contains 4112 conversations with an average of 21.43 turns per conversation. This positions this dataset to not only provide a relevant chat history while generating responses but also provide a source of information that the models could use. We describe two neural architectures that provide benchmark performance on the task of generating the next response. We also evaluate our models for engagement and fluency, and find that the information from the document helps in generating more engaging and fluent responses.",
                    "Citation Paper Authors": "Authors:Kangyan Zhou, Shrimai Prabhumoye, Alan W Black"
                }
            },
            {
                "Sentence ID": 20,
                "Sentence": "publish an emotional dialogue dataset called Empathetic\nDialogues21, including an extensive set of emotions and every speaker in it feels with a given emotion\nduring conversations. Chen et al. ",
                "Citation Text": "Sheng Yeh Chen, Chao Chun Hsu, Chuan Chun Kuo, Kenneth Huang, and Lun Wei Ku. 2019. Emotionlines: An\nemotion corpus of multi-party conversations. In 11th International Conference on Language Resources and Evaluation,\nLREC 2018 . European Language Resources Association (ELRA), 1597\u20131601.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1802.08379",
                    "Citation Paper Title": "Title:EmotionLines: An Emotion Corpus of Multi-Party Conversations",
                    "Citation Paper Abstract": "Abstract:Feeling emotion is a critical characteristic to distinguish people from machines. Among all the multi-modal resources for emotion detection, textual datasets are those containing the least additional information in addition to semantics, and hence are adopted widely for testing the developed systems. However, most of the textual emotional datasets consist of emotion labels of only individual words, sentences or documents, which makes it challenging to discuss the contextual flow of emotions. In this paper, we introduce EmotionLines, the first dataset with emotions labeling on all utterances in each dialogue only based on their textual content. Dialogues in EmotionLines are collected from Friends TV scripts and private Facebook messenger dialogues. Then one of seven emotions, six Ekman's basic emotions plus the neutral emotion, is labeled on each utterance by 5 Amazon MTurkers. A total of 29,245 utterances from 2,000 dialogues are labeled in EmotionLines. We also provide several strong baselines for emotion detection models on EmotionLines in this paper.",
                    "Citation Paper Authors": "Authors:Sheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo, Ting-Hao (Kenneth)Huang, Lun-Wei Ku"
                }
            },
            {
                "Sentence ID": 92,
                "Sentence": "present a high-quality personalized dialogue dataset\nnamed PERSONA CHAT18. In each dialogue, two parts of the conversation are given a group\nof profile information, and the whole dialogue process is conducted around these personalized\ncharacteristics. Humeau et al. ",
                "Citation Text": "Pierre-Emmanuel Mazar\u00e9, Samuel Humeau, Martin Raison, and Antoine Bordes. 2018. Training Millions of Personal-\nized Dialogue Agents. In EMNLP .",
                "Citation": {
                    "Citation Paper ID": "arXiv:1809.01984",
                    "Citation Paper Title": "Title:Training Millions of Personalized Dialogue Agents",
                    "Citation Paper Abstract": "Abstract:Current dialogue systems are not very engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the dataset used in Zhang et al. (2018) is synthetic and of limited size as it contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from Zhang et al. (2018) and achieving state-of-the-art results.",
                    "Citation Paper Authors": "Authors:Pierre-Emmanuel Mazar\u00e9, Samuel Humeau, Martin Raison, Antoine Bordes"
                }
            },
            {
                "Sentence ID": 151,
                "Sentence": "is another large multi-turn dialogue dataset containing almost one million conversa-\ntions extracted from the Ubuntu chat logs that is a great help for training context-sensitive technical\ndialogue systems.\nPersonalized datasets. Zhang et al. ",
                "Citation Text": "Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing\nDialogue Agents: I have a dog, do you have pets too?. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) . 2204\u20132213.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1801.07243",
                    "Citation Paper Title": "Title:Personalizing Dialogue Agents: I have a dog, do you have pets too?",
                    "Citation Paper Abstract": "Abstract:Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i) condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",
                    "Citation Paper Authors": "Authors:Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, Jason Weston"
                }
            },
            {
                "Sentence ID": 82,
                "Sentence": "is a large-scale multi-turn dialogue\ndataset providing contextual information during the conversation, which contains a large metadata-\nrich collection of fictional conversations extracted from raw movie scripts. Ubuntu Dialogue\nCorpus17 ",
                "Citation Text": "Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The ubuntu dialogue corpus: A large dataset for\nresearch in unstructured multi-turn dialogue systems. arXiv preprint arXiv:1506.08909 (2015).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1506.08909",
                    "Citation Paper Title": "Title:The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems",
                    "Citation Paper Abstract": "Abstract:This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.",
                    "Citation Paper Authors": "Authors:Ryan Lowe, Nissan Pow, Iulian Serban, Joelle Pineau"
                }
            },
            {
                "Sentence ID": 53,
                "Sentence": "samples from the next-token distribution after\nhaving filtered this distribution to keep only the top \ud835\udc58tokens, while the Top-p sampling ",
                "Citation Text": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The Curious Case of Neural Text Degeneration.\nInInternational Conference on Learning Representations .",
                "Citation": {
                    "Citation Paper ID": "arXiv:1904.09751",
                    "Citation Paper Title": "Title:The Curious Case of Neural Text Degeneration",
                    "Citation Paper Abstract": "Abstract:Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive.\nIn this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.",
                    "Citation Paper Authors": "Authors:Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi"
                }
            },
            {
                "Sentence ID": 73,
                "Sentence": "propose the Group diverse Beam Search to increase the variety\nof generated texts, which divides the beam into groups and utilizes a group dissimilarity penalty to\nreduce the similarity between different search groups. Similarly, Li et al. ",
                "Citation Text": "Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. A simple, fast diverse decoding algorithm for neural generation. arXiv\npreprint arXiv:1611.08562 (2016).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1611.08562",
                    "Citation Paper Title": "Title:A Simple, Fast Diverse Decoding Algorithm for Neural Generation",
                    "Citation Paper Abstract": "Abstract:In this paper, we propose a simple, fast decoding algorithm that fosters diversity in neural generation. The algorithm modifies the standard beam search algorithm by adding an inter-sibling ranking penalty, favoring choosing hypotheses from diverse parents. We evaluate the proposed model on the tasks of dialogue response generation, abstractive summarization and machine translation. We find that diverse decoding helps across all tasks, especially those for which reranking is needed.\nWe further propose a variation that is capable of automatically adjusting its diversity decoding rates for different inputs using reinforcement learning (RL). We observe a further performance boost from this RL technique. This paper includes material from the unpublished script \"Mutual Information and Diverse Decoding Improve Neural Machine Translation\" (Li and Jurafsky, 2016).",
                    "Citation Paper Authors": "Authors:Jiwei Li, Will Monroe, Dan Jurafsky"
                }
            },
            {
                "Sentence ID": 154,
                "Sentence": "propose the Adapter-Bot,\nwhich triggers different skills via different Adapters trained independently. The backbone of the\nAdapter-Bot is a pre-trained conversational model such as DialoGPT ",
                "Citation Text": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill\nDolan. 2019. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint\narXiv:1911.00536 (2019).",
                "Citation": {
                    "Citation Paper ID": "arXiv:1911.00536",
                    "Citation Paper Title": "Title:DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation",
                    "Citation Paper Abstract": "Abstract:We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.",
                    "Citation Paper Authors": "Authors:Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan"
                }
            },
            {
                "Sentence ID": 94,
                "Sentence": ". Using a large amount of\ndata to train the LSTM language model in an unsupervised way, the contextual word vector of\neach word can be obtained to demonstrate strong results across discriminative natural language\nunderstanding ( NLU ) tasks ",
                "Citation Text": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized\nword vectors. In Advances in Neural Information Processing Systems . 6294\u20136305.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1708.00107",
                    "Citation Paper Title": "Title:Learned in Translation: Contextualized Word Vectors",
                    "Citation Paper Abstract": "Abstract:Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.",
                    "Citation Paper Authors": "Authors:Bryan McCann, James Bradbury, Caiming Xiong, Richard Socher"
                }
            },
            {
                "Sentence ID": 26,
                "Sentence": "LSTM + CNNCNN and LSTM respectively encodes the\nimage and the question into vectors to\ncapture the semantic information, and\nthen another LSTM generates\ncorresponding answers\nDaiet al. ",
                "Citation Text": "Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin. 2017. Towards diverse and natural image descriptions via a\nconditional gan. In Proceedings of the IEEE International Conference on Computer Vision . 2970\u20132979.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1703.06029",
                    "Citation Paper Title": "Title:Towards Diverse and Natural Image Descriptions via a Conditional GAN",
                    "Citation Paper Abstract": "Abstract:Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect.Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the \"ground-truth\" captions while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity -- two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.",
                    "Citation Paper Authors": "Authors:Bo Dai, Sanja Fidler, Raquel Urtasun, Dahua Lin"
                }
            },
            {
                "Sentence ID": 89,
                "Sentence": "RNN + CNNEncoder CNN captures information in\nimages, and decoder RNN generates\nneural language descriptions based on\ntheir features\nMalinowski et al. ",
                "Citation Text": "Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. 2015. Ask your neurons: A neural-based approach to\nanswering questions about images. In Proceedings of the IEEE international conference on computer vision . 1\u20139.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1505.01121",
                    "Citation Paper Title": "Title:Ask Your Neurons: A Neural-based Approach to Answering Questions about Images",
                    "Citation Paper Abstract": "Abstract:We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.",
                    "Citation Paper Authors": "Authors:Mateusz Malinowski, Marcus Rohrbach, Mario Fritz"
                }
            },
            {
                "Sentence ID": 30,
                "Sentence": "is an extension of CNN in the graph domain, which can effectively learn the structural\ninformation of nodes and edges in the knowledge graph. De et al. ",
                "Citation Text": "Nicola De Cao, Wilker Aziz, and Ivan Titov. 2019. Question Answering by Reasoning Across Documents with Graph\nConvolutional Networks. In Proceedings of NAACL-HLT . 2306\u20132317.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1808.09920",
                    "Citation Paper Title": "Title:Question Answering by Reasoning Across Documents with Graph Convolutional Networks",
                    "Citation Paper Abstract": "Abstract:Most research in reading comprehension has focused on answering questions based on individual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within- and cross-document co-reference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, WikiHop (Welbl et al., 2018).",
                    "Citation Paper Authors": "Authors:Nicola De Cao, Wilker Aziz, Ivan Titov"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2110.15114v2": {
        "Paper Title": "UltraGCN: Ultra Simplification of Graph Convolutional Networks for\n  Recommendation",
        "Sentences": [
            {
                "Sentence ID": 21,
                "Sentence": "devise LightGCN\nfor recommendation by removing nonlinear activation and feature\ntransformation too. However, its efficiency is still limited by the\ntime-consuming message passing. Qiu et al. ",
                "Citation Text": "Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. 2018.\nNetwork Embedding as Matrix Factorization: Unifying Deepwalk, LINE, PTE,\nand Node2vec. In Proceedings of the Eleventh ACM International Conference on\nWeb Search and Data Mining (WSDM) . 459\u2013467.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1710.02971",
                    "Citation Paper Title": "Title:Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec",
                    "Citation Paper Abstract": "Abstract:Since the invention of word2vec, the skip-gram model has significantly advanced the research of network embedding, such as the recent emergence of the DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of the aforementioned models with negative sampling can be unified into the matrix factorization framework with closed forms. Our analysis and proofs reveal that: (1) DeepWalk empirically produces a low-rank transformation of a network's normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk when the size of vertices' context is set to one; (3) As an extension of LINE, PTE can be viewed as the joint factorization of multiple networks' Laplacians; (4) node2vec is factorizing a matrix related to the stationary distribution and transition probability tensor of a 2nd-order random walk. We further provide the theoretical connections between skip-gram based network embedding algorithms and the theory of graph Laplacian. Finally, we present the NetMF method as well as its approximation algorithm for computing network embedding. Our method offers significant improvements over DeepWalk and LINE for conventional network mining tasks. This work lays the theoretical foundation for skip-gram based network embedding methods, leading to a better understanding of latent network representation learning.",
                    "Citation Paper Authors": "Authors:Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang"
                }
            },
            {
                "Sentence ID": 10,
                "Sentence": "find the\nnon-necessity of nonlinear activation and feature transformation\nin GCN, proposing a simplified GCN (SGCN) model by removing\nthese two parts. Inspired by SGC, He et al. ",
                "Citation Text": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng\nWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Net-\nwork for Recommendation. In Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval (SIGIR) . 639\u2013648.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2002.02126",
                    "Citation Paper Title": "Title:LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                    "Citation Paper Abstract": "Abstract:Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
                    "Citation Paper Authors": "Authors:Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang"
                }
            },
            {
                "Sentence ID": 29,
                "Sentence": "propose UCMF that sim-\nplifies GCN for the node classification task. Wu et al. ",
                "Citation Text": "Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian\nWeinberger. 2019. Simplifying Graph Convolutional Networks. In International\nConference on Machine Learning (ICML) . 6861\u20136871.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1902.07153",
                    "Citation Paper Title": "Title:Simplifying Graph Convolutional Networks",
                    "Citation Paper Abstract": "Abstract:Graph Convolutional Networks (GCNs) and their variants have experienced significant attention and have become the de facto methods for learning graph representations. GCNs derive inspiration primarily from recent deep learning approaches, and as a result, may inherit unnecessary complexity and redundant computation. In this paper, we reduce this excess complexity through successively removing nonlinearities and collapsing weight matrices between consecutive layers. We theoretically analyze the resulting linear model and show that it corresponds to a fixed low-pass filter followed by a linear classifier. Notably, our experimental evaluation demonstrates that these simplifications do not negatively impact accuracy in many downstream applications. Moreover, the resulting model scales to larger datasets, is naturally interpretable, and yields up to two orders of magnitude speedup over FastGCN.",
                    "Citation Paper Authors": "Authors:Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr., Christopher Fifty, Tao Yu, Kilian Q. Weinberger"
                }
            },
            {
                "Sentence ID": 18,
                "Sentence": "extend fixed-point theory on GNN for bet-\nter representation learning. Liu et al. ",
                "Citation Text": "Qiang Liu, Haoli Zhang, and Zhaocheng Liu. 2020. Simplification of Graph Con-\nvolutional Networks: A Matrix Factorization-based Perspective. arXiv preprint\narXiv:2007.09036 (2020).",
                "Citation": {
                    "Citation Paper ID": "arXiv:2007.09036",
                    "Citation Paper Title": "Title:Simplification of Graph Convolutional Networks: A Matrix Factorization-based Perspective",
                    "Citation Paper Abstract": "Abstract:In recent years, substantial progress has been made on Graph Convolutional Networks (GCNs). However, the computing of GCN usually requires a large memory space for keeping the entire graph. In consequence, GCN is not flexible enough, especially for large scale graphs in complex real-world applications. Fortunately, methods based on Matrix Factorization (MF) naturally support constructing mini-batches, and thus are more friendly to distributed computing compared with GCN. Accordingly, in this paper, we analyze the connections between GCN and MF, and simplify GCN as matrix factorization with unitization and co-training. Furthermore, under the guidance of our analysis, we propose an alternative model to GCN named Unitized and Co-training Matrix Factorization (UCMF). Extensive experiments have been conducted on several real-world datasets. On the task of semi-supervised node classification, the experimental results illustrate that UCMF achieves similar or superior performances compared with GCN. Meanwhile, distributed UCMF significantly outperforms distributed GCN methods, which shows that UCMF can greatly benefit large scale and complex real-world applications. Moreover, we have also conducted experiments on a typical task of graph embedding, i.e., community detection, and the proposed UCMF model outperforms several representative graph embedding models.",
                    "Citation Paper Authors": "Authors:Qiang Liu, Haoli Zhang, Zhaocheng Liu"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2109.12613v3": {
        "Paper Title": "SimpleX: A Simple and Strong Baseline for Collaborative Filtering",
        "Sentences": [
            {
                "Sentence ID": 11,
                "Sentence": "which is a simple linear model\nthat combines the advantages of neighborhood- and model-based\nCF approaches, MLPs-based NeuMF ",
                "Citation Text": "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\nChua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th International\nConference on World Wide Web (WWW) . 173\u2013182.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1708.05031",
                    "Citation Paper Title": "Title:Neural Collaborative Filtering",
                    "Citation Paper Abstract": "Abstract:In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",
                    "Citation Paper Authors": "Authors:Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua"
                }
            },
            {
                "Sentence ID": 33,
                "Sentence": "which models the uncertainty in the user-item graph with bayesian\ngraph neural networks, DGCF ",
                "Citation Text": "Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, and Tat-Seng Chua.\n2020. Disentangled Graph Collaborative Filtering. In Proceedings of the 43rd\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR) . 1001\u20131010.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2007.01764",
                    "Citation Paper Title": "Title:Disentangled Graph Collaborative Filtering",
                    "Citation Paper Abstract": "Abstract:Learning informative representations of users and items from the interaction data is of crucial importance to collaborative filtering (CF). Present embedding functions exploit user-item relationships to enrich the representations, evolving from a single user-item instance to the holistic interaction graph. Nevertheless, they largely model the relationships in a uniform manner, while neglecting the diversity of user intents on adopting the items, which could be to pass time, for interest, or shopping for others like families. Such uniform approach to model user interests easily results in suboptimal representations, failing to model diverse relationships and disentangle user intents in representations.\nIn this work, we pay special attention to user-item relationships at the finer granularity of user intents. We hence devise a new model, Disentangled Graph Collaborative Filtering (DGCF), to disentangle these factors and yield disentangled representations. Specifically, by modeling a distribution over intents for each user-item interaction, we iteratively refine the intent-aware interaction graphs and representations. Meanwhile, we encourage independence of different intents. This leads to disentangled representations, effectively distilling information pertinent to each intent. We conduct extensive experiments on three benchmark datasets, and DGCF achieves significant improvements over several state-of-the-art models like NGCF, DisenGCN, and MacridVAE. Further analyses offer insights into the advantages of DGCF on the disentanglement of user intents and interpretability of representations. Our codes are available in this https URL.",
                    "Citation Paper Authors": "Authors:Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, Tat-Seng Chua"
                }
            },
            {
                "Sentence ID": 10,
                "Sentence": "that explicitly encoded the collaborative signals as high-order con-\nnectivities by performing embedding propagation. He et al. pro-\nposed LightGCN ",
                "Citation Text": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng\nWang. 2020. LightGCN: Simplifying and Powering Graph Convolution Net-\nwork for Recommendation. In Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval (SIGIR) . 639\u2013648.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2002.02126",
                    "Citation Paper Title": "Title:LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation",
                    "Citation Paper Abstract": "Abstract:Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped with many neural network operations. However, we empirically find that the two most common designs in GCNs -- feature transformation and nonlinear activation -- contribute little to the performance of collaborative filtering. Even worse, including them adds to the difficulty of training and degrades recommendation performance.\nIn this work, we aim to simplify the design of GCN to make it more concise and appropriate for recommendation. We propose a new model named LightGCN, including only the most essential component in GCN -- neighborhood aggregation -- for collaborative filtering. Specifically, LightGCN learns user and item embeddings by linearly propagating them on the user-item interaction graph, and uses the weighted sum of the embeddings learned at all layers as the final embedding. Such simple, linear, and neat model is much easier to implement and train, exhibiting substantial improvements (about 16.0\\% relative improvement on average) over Neural Graph Collaborative Filtering (NGCF) -- a state-of-the-art GCN-based recommender model -- under exactly the same experimental setting. Further analyses are provided towards the rationality of the simple LightGCN from both analytical and empirical perspectives.",
                    "Citation Paper Authors": "Authors:Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, Meng Wang"
                }
            },
            {
                "Sentence ID": 32,
                "Sentence": "to model the\nitem-item relationships for Pinterest. Wang et al. devised NGCF ",
                "Citation Text": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\nNeural Graph Collaborative Filtering. In Proceedings of the 42nd International\nACM SIGIR Conference on Research and Development in Information Retrieval\n(SIGIR) . 165\u2013174.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1905.08108",
                    "Citation Paper Title": "Title:Neural Graph Collaborative Filtering",
                    "Citation Paper Abstract": "Abstract:Learning vector representations (aka. embeddings) of users and items lies at the core of modern recommender systems. Ranging from early matrix factorization to recently emerged deep learning based methods, existing efforts typically obtain a user's (or an item's) embedding by mapping from pre-existing features that describe the user (or the item), such as ID and attributes. We argue that an inherent drawback of such methods is that, the collaborative signal, which is latent in user-item interactions, is not encoded in the embedding process. As such, the resultant embeddings may not be sufficient to capture the collaborative filtering effect.\nIn this work, we propose to integrate the user-item interactions -- more specifically the bipartite graph structure -- into the embedding process. We develop a new recommendation framework Neural Graph Collaborative Filtering (NGCF), which exploits the user-item graph structure by propagating embeddings on it. This leads to the expressive modeling of high-order connectivity in user-item graph, effectively injecting the collaborative signal into the embedding process in an explicit manner. We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOP-Rec and Collaborative Memory Network. Further analysis verifies the importance of embedding propagation for learning better user and item representations, justifying the rationality and effectiveness of NGCF. Codes are available at this https URL.",
                    "Citation Paper Authors": "Authors:Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua"
                }
            },
            {
                "Sentence ID": 18,
                "Sentence": ", which\napplied variational autoencoder (VAE) for CF. Ma et al. proposed\nMacridVAE ",
                "Citation Text": "Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. 2019. Learn-\ning Disentangled Representations for Recommendation. In Advances in Neural\nInformation Processing Systems (NeurIPS) . 5711\u20135722.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1910.14238",
                    "Citation Paper Title": "Title:Learning Disentangled Representations for Recommendation",
                    "Citation Paper Abstract": "Abstract:User behavior data in recommender systems are driven by the complex interactions of many latent factors behind the users' decision making processes. The factors are highly entangled, and may range from high-level ones that govern user intentions, to low-level ones that characterize a user's preference when executing an intention. Learning representations that uncover and disentangle these latent factors can bring enhanced robustness, interpretability, and controllability. However, learning such disentangled representations from user behavior is challenging, and remains largely neglected by the existing literature. In this paper, we present the MACRo-mIcro Disentangled Variational Auto-Encoder (MacridVAE) for learning disentangled representations from user behavior. Our approach achieves macro disentanglement by inferring the high-level concepts associated with user intentions (e.g., to buy a shirt or a cellphone), while capturing the preference of a user regarding the different concepts separately. A micro-disentanglement regularizer, stemming from an information-theoretic interpretation of VAEs, then forces each dimension of the representations to independently reflect an isolated low-level factor (e.g., the size or the color of a shirt). Empirical results show that our approach can achieve substantial improvement over the state-of-the-art baselines. We further demonstrate that the learned representations are interpretable and controllable, which can potentially lead to a new paradigm for recommendation where users are given fine-grained control over targeted aspects of the recommendation lists.",
                    "Citation Paper Authors": "Authors:Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, Wenwu Zhu"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2109.12894v6": {
        "Paper Title": "Training Spiking Neural Networks Using Lessons From Deep Learning",
        "Sentences": [
            {
                "Sentence ID": 130,
                "Sentence": ", but also reduces the cost of data communication in hardware ",
                "Citation Text": "Michael Laskin, Luke Metz, Seth Nabarrao, Mark Saroufim, Badreddine Noune, Carlo Luschi, Jascha\nSohl-Dickstein, and Pieter Abbeel. Parallel training of deep networks with local updates. arXiv preprint\narXiv:2012.03837 , 2020.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2012.03837",
                    "Citation Paper Title": "Title:Parallel Training of Deep Networks with Local Updates",
                    "Citation Paper Abstract": "Abstract:Deep learning models trained on large data sets have been widely successful in both vision and language domains. As state-of-the-art deep learning architectures have continued to grow in parameter count so have the compute budgets and times required to train them, increasing the need for compute-efficient methods that parallelize training. Two common approaches to parallelize the training of deep networks have been data and model parallelism. While useful, data and model parallelism suffer from diminishing returns in terms of compute efficiency for large batch sizes. In this paper, we investigate how to continue scaling compute efficiently beyond the point of diminishing returns for large batches through local parallelism, a framework which parallelizes training of individual layers in deep networks by replacing global backpropagation with truncated layer-wise backpropagation. Local parallelism enables fully asynchronous layer-wise parallelism with a low memory footprint, and requires little communication overhead compared with model parallelism. We show results in both vision and language domains across a diverse set of architectures, and find that local parallelism is particularly effective in the high-compute regime.",
                    "Citation Paper Authors": "Authors:Michael Laskin, Luke Metz, Seth Nabarro, Mark Saroufim, Badreddine Noune, Carlo Luschi, Jascha Sohl-Dickstein, Pieter Abbeel"
                }
            },
            {
                "Sentence ID": 115,
                "Sentence": ". The multi-faceted nature of the brain\u2019s function likely calls for the existence of multiple\nobjectives ",
                "Citation Text": "Adam H Marblestone, Greg Wayne, and Konrad P Kording. Toward an integration of deep learning and\nneuroscience. Frontiers in Comput. Neuroscience , 10:94, 2016.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1606.03813",
                    "Citation Paper Title": "Title:Towards an integration of deep learning and neuroscience",
                    "Citation Paper Abstract": "Abstract:Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) these cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.",
                    "Citation Paper Authors": "Authors:Adam Marblestone, Greg Wayne, Konrad Kording"
                }
            },
            {
                "Sentence ID": 104,
                "Sentence": "4 classes of playing cards flipped in rapid succession in front of a DVS.\nDSEC ",
                "Citation Text": "Mathias Gehrig, Willem Aarents, Daniel Gehrig, and Davide Scaramuzza. Dsec: A stereo event camera dataset\nfor driving scenarios. IEEE Robot. and Automat. Lett. , 6(3):4947\u20134954, 2021.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2103.06011",
                    "Citation Paper Title": "Title:DSEC: A Stereo Event Camera Dataset for Driving Scenarios",
                    "Citation Paper Abstract": "Abstract:Once an academic venture, autonomous driving has received unparalleled corporate funding in the last decade. Still, the operating conditions of current autonomous cars are mostly restricted to ideal scenarios. This means that driving in challenging illumination conditions such as night, sunrise, and sunset remains an open problem. In these cases, standard cameras are being pushed to their limits in terms of low light and high dynamic range performance. To address these challenges, we propose, DSEC, a new dataset that contains such demanding illumination conditions and provides a rich set of sensory data. DSEC offers data from a wide-baseline stereo setup of two color frame cameras and two high-resolution monochrome event cameras. In addition, we collect lidar data and RTK GPS measurements, both hardware synchronized with all camera data. One of the distinctive features of this dataset is the inclusion of high-resolution event cameras. Event cameras have received increasing attention for their high temporal resolution and high dynamic range performance. However, due to their novelty, event camera datasets in driving scenarios are rare. This work presents the first high-resolution, large-scale stereo dataset with event cameras. The dataset contains 53 sequences collected by driving in a variety of illumination conditions and provides ground truth disparity for the development and evaluation of event-based stereo algorithms.",
                    "Citation Paper Authors": "Authors:Mathias Gehrig, Willem Aarents, Daniel Gehrig, Davide Scaramuzza"
                }
            },
            {
                "Sentence ID": 102,
                "Sentence": "Spikes, frames and optical flow from stereo cameras for indoor and outdoor scenarios.\nN-MNIST ",
                "Citation Text": "Garrick Orchard, Ajinkya Jayawant, Gregory K Cohen, and Nitish Thakor. Converting static image datasets to\nspiking neuromorphic datasets using saccades. Frontiers in Neuroscience , 9:437, 2015.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1507.07629",
                    "Citation Paper Title": "Title:Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades",
                    "Citation Paper Abstract": "Abstract:Creating datasets for Neuromorphic Vision is a challenging task. A lack of available recordings from Neuromorphic Vision sensors means that data must typically be recorded specifically for dataset creation rather than collecting and labelling existing data. The task is further complicated by a desire to simultaneously provide traditional frame-based recordings to allow for direct comparison with traditional Computer Vision algorithms. Here we propose a method for converting existing Computer Vision static image datasets into Neuromorphic Vision datasets using an actuated pan-tilt camera platform. Moving the sensor rather than the scene or image is a more biologically realistic approach to sensing and eliminates timing artifacts introduced by monitor updates when simulating motion on a computer monitor. We present conversion of two popular image datasets (MNIST and Caltech101) which have played important roles in the development of Computer Vision, and we provide performance metrics on these datasets using spike-based recognition algorithms. This work contributes datasets for future use in the field, as well as results from spike-based algorithms against which future works can compare. Furthermore, by converting datasets already popular in Computer Vision, we enable more direct comparison with frame-based approaches.",
                    "Citation Paper Authors": "Authors:Garrick Orchard, Ajinkya Jayawant, Gregory Cohen, Nitish Thakor"
                }
            },
            {
                "Sentence ID": 98,
                "Sentence": "100,800 samples of American sign language recorded with DA VIS.\nDA VIS Dataset ",
                "Citation Text": "Elias Mueggler, Henri Rebecq, Guillermo Gallego, Tobi Delbruck, and Davide Scaramuzza. The event-camera\ndataset and simulator: Event-based data for pose estimation, visual odometry, and slam. The Int. J. of Robot.\nRes., 36(2):142\u2013149, 2017.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1610.08336",
                    "Citation Paper Title": "Title:The Event-Camera Dataset and Simulator: Event-based Data for Pose Estimation, Visual Odometry, and SLAM",
                    "Citation Paper Abstract": "Abstract:New vision sensors, such as the Dynamic and Active-pixel Vision sensor (DAVIS), incorporate a conventional global-shutter camera and an event-based sensor in the same pixel array. These sensors have great potential for high-speed robotics and computer vision because they allow us to combine the benefits of conventional cameras with those of event-based sensors: low latency, high temporal resolution, and very high dynamic range. However, new algorithms are required to exploit the sensor characteristics and cope with its unconventional output, which consists of a stream of asynchronous brightness changes (called \"events\") and synchronous grayscale frames. For this purpose, we present and release a collection of datasets captured with a DAVIS in a variety of synthetic and real environments, which we hope will motivate research on new algorithms for high-speed and high-dynamic-range robotics and computer-vision applications. In addition to global-shutter intensity images and asynchronous events, we provide inertial measurements and ground-truth camera poses from a motion-capture system. The latter allows comparing the pose accuracy of ego-motion estimation algorithms quantitatively. All the data are released both as standard text files and binary files (i.e., rosbag). This paper provides an overview of the available data and describes a simulator that we release open-source to create synthetic event-camera data.",
                    "Citation Paper Authors": "Authors:Elias Mueggler, Henri Rebecq, Guillermo Gallego, Tobi Delbruck, Davide Scaramuzza"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2107.06881v2": {
        "Paper Title": "Evolution of Non-Terrestrial Networks From 5G to 6G: A Survey",
        "Sentences": [
            {
                "Sentence ID": 7,
                "Sentence": "focused on UA V cellular communication,\nmostly where UA Vs act as end-users, and provides important\ntakeaways for the corresponding networks. The authors review\nthe use cases, communication requirements, challenges, and\npotential solutions. In ",
                "Citation Text": "N.-N. Dao, Q.-V . Pham, N. H. Tu, T. T. Thanh, V . N. Q. Bao, D. S.\nLakew, and S. Cho, \u201cSurvey on aerial radio access networks: Toward\na comprehensive 6G access infrastructure,\u201d IEEE Communications\nSurveys Tutorials , vol. 23, no. 2, pp. 1193\u20131225, 2021.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2102.07087",
                    "Citation Paper Title": "Title:Survey on Aerial Radio Access Networks: Toward a Comprehensive 6G Access Infrastructure",
                    "Citation Paper Abstract": "Abstract:Current network access infrastructures are characterized by heterogeneity, low latency, high throughput, and high computational capability, enabling massive concurrent connections and various services. Unfortunately, this design does not pay significant attention to mobile services in underserved areas. In this context, the use of aerial radio access networks (ARANs) is a promising strategy to complement existing terrestrial communication systems. Involving airborne components such as unmanned aerial vehicles, drones, and satellites, ARANs can quickly establish a flexible access infrastructure on demand. ARANs are expected to support the development of seamless mobile communication systems toward a comprehensive sixth-generation (6G) global access infrastructure. This paper provides an overview of recent studies regarding ARANs in the literature. First, we investigate related work to identify areas for further exploration in terms of recent knowledge advancements and analyses. Second, we define the scope and methodology of this study. Then, we describe ARAN architecture and its fundamental features for the development of 6G networks. In particular, we analyze the system model from several perspectives, including transmission propagation, energy consumption, communication latency, and network mobility. Furthermore, we introduce technologies that enable the success of ARAN implementations in terms of energy replenishment, operational management, and data delivery. Subsequently, we discuss application scenarios envisioned for these technologies. Finally, we highlight ongoing research efforts and trends toward 6G ARANs.",
                    "Citation Paper Authors": "Authors:Nhu-Ngoc Dao, Quoc-Viet Pham, Ngo Hoang Tu, Tran Thien Thanh, Vo Nguyen Quoc Bao, Demeke Shumeye Lakew, Sungrae Cho"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2103.17193v3": {
        "Paper Title": "QPack: Quantum Approximate Optimization Algorithms as universal\n  benchmark for quantum computers",
        "Sentences": [
            {
                "Sentence ID": 67,
                "Sentence": ". As the quantum hardware scales further, this parallelism\ncan result in signi\ufb01cant speedup.\nThe QPack benchmark aims to expand its repertoire of\nalgorithms in the future, as applications based on Shor\u2019s\nand Grover\u2019s algorithm ",
                "Citation Text": "A. Sarkar, Z. Al-Ars, C. G. Almudever, and K. Bertels, \u201cAn algo-\nrithm for dna read alignment on quantum accelerators,\u201d arXiv preprint\narXiv:1909.05563, 2019.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1909.05563",
                    "Citation Paper Title": "Title:An algorithm for DNA read alignment on quantum accelerators",
                    "Citation Paper Abstract": "Abstract:With small-scale quantum processors transitioning from experimental physics labs to industrial products, these processors allow us to efficiently compute important algorithms in various fields. In this paper, we propose a quantum algorithm to address the challenging field of big data processing for genome sequence reconstruction. This research describes an architecture-aware implementation of a quantum algorithm for sub-sequence alignment. A new algorithm named QiBAM (quantum indexed bidirectional associative memory) is proposed, that uses approximate pattern-matching based on Hamming distances. QiBAM extends the Grover's search algorithm in two ways to allow for: (1) approximate matches needed for read errors in genomics, and (2) a distributed search for multiple solutions over the quantum encoding of DNA sequences. This approach gives a quadratic speedup over the classical algorithm. A full implementation of the algorithm is provided and verified using the OpenQL compiler and QX simulator framework. This represents a first exploration towards a full-stack quantum accelerated genome sequencing pipeline design. The open-source implementation can be found on this https URL.",
                    "Citation Paper Authors": "Authors:Aritra Sarkar, Zaid Al-Ars, Carmen G. Almudever, Koen Bertels"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2111.09272v3": {
        "Paper Title": "ReaLPrune: ReRAM Crossbar-aware Lottery Ticket Pruned CNNs",
        "Sentences": [
            {
                "Sentence ID": 31,
                "Sentence": ". However, GPUs are relatively slower than ReRAMs for performing IMA oper-ations. This can result in relatively sub-optimal AUTHOR ET AL.:  TITLE 3 \n performance. In ",
                "Citation Text": "X. Peng, S. Huang, H. Jiang, A. Lu and S. Yu, \"DNN+NeuroSim V2.0: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators for On-Chip Training,\" in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 40, no. 11, pp. 2306-2319, Nov. 2021.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2003.06471",
                    "Citation Paper Title": "Title:DNN+NeuroSim V2.0: An End-to-End Benchmarking Framework for Compute-in-Memory Accelerators for On-chip Training",
                    "Citation Paper Abstract": "Abstract:DNN+NeuroSim is an integrated framework to benchmark compute-in-memory (CIM) accelerators for deep neural networks, with hierarchical design options from device-level, to circuit-level and up to algorithm-level. A python wrapper is developed to interface NeuroSim with a popular machine learning platform: Pytorch, to support flexible network structures. The framework provides automatic algorithm-to-hardware mapping, and evaluates chip-level area, energy efficiency and throughput for training or inference, as well as training/inference accuracy with hardware constraints. Our prior work (DNN+NeuroSim V1.1) was developed to estimate the impact of reliability in synaptic devices, and analog-to-digital converter (ADC) quantization loss on the accuracy and hardware performance of inference engines. In this work, we further investigated the impact of the analog emerging non-volatile memory non-ideal device properties for on-chip training. By introducing the nonlinearity, asymmetry, device-to-device and cycle-to-cycle variation of weight update into the python wrapper, and peripheral circuits for error/weight gradient computation in NeuroSim core, we benchmarked CIM accelerators based on state-of-the-art SRAM and eNVM devices for VGG-8 on CIFAR-10 dataset, revealing the crucial specs of synaptic devices for on-chip training. The proposed DNN+NeuroSim V2.0 framework is available on GitHub.",
                    "Citation Paper Authors": "Authors:Xiaochen Peng, Shanshi Huang, Hongwu Jiang, Anni Lu, Shimeng Yu"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2109.13101v4": {
        "Paper Title": "Half a Dozen Real-World Applications of Evolutionary Multitasking, and\n  More",
        "Sentences": [
            {
                "Sentence ID": 12,
                "Sentence": ", most\nalgorithms typify one of the two methodological classes sta ted\nbelow. An extensive analysis of these methods is not include d\nherein as excellent reviews are available elsewhere ",
                "Citation Text": "E. Osaba, A. D. Martinez, and J. Del Ser, \u201cEvolutionary m ultitask op-\ntimization: a methodological overview, challenges and fut ure research\ndirections,\u201d arXiv preprint arXiv:2102.02558 , 2021.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2102.02558",
                    "Citation Paper Title": "Title:Evolutionary Multitask Optimization: a Methodological Overview, Challenges and Future Research Directions",
                    "Citation Paper Abstract": "Abstract:In this work we consider multitasking in the context of solving multiple optimization problems simultaneously by conducting a single search process. The principal goal when dealing with this scenario is to dynamically exploit the existing complementarities among the problems (tasks) being optimized, helping each other through the exchange of valuable knowledge. Additionally, the emerging paradigm of Evolutionary Multitasking tackles multitask optimization scenarios by using as inspiration concepts drawn from Evolutionary Computation. The main purpose of this survey is to collect, organize and critically examine the abundant literature published so far in Evolutionary Multitasking, with an emphasis on the methodological patterns followed when designing new algorithmic proposals in this area (namely, multifactorial optimization and multipopulation-based multitasking). We complement our critical analysis with an identification of challenges that remain open to date, along with promising research directions that can stimulate future efforts in this topic. Our discussions held throughout this manuscript are offered to the audience as a reference of the general trajectory followed by the community working in this field in recent times, as well as a self-contained entry point for newcomers and researchers interested to join this exciting research avenue.",
                    "Citation Paper Authors": "Authors:Eneko Osaba, Aritz D. Martinez, Javier Del Ser"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2011.08722v3": {
        "Paper Title": "RAIST: Learning Risk Aware Traffic Interactions via Spatio-Temporal\n  Graph Convolutional Networks",
        "Sentences": [
            {
                "Sentence ID": 42,
                "Sentence": "have demonstrated significant progress\nin tasks such as action recognition ",
                "Citation Text": "Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph\nconvolutional networks for skeleton-based action recognition. arXiv\npreprint arXiv:1801.07455 , 2018.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1801.07455",
                    "Citation Paper Title": "Title:Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition",
                    "Citation Paper Abstract": "Abstract:Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.",
                    "Citation Paper Authors": "Authors:Sijie Yan, Yuanjun Xiong, Dahua Lin"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2007.06676v4": {
        "Paper Title": "UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a\n  Generic Framework for Handling Common Camera Distortion Models",
        "Sentences": [
            {
                "Sentence ID": 30,
                "Sentence": "mainly to reduce the chance of training reaching\na local minimum. The overall objective function is averagedMethod Resolution Dataset Abs Rel Sq Rel RMSE RMSE log\u03b4 <1.25\u03b4 <1.252\u03b4 <1.253\nlower is better higher is betterOriginal ",
                "Citation Text": "D. Eigen, C. Puhrsch, and R. Fergus, \u201cDepth map prediction from a\nsingle image using a multi-scale deep network,\u201d in Advances in neural\ninformation processing systems , 2014, pp. 2366\u20132374. 5",
                "Citation": {
                    "Citation Paper ID": "arXiv:1406.2283",
                    "Citation Paper Title": "Title:Depth Map Prediction from a Single Image using a Multi-Scale Deep Network",
                    "Citation Paper Abstract": "Abstract:Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.",
                    "Citation Paper Authors": "Authors:David Eigen, Christian Puhrsch, Rob Fergus"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2005.04291v2": {
        "Paper Title": "Learning Stable Models for Prediction and Control",
        "Sentences": [
            {
                "Sentence ID": 66,
                "Sentence": "(referred to\nas SOC) to \ufb01nd locally optimal stable solutions \u02dcKd. The SOC\nalgorithm builds upon the work in ",
                "Citation Text": "N. Gillis, M. Karow, and P. Sharma, \u201cApproximating the nearest stable\ndiscrete-time system,\u201d Linear Algebra and its Applications , vol. 573, pp.\n37\u201353, 2019.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1802.08033",
                    "Citation Paper Title": "Title:Approximating the nearest stable discrete-time system",
                    "Citation Paper Abstract": "Abstract:In this paper, we consider the problem of stabilizing discrete-time linear systems by computing a nearby stable matrix to an unstable one. To do so, we provide a new characterization for the set of stable matrices. We show that a matrix $A$ is stable if and only if it can be written as $A=S^{-1}UBS$, where $S$ is positive definite, $U$ is orthogonal, and $B$ is a positive semidefinite contraction (that is, the singular values of $B$ are less or equal to 1). This characterization results in an equivalent non-convex optimization problem with a feasible set on which it is easy to project. We propose a very efficient fast projected gradient method to tackle the problem in variables $(S,U,B)$ and generate locally optimal solutions. We show the effectiveness of the proposed method compared to other approaches.",
                    "Citation Paper Authors": "Authors:Nicolas Gillis, Michael Karow, Punit Sharma"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2008.05058v4": {
        "Paper Title": "Dynamic Object Removal and Spatio-Temporal RGB-D Inpainting via\n  Geometry-Aware Adversarial Learning",
        "Sentences": [
            {
                "Sentence ID": 26,
                "Sentence": "\ufb01rst\nsynthesize missing optical \ufb02ow which is then used to propagate\nneighboring pixels to \ufb01ll missing regions. Chang et al. ",
                "Citation Text": "Y .-L. Chang, Z. Y . Liu, K.-Y . Lee, and W. Hsu, \u201cLearnable gated\ntemporal shift module for deep video inpainting\u201d,\u201d British Machine Vision\nConference (BMVC) , 2019.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1907.01131",
                    "Citation Paper Title": "Title:Learnable Gated Temporal Shift Module for Deep Video Inpainting",
                    "Citation Paper Abstract": "Abstract:How to efficiently utilize temporal information to recover videos in a consistent way is the main issue for video inpainting problems. Conventional 2D CNNs have achieved good performance on image inpainting but often lead to temporally inconsistent results where frames will flicker when applied to videos (see this https URL 3D CNNs can capture temporal information but are computationally intensive and hard to train. In this paper, we present a novel component termed Learnable Gated Temporal Shift Module (LGTSM) for video inpainting models that could effectively tackle arbitrary video masks without additional parameters from 3D convolutions. LGTSM is designed to let 2D convolutions make use of neighboring frames more efficiently, which is crucial for video inpainting. Specifically, in each layer, LGTSM learns to shift some channels to its temporal neighbors so that 2D convolutions could be enhanced to handle temporal information. Meanwhile, a gated convolution is applied to the layer to identify the masked areas that are poisoning for conventional convolutions. On the FaceForensics and Free-form Video Inpainting (FVI) dataset, our model achieves state-of-the-art results with simply 33% of parameters and inference time.",
                    "Citation Paper Authors": "Authors:Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee, Winston Hsu"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2010.04514v2": {
        "Paper Title": "Robust walking based on MPC with viability guarantees",
        "Sentences": [
            {
                "Sentence ID": 24,
                "Sentence": ", etc.) in contrast with\nexpressive policy representation without any prior (e.g., Neural\nNetworks) ",
                "Citation Text": "K. Chatzilygeroudis, V . Vassiliades, F. Stulp, S. Calinon, and J.-B.\nMouret, \u201cA survey on policy search algorithms for learning robot\ncontrollers in a handful of trials,\u201d IEEE Transactions on Robotics ,\nvol. 36, no. 2, pp. 328\u2013347, 2019.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1807.02303",
                    "Citation Paper Title": "Title:A survey on policy search algorithms for learning robot controllers in a handful of trials",
                    "Citation Paper Abstract": "Abstract:Most policy search algorithms require thousands of training episodes to find an effective policy, which is often infeasible with a physical robot. This survey article focuses on the extreme other end of the spectrum: how can a robot adapt with only a handful of trials (a dozen) and a few minutes? By analogy with the word \"big-data\", we refer to this challenge as \"micro-data reinforcement learning\". We show that a first strategy is to leverage prior knowledge on the policy structure (e.g., dynamic movement primitives), on the policy parameters (e.g., demonstrations), or on the dynamics (e.g., simulators). A second strategy is to create data-driven surrogate models of the expected reward (e.g., Bayesian optimization) or the dynamical model (e.g., model-based policy search), so that the policy optimizer queries the model instead of the real system. Overall, all successful micro-data algorithms combine these two strategies by varying the kind of model and prior knowledge. The current scientific challenges essentially revolve around scaling up to complex robots (e.g., humanoids), designing generic priors, and optimizing the computing time.",
                    "Citation Paper Authors": "Authors:Konstantinos Chatzilygeroudis, Vassilis Vassiliades, Freek Stulp, Sylvain Calinon, Jean-Baptiste Mouret"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2003.08854v3": {
        "Paper Title": "Goal-Conditioned End-to-End Visuomotor Control for Versatile Skill\n  Primitives",
        "Sentences": [
            {
                "Sentence ID": 15,
                "Sentence": ". Another\nline of work attempts to learn forward dynamics models in\nsuitable latent spaces. After projecting an observation and a\ngoal image into the latent space, a feasible action sequence\ncan then be computed using gradient-based optimisation\nmethods ",
                "Citation Text": "M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller, \u201cEmbed\nto control: A locally linear latent dynamics model for control from\nraw images,\u201d in Advances in neural information processing systems ,\n2015, pp. 2746\u20132754.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1506.07365",
                    "Citation Paper Title": "Title:Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
                    "Citation Paper Abstract": "Abstract:We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.",
                    "Citation Paper Authors": "Authors:Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin Riedmiller"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2001.00735v2": {
        "Paper Title": "Trajectory Forecasts in Unknown Environments Conditioned on Grid-Based\n  Plans",
        "Sentences": [
            {
                "Sentence ID": 43,
                "Sentence": "2.44 1.57 0.70 0.55 0.11 0.11\nTrajectron ++ (3rd place) ",
                "Citation Text": "T. Salzmann, B. Ivanovic, P. Chakravarty, and M. Pavone, \u201cTrajectron++:\nDynamically-feasible trajectory forecasting with heterogeneous data,\u201d\narXiv preprint arXiv:2001.03093 , 2020.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2001.03093",
                    "Citation Paper Title": "Title:Trajectron++: Dynamically-Feasible Trajectory Forecasting With Heterogeneous Data",
                    "Citation Paper Abstract": "Abstract:Reasoning about human motion is an important prerequisite to safe and socially-aware robotic navigation. As a result, multi-agent behavior prediction has become a core component of modern human-robot interactive systems, such as self-driving cars. While there exist many methods for trajectory forecasting, most do not enforce dynamic constraints and do not account for environmental information (e.g., maps). Towards this end, we present Trajectron++, a modular, graph-structured recurrent model that forecasts the trajectories of a general number of diverse agents while incorporating agent dynamics and heterogeneous data (e.g., semantic maps). Trajectron++ is designed to be tightly integrated with robotic planning and control frameworks; for example, it can produce predictions that are optionally conditioned on ego-agent motion plans. We demonstrate its performance on several challenging real-world trajectory forecasting datasets, outperforming a wide array of state-of-the-art deterministic and generative methods.",
                    "Citation Paper Authors": "Authors:Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, Marco Pavone"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2011.12790v2": {
        "Paper Title": "Fast Region Proposal Learning for Object Detection for Robotics",
        "Sentences": [
            {
                "Sentence ID": 4,
                "Sentence": ", i.e. (i) region candidates\ngeneration, (ii) per region feature extraction and (iii) region\nclassi\ufb01cation and re\ufb01nement. The major trend in the state-\nof-the-art is to integrate the three steps into \u201cmonolithic\u201d\ndeep learning based architectures ",
                "Citation Text": "Kaiming He, Georgia Gkioxari, Piotr Doll \u00b4ar, and Ross B. Girshick.\nMask r-cnn. 2017 IEEE International Conference on Computer Vision\n(ICCV) , pages 2980\u20132988, 2017.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1703.06870",
                    "Citation Paper Title": "Title:Mask R-CNN",
                    "Citation Paper Abstract": "Abstract:We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: this https URL",
                    "Citation Paper Authors": "Authors:Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2010.16272v2": {
        "Paper Title": "PATHoBot: A Robot for Glasshouse Crop Phenotyping and Intervention",
        "Sentences": [
            {
                "Sentence ID": 9,
                "Sentence": "presented a sweet pepper\nharvesting robot for ad-hoc cropping systems. More recently,\nHalstead et al. ",
                "Citation Text": "M. Halstead, S. Denman, C. Fookes, and C. McCool, \u201cFruit detection\nin the wild: The impact of varying conditions and cultivar,\u201d in to\nappear in Proceedings of Digital Image Computing: Techniques and\nApplications (DICTA) , 2020.",
                "Citation": {
                    "Citation Paper ID": "arXiv:1801.05560",
                    "Citation Paper Title": "Title:Fruit Quantity and Quality Estimation using a Robotic Vision System",
                    "Citation Paper Abstract": "Abstract:Accurate localisation of crop remains highly challenging in unstructured environments such as farms. Many of the developed systems still rely on the use of hand selected features for crop identification and often neglect the estimation of crop quantity and quality, which is key to assigning labor during farming processes. To alleviate these limitations we present a robotic vision system that can accurately estimate the quantity and quality of sweet pepper (Capsicum annuum L), a key horticultural crop. This system consists of three parts: detection, quality estimation, and tracking. Efficient detection is achieved using the FasterRCNN framework. Quality is then estimated in the same framework by learning a parallel layer which we show experimentally results in superior performance than treating quality as extra classes in the traditional Faster-RCNN framework. Evaluation of these two techniques outlines the improved performance of the parallel layer, where we achieve an F1 score of 77.3 for the parallel technique yet only 72.5 for the best scoring (red) of the multi-class implementation. To track the crop we present a tracking via detection approach, which uses the FasterRCNN with parallel layers, that is also a vision-only solution. This approach is cheap to implement as it only requires a camera and in experiments across 2 days we show that our proposed system can accurately estimate the number of sweet pepper present, within 4.1% of the ground truth.",
                    "Citation Paper Authors": "Authors:M. Halstead, C. McCool, S. Denman, T. Perez, C. Fookes"
                }
            }
        ]
    },
    "http://arxiv.org/abs/2012.02148v3": {
        "Paper Title": "Graph-SIM: A Graph-based Spatiotemporal Interaction Modelling for\n  Pedestrian Action Prediction",
        "Sentences": [
            {
                "Sentence ID": 9,
                "Sentence": "use a 3D DenseNet architecture that detects\nand predicts pedestrian actions in a single framework.\nRecurrent approaches, which rely on multiple data modal-\nities, are also widely used ",
                "Citation Text": "B. Liu, E. Adeli, Z. Cao, K.-H. Lee, A. Shenoi, A. Gaidon, and J. C.\nNiebles, \u201cSpatiotemporal relationship reasoning for pedestrian intent\nprediction,\u201d IEEE RA-L , 2020.",
                "Citation": {
                    "Citation Paper ID": "arXiv:2002.08945",
                    "Citation Paper Title": "Title:Spatiotemporal Relationship Reasoning for Pedestrian Intent Prediction",
                    "Citation Paper Abstract": "Abstract:Reasoning over visual data is a desirable capability for robotics and vision-based applications. Such reasoning enables forecasting of the next events or actions in videos. In recent years, various models have been developed based on convolution operations for prediction or forecasting, but they lack the ability to reason over spatiotemporal data and infer the relationships of different objects in the scene. In this paper, we present a framework based on graph convolution to uncover the spatiotemporal relationships in the scene for reasoning about pedestrian intent. A scene graph is built on top of segmented object instances within and across video frames. Pedestrian intent, defined as the future action of crossing or not-crossing the street, is a very crucial piece of information for autonomous vehicles to navigate safely and more smoothly. We approach the problem of intent prediction from two different perspectives and anticipate the intention-to-cross within both pedestrian-centric and location-centric scenarios. In addition, we introduce a new dataset designed specifically for autonomous-driving scenarios in areas with dense pedestrian populations: the Stanford-TRI Intent Prediction (STIP) dataset. Our experiments on STIP and another benchmark dataset show that our graph modeling framework is able to predict the intention-to-cross of the pedestrians with an accuracy of 79.10% on STIP and 79.28% on \\rev{Joint Attention for Autonomous Driving (JAAD) dataset up to one second earlier than when the actual crossing happens. These results outperform the baseline and previous work. Please refer to this http URL for the dataset and code.",
                    "Citation Paper Authors": "Authors:Bingbin Liu, Ehsan Adeli, Zhangjie Cao, Kuan-Hui Lee, Abhijeet Shenoi, Adrien Gaidon, Juan Carlos Niebles"
                }
            }
        ]
    }
  }
}